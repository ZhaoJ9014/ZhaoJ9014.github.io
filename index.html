<!DOCTYPE html>
<html lang="en" class=" js no-touch csstransitions" style=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>Jian Zhao - Deep Learning & Computer Vision, NUS </title>
        <link href="images/favicon.png" rel="shortcut icon" type="image/x-icon" />
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        <meta name="google-site-verification" content="tXGinevmz43rJKRTfCj6v5f8zN91tcmg6fdrrYUn-gI" />
        <!--[if lt IE 9]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">
        <link rel="stylesheet" href="css/academicons.min.css">
        <link rel="stylesheet" href="css/style.css?v6">
        <script type="text/javascript" src="./js/jquery-1.10.2.js"></script>
        <script type="text/javascript" src="./js/TweenMax.min.js"></script>
        <script type="text/javascript" src="./js/modernizr.custom.63321.js"></script>
        <script type="text/javascript" src="./js/bootstrap.min.js"></script>
        <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
        <script type="text/javascript" src="./js/custom.js"></script>
        <!-- start Mixpanel --><script type="text/javascript">(function(e,a){if(!a.__SV){var b=window;try{var c,l,i,j=b.location,g=j.hash;c=function(a,b){return(l=a.match(RegExp(b+"=([^&]*)")))?l[1]:null};g&&c(g,"state")&&(i=JSON.parse(decodeURIComponent(c(g,"state"))),"mpeditor"===i.action&&(b.sessionStorage.setItem("_mpcehash",g),history.replaceState(i.desiredHash||"",e.title,j.pathname+j.search)))}catch(m){}var k,h;window.mixpanel=a;a._i=[];a.init=function(b,c,f){function e(b,a){var c=a.split(".");2==c.length&&(b=b[c[0]],a=c[1]);b[a]=function(){b.push([a].concat(Array.prototype.slice.call(arguments,
0)))}}var d=a;"undefined"!==typeof f?d=a[f]=[]:f="mixpanel";d.people=d.people||[];d.toString=function(b){var a="mixpanel";"mixpanel"!==f&&(a+="."+f);b||(a+=" (stub)");return a};d.people.toString=function(){return d.toString(1)+".people (stub)"};k="disable time_event track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config reset people.set people.set_once people.increment people.append people.union people.track_charge people.clear_charges people.delete_user".split(" ");
for(h=0;h<k.length;h++)e(d,k[h]);a._i.push([b,c,f])};a.__SV=1.2;b=e.createElement("script");b.type="text/javascript";b.async=!0;b.src="undefined"!==typeof MIXPANEL_CUSTOM_LIB_URL?MIXPANEL_CUSTOM_LIB_URL:"file:"===e.location.protocol&&"//cdn.mxpnl.com/libs/mixpanel-2-latest.min.js".match(/^\/\//)?"https://cdn.mxpnl.com/libs/mixpanel-2-latest.min.js":"//cdn.mxpnl.com/libs/mixpanel-2-latest.min.js";c=e.getElementsByTagName("script")[0];c.parentNode.insertBefore(b,c)}})(document,window.mixpanel||[]);
mixpanel.init("ebfda33a74d82c326f768bb95f1db5ab");</script><!-- end Mixpanel -->
    </head>
    <body>

        <div id="wrapper">
            <a class="mobilemenu"><i class="fa fa-bars"></i></a>

            <div id="sidebar">
                <div id="main-nav" class="ps-container">
                    <div id="nav-container">
                        <div id="profile" class="clearfix">
                            <div class="portrait hidden-xs"></div>
                            <div class="title">
                                <h2>Jian Zhao</h2>
                                <h3>National University of Singapore</h3>
                            </div>
                            
                        </div>
                        <ul id="navigation">
                            <li class="currentmenu">
                              <a href="#biography">
                                  <div class="icon"><i class="fa fa-user"></i></div>
                                <div class="text">About Me</div>
                              </a>
                            </li>  
                            <li>
                              <a href="#education">
                                <div class="icon"><i class="fa fa-university"></i></div>
                                <div class="text">Education</div>
                              </a>
                            </li>

                            <li>
                              <a href="#work">
                                <div class="icon"><i class="fa fa-suitcase"></i></div>
                                <div class="text">Work Experience</div>
                              </a>
                            </li>

                            <li>
                                <a href="#publications">
                                    <div class="icon"><i class="fa fa-book"></i></div>
                                    <div class="text">Publications</div>
                                </a>
                            </li>

                            <li>
                              <a href="#contact">
                                  <div class="icon"><i class="fa fa-envelope"></i></div>
                                  <div class="text">Contact</div>
                              </a>
                            </li>

                            <li class="external CV">
                              <a target="_blank"  href="pub/CV.pdf">
                                  <div class="icon"><i class="fa fa-download"></i></div>
                                  <div class="text">Download CV</div>
                              </a>
                            </li>
                        </ul>
                    </div>        
                </div>
                    <div class="social-icons">
                        <ul>
                            <li><a href="https://github.com/ZhaoJ9014" target="_blank"><i class="fa fa-github"></i></a></li>
                            <li><a href="https://www.linkedin.com/in/jian-zhao-951089140" target="_blank"><i class="fa fa-linkedin"></i></a></li>
                            <li><a href="https://scholar.google.com.sg/citations?hl=en&user=zdhRJCkAAAAJ&view_op=list_works&gmla=AJsN-F4PURIx5GMQHVpprJJBjTsNC62YCHjxGsKOwVhrkZ1aJsLgBiuKPBbAgbdcE5_KNw3OnLQgOVSjlqmS6gc-6ti0M2K5o-klHgoOywFCbdaaGnpis130zvgoZFJkVfmoNKpo8Krp" target="_blank"><i class="ai ai-google-scholar"></i></a></li>
                        </ul>
                    </div>
            </div>

            <div id="main">
            
                <div class="page home" data-pos="home">
                    <div id="biography"  class="pageheader">
                        <div class="headercontent">
                            <div class="section-container">
                                
                                <div class="row">
                                    <div class="col-sm-2 visible-sm"></div>
                                    <div class="col-sm-8 col-md-5">
                                        <div class="biothumb">
                                            <img alt="ZHAO Jian, deep learning and computer vision Ph.D. candidate" title="Jian Zhao, deep learning and computer vision Ph.D. candidate" src="images/Ilija_Ilievski.jpg" class="img-responsive">
                                            <div class="overlay">
                                                
                                                <h1 class="">Jian Zhao</h1>
                                                <ul class="list-unstyled">
                                                    <li>Department of Electrical and Computer Engineering</li>
                                                    <li>Faculty of Engineering</li>
                                                    <li>National University of Singapore</li>
                                                </ul>
                                            </div> 
                                            
                                               
                                        </div>
                                        
                                    </div>
                                    <div class="clearfix visible-sm visible-xs"></div>
                                    <div  class="col-sm-12 col-md-7">
                                        <h3  class="title">Bio
                                         <i class="quote">"聚焦 Knowledge changes fate."</i></h3>

                                            <p>I am a Ph.D. candidate at <a href="http://www.lv-nus.org/" target="_blank">Learning and Vision Group</a>, Department of Electrical and Computer Engineering (ECE), Faculty of Engineering, National University of Singapore (NUS). My main supervisor is
                                                <a href="https://sites.google.com/site/jshfeng/" target="_blank">Assist. Prof. Jiashi Feng</a>
                                                and my co-supervisor is <a href="https://www.ece.nus.edu.sg/stfpage/eleyans/" target="_blank">Assoc. Prof. Shuicheng Yan</a>. I am
                                                generously supported by China Scholarship Council (CSC) and School of Computer, National University of Defense Technology (NUDT), China.
                                                My domestic supervisor of NUDT is Prof. Hengzhu Liu. Currently, I am working on developing deep neural network models and algorithms for human-centric image analysis, applied to face recognition, image generation and human parsing.

                                            </p>
                                            <p>
                                                I have an M.Eng. degree in Computer Science for signal processing with a thesis titled "Research on the Equalization Technologies for the Wireless Image Transmission Data Link System Based on the UAV Platform".
                                            </p>
                                            <p><b>Research interests:</b> Artificial intelligence, deep learning and computer vision, including unconstrained/large-scale/low-shot face recognition, image generation with adversarial learning, and human parsing.</p>
                                            <p><b>Past interests:</b> Signal processing, wireless communication, system modeling and simulation.</p>
                                            <p><b>Professional activities:</b> Reviewer of T-MM, T-IFS, CSSP, NIPS (one of the top 30% highest-scoring reviewers of NIPS 2018), CVPR, ICCV, ACM MM, AAAI, ICLR, ICML. Organizer of the CVPR 2018 Workshop on Visual Understanding of Humans in Crowd Scene (<a href="https://vuhcs.github.io/" target="_blank">VUHCS</a>), and <a href="http://lv-mhp.github.io" target="_blank">MHP Challenges</a> on Multi-Human Pose Estimation and Fine-Grained Multi-Human Parsing. Program Committee member of the ECCV 2018 Workshop on Compact and Efficient Feature Representation Learning in Computer Vision.</p>
					    <p><b>Memberships:</b> IEEE student member, IEEE Computer Society student member, Computer Vision Foundation student member, CCF student member.</p>
					    <p><div class="container" >    
                    		            <div class="progress progress-striped">
                                            <div class="progress-bar progress-bar-success" role="progressbar" aria-valuenow="80" aria-valuemin="0" aria-valuemax="100" style="width: 100.00%">
                                            <span>Ph.D. Requirements (7 points): 100.00% Completed (11 points now, more on the way)</span>
                                            </div>     
                     			    </div>                
                    			    </div></p>    
					    <p><b style="color: red;">Latest: </b>
					    <div><b>06 Mar 2019  &mdash;</b> I will have my PhD Oral Defense ("DEEP LEARNING FOR HUMAN-CENTRIC IMAGE ANALYSIS: FROM FACE RECOGNITION TO HUMAN PARSING") on 12:00 p.m., Wednesday, 13th March 2019, at E4-05-39, ECE, NUS.
				            <div><b>04 Jan 2019  &mdash;</b> I launched a new GitHub repo <a target="_blank" href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch">face.evoLVe.PyTorch</a> to help researchers/engineers develop high-performance deep face recognition models and algorithms quickly for practical use and deployment. (<a href="https://mp.weixin.qq.com/s/V8VoyMqVvjblH358ozcWEg" target="_blank">WeChat News</a>)
				            <div><b>06 Dec 2018  &mdash;</b> I am now working at Tencent FiT DeepSea AI Lab, Shenzhen, China as a short-term "Texpert" Research Scientist.</div> 
				            <div><b>01 Nov 2018  &mdash;</b> I have one paper accepted by AAAI 2019 as the 1st author. I will attend the conference at Honolulu, Hawaii, U.S. during 27th January 2019 to 1st February 2019 and present our work there.</div>  
				            <div><b>24 Oct 2018  &mdash;</b> I received ACM MM 2018 <a target="_blank" href="http://www.acmmm.org/2018/awards/">Best Student Paper Award</a> as the 1st author.</div>
				            <div><b>04 Jul 2018  &mdash;</b> I have one paper accepted by T-PAMI (IF: 9.455) as the 1st author.</div>              
                                            </p>
                                            <p><b>Hobbies: </b>Travel, Cooking, Rollar Skating, Fitness, Mixed Martial Arts (MMA) and Brazilian Jiu-Jitsu (BJJ). <a href="pub/Gym.jpg" target="_blank">Pic</a></p>
                                            <p><b>PS: </b> My English name is Jason. I am a <a href="pub/BJJ_BlueBelt_Certificate.jpg" target="_blank">Blue Belt</a> in BJJ under <a href="pub/BJJ_BlueBelt_Ceremony.jpeg" target="_blank">Prof. Leandro Thomas Issa da Silva</a> of <a href="https://evolve-mma.com/" target="_blank">Evolve MMA</a> Singapore.</a> <a href="pub/BJJ.jpg" target="_blank">Pic</a></p>
                                    </div>
                                    
                                </div>
                            </div>        
                        </div>
                    </div>

                    <div class="pagecontents">
                        <div class="section color-1">
                            <div class="section-container">
                                <div class="row">
                                    <div id="education" class="col-md-5 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Education</h3>
                                        </div>
                                        <ul class="ul-card">
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">Ph.D.</span>
                                                    <span class="year">2016 - 2019 (Expected)</span>
                                                </div>
                                                <div class="description">
                                                    <p>Department of Electrical and Computer Engineering, Faculty of Engineering</p>
                                                    <p class="where">National University of Singapore</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">Ph.D.</span>
                                                    <span class="year">2014 - 2015</span>
                                                </div>
                                                <div class="description">
                                                    <p>School of Computer</p>
                                                    <p class="where">National University of Defense Technology， China</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">M.Eng.</span>
                                                    <span class="year">2012 - 2014</span>
                                                </div>
                                                <div class="description">
                                                    <p>School of Computer</p>
                                                    <p class="where">Ss. National University of Defense Technology, China</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">B.Eng.</span><span class="year">2008 - 2012</span>
                                                </div>
                                                <div class="description">
                                                    <p >School of Automation Science and Electrical Engineering</p>
                                                    <p class="where">Ss. Beihang University, China</p>
                                                </div>
                                            </li>
                                        </ul>
                                    </div>
                                    <div id="work" class="col-md-5">
                                        <div class="title text-center">
                                            <h3>Work Experience</h3>
                                            <i class="quote">&nbsp;</i>
                                        </div>
                                        <ul class="ul-card">
					     <li>
                                                <div class="dy">
                                                    <span class="year">2018 - 2019</span>
                                                </div>
                                                <div class="description">
                                                    <p>"Texpert" Research Scientist</p>
                                                    <p class="where">FiT DeepSea AI Lab, Tencent</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="year">2016 - 2018</span>
                                                </div>
                                                <div class="description">
                                                    <p>Research Intern</p>
                                                    <p class="where">Core Technology Group, Learning & Vision, Panasonic R&D Center Singapore</p>
                                                </div>
                                            </li>
					    <li>
                                                <div class="dy">
                                                    <span class="year">2016 - 2017</span>
                                                </div>
                                                <div class="description">
                                                    <p>Graduate Assistant</p>
                                                    <p class="where">NUS Module: <a href="https://myaces.nus.edu.sg/cors/jsp/report/ModuleDetailedInfo.jsp?acad_y=2014/2015&sem_c=1&mod_c=EE2024" target="_blank">EE2024 PROGRAMMING FOR COMPUTER INTERFACES</a></p>
                                                </div>
                                            </li>
                                             <li>
                                                <div class="dy">
                                                    <span class="year">2011 - 2012</span>
                                                </div>
                                                <div class="description">
                                                    <p>Research Intern</p>
                                                    <p class="where">China Aerospace Science and Industry Corporation (CASIC)</p>
                                                </div>
                                            </li>

                                        </ul>
                                    </div>
                                </div>    
                            </div>
                                
                        </div>

                        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Publications</h3>
                                        </div>
                                        <ul class="timeline">
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-Prototype Networks for Unconstrained Set-based Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       In this paper, we study the challenging unconstrained set-based face recognition problem where each subject face is instantiated by a set of media (images and videos) instead of a single image. Naively aggregating information from all the media within a set would suffer from the large intraset variance caused by heterogeneous factors (e.g., varying media modalities, poses and illuminations) and fail to learn discriminative face representations. A novel MultiPrototype Network (MPNet) model is thus proposed to learn multiple prototype face representations adaptively from the media sets. Each learned prototype is representative for the subject face under certain condition in terms of pose, illumination and media modality. Instead of handcrafting the set partition for prototype learning, MPNet introduces a Dense SubGraph (DSG) learning sub-net that implicitly untangles inconsistent media and learns a number of representative prototypes. Qualitative and quantitative experiments clearly demonstrate superiority of the proposed model over state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Xiaoguang Tu, Fang Zhao, Yuan Xin, Junliang Xing, Hengzhu Liu, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>Under Review&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1902.04755.pdf">PDF</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
				            <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Face anti-spoofing (a.k.a presentation attack detection) has drawn growing attention due to the high security demand in face authentication systems. Existing CNN-based approaches usually well recognize the spoofing faces when training and testing spoofing samples display similar patterns, but their performance would drop drastically on testing spoofing faces of unseen scenes. In this paper, we try to boost the generalizability and applicability of these methods by designing a CNN model with two major novelties. First, we propose a simple yet effective Total Pairwise Confusion (TPC) loss for CNN training, which enhances the generalizability of the learned Presentation Attack (PA) representations. Secondly, we incorporate a Fast Domain Adaptation (FDA) component into the CNN model to alleviate negative effects brought by domain changes. Besides, our proposed model, which is named Generalizable Face Authentication CNN (GFA-CNN), works in a multi-task manner, performing face anti-spoofing and face recognition simultaneously. Experimental results show that GFA-CNN outperforms previous face anti-spoofing approaches and also well preserves the identity information of input face images.
                                                            <p>Xiaoguang Tu, <b>Jian Zhao</b>, Mei Xie, Guodong Du, Hengsheng Zhang, Jianshu Li, Zheng Ma, and Jiashi Feng</p>
                                                            <p>Under Review&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1901.05602.pdf">PDF</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Task Relation Networks</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Multi-task learning is popular in machine learning and computer vision. In multitask learning, properly modeling task relations is important for boosting the performance of jointly learned tasks. Task covariance modeling has been successfully used to model the relations of tasks but is limited to homogeneous multi-task learning. In this paper, we propose a feature based task relation modeling approach, suitable for both homogeneous and heterogeneous multi-task learning. First, we propose a new metric to quantify the relations between tasks. Based on the quantitative metric, we then develop the task relation layer, which can be combined with any deep learning architecture to form task relation networks to fully exploit the relations of different tasks in an online fashion. Benefiting from the task relation layer, the task relation networks can better leverage the mutual information from the data. We demonstrate our proposed task relation networks are effective in improving the performance in both homogeneous and heterogeneous multi-task learning settings through extensive experiments on computer vision tasks.
                                                            <p>Jianshu Li, Pan Zhou, Yunpeng Chen, <b>Jian Zhao</b>, Sujoy Roy, Yan Shuicheng, Jiashi Feng, and Terence Sim</p>
                                                            <p>WACV 2019&nbsp;</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recognizing faces in the wild.
                                                            <p><b>Jian Zhao</b>, Yu Cheng, Yi Cheng, Yang Yang, Haochong Lan, Fang Zhao, Lin Xiong, Yan Xu, Jianshu Li, Sugiri Pranata, Shengmei Shen, Junliang Xing, Hengzhu Liu, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>AAAI 2019 (Oral)&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1809.00338.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Look+Across+Elapse%3A+Disentangled+Representation+Learning+and+Photorealistic+Cross-Age+Face+Synthesis+for+Age-Invariant+Face+Recognition&btnG=#d=gs_cit&p=&u=%2Fscholar%3Fq%3Dinfo%3AkfCI2TPozcgJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Dzh-CN">BibTeX</a>, <a target="_blank" href="https://github.com/ZhaoJ9014/High_Performance_Face_Recognition/tree/master/src/Look%20Across%20Elapse-%20Disentangled%20Representation%20Learning%20and%20Photorealistic%20Cross-Age%20Face%20Synthesis%20for%20Age-Invariant%20Face%20Recognition.TensorFlow">Code</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Object Relation Detection Based on One-shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Detecting the relations among objects, such as "cat on sofa" and "person ride horse", is a crucial task in image understanding, and beneficial to bridging the semantic gap between images and natural language. Despite the remarkable progress of deep learning in detection and recognition of individual objects, it is still a challenging task to localize and recognize the relations between objects due to the complex combinatorial nature of various kinds of object relations. Inspired by the recent advances in one-shot learning, we propose a simple yet effective Semantics Induced Learner (SIL) model for solving this challenging task. Learning in one-shot manner can enable a detection model to adapt to a huge number of object relations with diverse appearance effectively and robustly. In addition, the SIL combines bottom-up and top-down attention mech- anisms, therefore enabling attention at the level of vision and semantics favorably. Within our proposed model, the bottom-up mechanism, which is based on Faster R-CNN, proposes objects regions, and the top-down mechanism selects and integrates visual features according to semantic information. Experiments demonstrate the effectiveness of our framework over other state-of-the-art methods on two large-scale data sets for object relation detection.
                                                            <p>Li Zhou, <b>Jian Zhao</b>, Jianshu Li, Li Yuan, and Jiashi Feng</p>
                                                            <p>Under Review&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1807.05857.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Object+Relation+Detection+Based+on+One-shot+Learning&btnG=#d=gs_cit&p=&u=%2Fscholar%3Fq%3Dinfo%3A4I7gw3DQI9gJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Dzh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">3D-Aided Dual-Agent GANs for Unconstrained Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Synthesizing realistic profile faces is beneficial for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by augmenting the number of samples with extreme poses and avoiding costly annotation work. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator’s output using unlabeled real faces while preserving the identity information during the realism refinement. The dual agents are specially designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose, texture as well as identity, and stabilize the training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only achieves outstanding perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A and CFP unconstrained face recognition benchmarks. In addition, the proposed DA-GAN is also a promising new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our winning entry to the NIST IJB-A face recognition competition in which we secured the 1st places on the tracks of verification and identification.
                                                            <p><b>Jian Zhao</b>, Lin Xiong, Jianshu Li, Junliang Xing, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>T-PAMI (IF: 9.455)&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/8417439">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=3D-Aided+Dual-Agent+GANs+for+Unconstrained+Face+Recognition&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Dynamic Conditional Networks for Few-Shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       This paper proposes a novel Dynamic Conditional Convolutional Network (DCCN) to handle conditional few-shot learning, i.e, only a few training samples are available for each condition. DCCN consists of dual subnets: DyConvNet contains a dynamic convolutional layer with a bank of basis filters; CondiNet predicts a set of adaptive weights from conditional inputs to linearly combine the basis filters. In this manner, a specific convolutional kernel can be dynamically obtained for each conditional input. The filter bank is shared between all conditions thus only a low-dimension weight vector needs to be learned. This significantly facilitates the parameter learning across different conditions when training data are limited. We evaluate DCCN on four tasks which can be formulated as conditional model learning, including specific object counting, multi-modal image classification, phrase grounding and identity based face generation. Extensive experiments demonstrate the superiority of the proposed model in the conditional few-shot learning setting.
                                                            <p>Fang Zhao, <b>Jian Zhao</b>, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
                                                            <p>ECCV 2018&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper.pdf">PDF</a>, <a target="_blank" href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Dynamic+Conditional+Networks+for+Few-Shot+Learning&btnG=">BibTeX</a>, <a target="_blank" href="pub/ECCV2018_POSTER.pdf">Poster</a>, <a target="_blank" href="https://github.com/ZhaoJ9014/Dynamic-Conditional-Networks-for-Few-Shot-Learning.pytorch">Code</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database “Multi-Human Parsing (MHP)” for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Yu Cheng, Li Zhou, Terence Sim, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>ACM MM 2018 (<a target="_blank" href="http://www.acmmm.org/2018/awards/">Best Student Paper</a>)&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1804.03287.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Understanding+Humans+in+Crowded+Scenes%3A+Deep+Nested+Adversarial+Learning+and+A+New+Benchmark+for+Multi-Human+Parsing&btnG=">BibTeX</a>, <a href="https://mp.weixin.qq.com/s/0W3AOMTeOyngnM1WA-8C5w" target="_blank">WeChat News</a>, MHP Dataset v2.0, annotation tools, and source codes for NAN and evaluation metrics <a href="https://github.com/ZhaoJ9014/Multi-Human-Parsing_MHP" target="_blank">Download</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-Human Parsing Machines</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Human parsing is an important task in human-centric analysis. Despite the remarkable progress in single-human parsing, the more realistic case of multi-human parsing remains challenging in terms of the data and the model. Compared with the considerable number of available single-human parsing datasets, the datasets for multi-human parsing are very limited in number mainly due to the huge annotation effort required. Besides the data challenge to multi-human parsing, the persons in real-world scenarios are often entangled with each other due to close interaction and body occlusion, making it difficult to distinguish body parts from different person instances. In this paper we propose the Multi-Human Parsing Machines (MHPM), which contains an MHP Montage model and an MHP Solver, to address both challenges in multi-human parsing. Specifically, the MHP Montage model in MHPM generates realistic images with multiple persons together with the parsing labels. It intelligently composes single persons onto background scene images while maintaining the structural information between persons and the scene. The generated images can be used to train better multi-human parsing algorithms. On the other hand, the MHP Solver in MHPM solves the bottleneck of distinguishing multiple entangled persons with close interaction. It employs a Group-Individual Push and Pull (GIPP) loss function, which can effectively separate persons with close interaction. We experimentally show that the proposed MHPM can achieve state-of-the-art performance on the multi-human parsing benchmark and the person individualization benchmark, which distinguishes closely entangled person instances.
                                                            <p>Jianshu Li, <b>Jian Zhao</b>, Yunpeng Chen, Sujoy Roy, Shuicheng Yan, Jiashi Feng, and Terence Sim</p>
                                                            <p>ACM MM 2018&nbsp; <a target="_blank" href="pub/fp0029-liA.pdf">PDF</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">3D-Aided Deep Pose-Invariant Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Learning from synthetic faces, though perhaps appealing for high data efficiency, may not bring satisfactory performance due to the distribution discrepancy of the synthetic and real face images. To mitigate this gap, we propose a 3D-Aided Deep Pose-Invariant Face Recognition Model (3D-PIM), which automatically recovers realistic frontal faces from arbitrary poses through a 3D face model in a novel way. Specifically, 3D-PIM incorporates a simulator with the aid of a 3D Morphable Model (3D MM) to obtain shape and appearance prior for accelerating face normalization learning, requiring less training data. It further leverages a global-local Generative Adversarial Network (GAN) with multiple critical improvements as a refiner to enhance the realism of both global structures and local details of the face simulator’s output using unlabelled real data only, while preserving the identity information. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks clearly demonstrate superiority of the proposed model over state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Lin Xiong, Yu Cheng, Yi Cheng, Jianshu Li, Li Zhou, Yan Xu, Karlekar Jayashree, Sugiri Pranata, Shengmei Shen, Junliang Xing, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>IJCAI 2018 (Oral)&nbsp; <a target="_blank" href="https://www.ijcai.org/proceedings/2018/0165.pdf">PDF</a>, <a href="https://www.ijcai.org/proceedings/2018/165">BibTeX</a>, Foundation of Panasonic FacePRO (YouTube <a href="https://www.youtube.com/watch?v=Z5vxhhM0JGQx">News1</a>, <a href="https://www.youtube.com/watch?v=J_-iRx7z1qQ">News2</a>)</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Towards Pose Invariant Face Recognition in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a "learning to learn" strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Yu Cheng, Yan Xu, Lin Xiong, Jianshu Li, Fang Zhao, Karlekar Jayashree, Sugiri Pranata, Shengmei Shen, Junliang Xing, Shuicheng Yan, and Jiashi Feng</p>
							    <p>CVPR 2018&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.pdf">PDF</a>, <a target="_blank" href="pub/CVPR18.pdf">Poster</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Towards+Pose+Invariant+Face+Recognition+in+the+Wild&btnG=">BibTeX</a>, Foundation of Panasonic FacePRO (YouTube <a href="https://www.youtube.com/watch?v=Z5vxhhM0JGQx">News1</a>, <a href="https://www.youtube.com/watch?v=J_-iRx7z1qQ">News2</a>)</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Weakly Supervised Phrase Localization with Multi-Scale Anchored Transformer Network</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       In this paper, we propose a novel weakly supervised model, Multi-scale Anchored Transformer Network (MATN), to accurately localize free-form textual phrases with only image-level supervision. The proposed MATN takes region proposals as localization anchors, and learns a multi-scale correspondence network to continuously search for phrase regions referring to the anchors. In this way, MATN can exploit useful cues from these anchors to reliably reason about locations of the regions described by the phrases given only image-level supervision. Through differentiable sampling on image spatial feature maps, MATN introduces a novel training objective to simultaneously minimize a contrastive reconstruction loss between different phrases from a single image and a set of triplet losses among multiple images with similar phrases. Superior to existing region proposal based methods, MATN searches for the optimal bounding box over the entire feature map instead of selecting a sub-optimal one from discrete region proposals. We evaluate MATN on the Flickr30K Entities and ReferItGame datasets. The experimental results show that MATN significantly outperforms the state-of-the-art methods.
                                                            <p>Fang Zhao, Jianshu Li, <b>Jian Zhao</b>, and Jiashi Feng</p>
							    <p>CVPR 2018&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Weakly_Supervised_Phrase_CVPR_2018_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Weakly+Supervised+Phrase+Localization+With+Multi-Scale+Anchored+Transformer+Network&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Synthesizing realistic profile faces is promising for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our submissions to NIST IJB-A 2017 face recognition competitions, where we won the 1st places on the tracks of verification and identification.
                                                            <p><b>Jian Zhao</b>, Lin Xiong, Karlekar Jayashree, Jianshu Li, Fang Zhao, Zhecan Wang, Sugiri Pranata, Shengmei Shen, Shuicheng Yan, and Jiashi Feng</p>
							    <p>NIPS 2017&nbsp;<a target="_blank" href="http://papers.nips.cc/paper/6612-dual-agent-gans-for-photorealistic-and-identity-preserving-profile-face-synthesis.pdf">PDF</a>, <a target="_blank" href="pub/ZHAOJIAN_ID70.pdf">Poster</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Dual-agent+gans+for+photorealistic+and+identity+preserving+profile+face+synthesis&btnG=">BibTeX</a>, Foundation of Panasonic FacePRO (YouTube <a href="https://www.youtube.com/watch?v=Z5vxhhM0JGQx">News1</a>, <a href="https://www.youtube.com/watch?v=J_-iRx7z1qQ">News2</a>)</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Robust LSTM-Autoencoders for Face De-Occlusion in the Wild</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers, which is heavily desired in real-world applications concerning surveillance and security. Although much research effort has been devoted to developing face de-occlusion methods, most of them can only work well under constrained conditions, such as all of faces are from a pre-defined closed set of subjects. In this paper, we propose a robust LSTM-Autoencoders (RLA) model to effectively restore partially occluded faces even in the wild. The RLA model consists of two LSTM components, which aims at occlusion-robust face encoding and recurrent occlusion removal respectively. The first one, named multi-scale spatial LSTM encoder, reads facial patches of various scales sequentially to output a latent representation, and occlusion-robustness is achieved owing to the fact that the influence of occlusion is only upon some of the patches. Receiving the representation learned by the encoder, the LSTM decoder with a dual channel architecture reconstructs the overall face and detects occlusion simultaneously, and by feat of LSTM, the decoder breaks down the task of face de-occlusion into restoring the occluded part step by step. Moreover, to minimize identify information loss and guarantee face recognition accuracy over recovered faces, we introduce an identity-preserving adversarial training scheme to further improve RLA. Extensive experiments on both synthetic and real data sets of faces with occlusion clearly demonstrate the effectiveness of our proposed RLA in removing different types of facial occlusion at various locations. The proposed method also provides significantly larger performance gain than other de-occlusion methods in promoting recognition performance over partially-occluded faces.
                                                            <p>Fang Zhao, Jiashi Feng, <b>Jian Zhao</b>, Wenhan Yang, and Shuicheng Yan</p>
                                                            <p>T-IP (IF: 5.071)&nbsp; <a target="_blank" href="http://ieeexplore.ieee.org/document/8101544/">Link</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Robust+LSTM-Autoencoders+for+Face+De-Occlusion+in+the+Wild&btnG=" target="_blank">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Conditional Dual-Agent GANs for Photorealistic and Annotation Preserving Image Synthesis</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
        In this paper, we propose a novel Conditional Dual-Agent GAN (CDA-GAN) for photorealistic and annotation preserving image synthesis, which significantly benefits Deep Convolutional Neural Networks (DCNNs) learning. Instead of merely distinguishing real or fake, the proposed dual agents of the Discriminator are able to preserve both of realism and annotation information simultaneously through a standard adversarial loss and an annotation perception loss. During training, the Generator is conditioned on the desired image features learned by a pre-trained CNN sharing the same architecture of the Discriminator yet different weights. Thus, CDA-GAN is flexible in terms of scalability and able to generate photorealistic image with well preserved annotation information for learning DCNNs in specific domains. We perform detailed experiments to verify the effectiveness of CDA-GAN, which outperforms other state-of-the-arts on MNIST digits classification dataset and IJB-A face recognition dataset.
                                                            <p>Zhecan Wang, <b>Jian Zhao</b>, Yu Cheng, Shengtao Xiao, Jianshu Li, Fang Zhao, Jiashi Feng, and Ashraf Kassim</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
							    <p>BMVC 2017 FaceHUB Workshop (Oral)&nbsp;<a target="_blank" href="pub/0004.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Conditional+Dual-Agent+GANs+for+Photorealistic+and+Annotation+Preserving+Image+Synthesis&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">High Performance Large Scale Face Recognition with Multi-Cognition Softmax and Feature Retrieval</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	To solve this large scale face recognition problem, a Multi-Cognition Softmax Model (MCSM) is proposed to distribute training data to several cognition units by a data shuffling strategy in this paper. Here we introduce one cognition unit as a group of independent softmax models, which is designed to increase the diversity of the one softmax model to boost the performance for models ensemble. Meanwhile, a template-based Feature Retrieval (FR) module is adopted to improve the performance of MCSM by a specific voting scheme. Moreover, a one-shot learning method is applied on collected extra 600K identities due to each identity has one image only. Finally, testing images with lower score from MCSM and FR are assigned new labels with higher score by merging one-shot learning results. Our solution ranks the first place in both two settings of the final evaluation and outperforms other teams by a large margin.
                                                            <p>Yan Xu, Yu Cheng, <b>Jian Zhao</b>, Zhecan Wang, Lin Xiong, Karlekar Jayashree, Hajime Tamura, Tomoyuki Kagaya, Sugiri Pranata, Shengmei Shen, Jiashi Feng, and Junliang Xing</p>
                                                            <p>ICCV 2017 MS-Celeb-1M Workshop (Oral)&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w27/Xu_High_Performance_Large_ICCV_2017_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=High+Performance+Large+Scale+Face+Recognition+with+Multi-Cognition+Softmax+and+Feature+Retrieval&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Know You at One Glance: A Compact Vector Representation for Low-Shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	In this paper, we propose an enforced Softmax optimization approach which is able to improve the model's representational capacity by producing a “compact vector representation” for effectively solving the challenging low-shot learning face recognition problem. Compact vector representations are significantly helpful to overcome the underlying multi-modality variations and remain the primary key features as close to the mean face of the identity as possible in the high-dimensional feature space. Therefore, the gallery facial representations become more robust under various situations, leading to the overall performance improvement for low-shot learning. Comprehensive evaluations on the MNIST, LFW, and the challenging MS-Celeb-1M Low-Shot Learning Face Recognition benchmark datasets clearly demonstrate the superiority of our proposed method over state-of-the-arts.
                                                            <p>Yu Cheng, <b>Jian Zhao</b>, Zhecan Wang, Yan Xu, Karlekar Jayashree, Shengmei Shen, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
							    <p>ICCV 2017 MS-Celeb-1M Workshop (Oral)&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w27/Cheng_Know_You_at_ICCV_2017_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Know+You+at+One+Glance%3A+A+Compact+Vector+Representation+for+Low-Shot+Learning&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Integrated Face Analytics Networks through Cross-Dataset Hybrid Training</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	Face analytics benefits many multimedia applications. It consists of several tasks and most existing approaches generally treat these tasks independently, which limits their deployment in real scenarios. In this paper we propose an integrated Face Analytics Network (iFAN), which is able to perform multiple tasks jointly for face analytics with a novel carefully designed network architecture to fully facilitate the informative interaction among different tasks. The proposed integrated network explicitly models the interactions between tasks so that the correlations between tasks can be fully exploited for performance boost. In addition, to solve the bottleneck of the absence of datasets with comprehensive training data for various tasks, we propose a novel cross-dataset hybrid training strategy. It allows ``plug-in and play'' of multiple datasets annotated for different tasks without the requirement of a fully labeled common dataset for all the tasks. We experimentally show that the proposed iFAN achieves state-of-the-art performance on multiple face analytics tasks using a single integrated model. Specifically, iFAN achieves an overall F-score of 91.15% on the Helen dataset for face parsing, a normalized mean error of 5.81% on the MTFL dataset for facial landmark localization and an accuracy of 45.73% on the BNU dataset for emotion recognition with a single model.
                                                            <p>Jianshu Li, Shengtao Xiao, Fang Zhao, <b>Jian Zhao</b>, Jianan Li, Jiashi Feng, Shuicheng Yan, and Terence Sim</p>
                                                            <p>ACM MM 2017 (Oral) &nbsp; <a target="_blank" href="https://arxiv.org/pdf/1711.06055.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Integrated+Face+Analytics+Networks+through+Cross-Dataset+Hybrid+Training&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-Human Parsing in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Human parsing is attracting increasing research attention. In this work, we aim to push the frontier of human parsing by introducing the problem of multi-human parsing in the wild. Existing works on human parsing mainly tackle single-person scenarios, which deviates from real-world applications where multiple persons are present simultaneously with interaction and occlusion. To address the multi-human parsing problem, we introduce a new multi-human parsing (MHP) dataset and a novel multi-human parsing model named MH-Parser. The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained semantic annotations in an instance-aware setting. The MH-Parser generates global parsing maps and person instance masks simultaneously in a bottom-up fashion with the help of a new Graph-GAN model. We envision that the MHP dataset will serve as a valuable data resource to develop new multi-human parsing models, and the MH-Parser offers a strong baseline to drive future research for multi-human parsing in the wild.
                                                            <p>Jianshu Li, <b>Jian Zhao</b>, Yunchao Wei, Congyan Lang, Yidong Li, Terence Sim, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
                                                            <p>Under review&nbsp; <a href="https://mp.weixin.qq.com/s/tfiPHvkhPW4HDEUzBMseGQ" target="_blank">WeChat News</a>, <a target="_blank" href="https://arxiv.org/pdf/1705.07206.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Towards+Real+World+Human+Parsing%3A+Multiple-Human+Parsing+in+the+Wild&btnG=">BibTeX</a>, MHP Dataset v1.0 <a href="https://github.com/ZhaoJ9014/Multi-Human-Parsing_MHP" target="_blank">Download</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Self-Supervised Neural Aggregation Networks for Human Parsing</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
In this paper, we present a Self-Supervised Neural Aggregation Network (SS-NAN) for human parsing. SS-NAN adaptively learns to aggregate the multi-scale features at each pixel "address". In order to further improve the feature discriminative capacity, a self-supervised joint loss is adopted as an auxiliary learning strategy, which imposes human joint structures into parsing results without resorting to extra supervision. The proposed SS-NAN is end-to-end trainable. SS-NAN can be integrated into any advanced neural networks to help aggregate features regarding the importance at different positions and scales and incorporate rich high-level knowledge regarding human joint structures from a global perspective, which in turn improve the parsing results. Comprehensive evaluations on the recent Look into Person (LIP) and the PASCAL-Person-Part benchmark datasets demonstrate the significant superiority of our method over other state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Xuecheng Nie, Yunpeng Chen, Zhecan Wang, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>CVPR 2017 Visual Understanding of Human in Crowd Scene Workshop (Oral)&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w19/papers/Zhao_Self-Supervised_Neural_Aggregation_CVPR_2017_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=+Self-Supervised+Neural+Aggregation+Networks+for+Human+Parsing&btnG=">BibTeX</a>, <a target="_blank" href="https://github.com/ZhaoJ9014/SS-NAN">Code</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Estimation of Affective Level in the Wild with Multiple Memory Networks</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
This paper presents the proposed solution to the ''affect in the wild'' challenge, which aims to estimate the affective level, i.e. the valence and arousal values, of every frame in a video. A carefully designed deep convolutional neural network (a variation of residual network) for affective level estimation of facial expressions is first implemented as a baseline. Next we use multiple memory networks to model the temporal relations between the frames. Finally ensemble models are used to combine the predictions from multiple memory networks. Our proposed solution outperforms the baseline model by a factor of 10.62% in terms of mean square error (MSE).
                                                            <p>Jianshu Li, Yunpeng Chen, Shengtao Xiao, <b>Jian Zhao</b>, Sujoy Roy, Jiashi Feng, Shuicheng Yan, and Terencei Sim</p>
                                                            <p>CVPR 2017 Faces in-the-wild Workshop (Oral)&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Li_Estimation_of_Affective_CVPR_2017_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Estimation+of+Affective+Level+in+the+Wild+with+Multiple+Memory+Networks&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">A Good Practice Towards Top Performance of Face Recognition: Transferred Deep Feature Fusion</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Unconstrained face recognition performance evaluations have traditionally focused on Labeled Faces in the Wild (LFW) dataset for imagery and the YouTubeFaces (YTF) dataset for videos in the last couple of years. Spectacular progress in this field has resulted in a saturation on verification and identification accuracies for those benchmark datasets. In this paper, we propose a unified learning framework named transferred deep feature fusion targeting at the new IARPA Janus Bechmark A (IJB-A) face recognition dataset released by NIST face challenge. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the LFW and YTF datasets. Inspired by transfer learning, we train two advanced deep convolutional neural networks (DCNN) with two different large datasets in source domain, respectively. By exploring the complementarity of two distinct DCNNs, deep feature fusion is utilized after feature extraction in target domain. Then, template specific linear SVMs is adopted to enhance the discrimination of framework. Finally, multiple matching scores corresponding different templates are merged as the final results. This simple unified framework outperforms the state-of-the-art by a wide margin on IJB-A dataset. Based on the proposed approach, we have submitted our IJB-A results to National Institute of Standards and Technology (NIST) for official evaluation. 
                                                            <p>Lin Xiong, Jayashree Karlekar, <b>Jian Zhao</b>, Jiashi Feng, and Shengmei Shen</p>
                                                            <p><b>(The first three authors are with equal contributions.)</b></p>
                                                            <p>arXiv&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.00438">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=A+Good+Practice+Towards+Top+Performance+of+Face+Recognition%3A+Transferred+Deep+Feature+Fusion&btnG=">BibTeX</a>, Foundation of Panasonic FacePRO (YouTube <a href="https://www.youtube.com/watch?v=Z5vxhhM0JGQx">News1</a>, <a href="https://www.youtube.com/watch?v=J_-iRx7z1qQ">News2</a>)</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Marginalized CNN: Learning Deep Invariant Representations</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Training a deep neural network usually requires sufficient annotated samples. The scarcity of supervision samples in practice thus becomes the major bottleneck on performance of the network. In this work, we propose a principled method to circumvent this difficulty through marginalizing all the possible transformations over samples, termed as Marginalized Convolutional Neural Network (mCNN). mCNN implicitly considers in- finitely many transformed copies of the training data in every training epoch and therefore is able to learn representations invariant for transformation in an end-to-end way. We prove that such marginalization can be understood as a classic CNN with a special form of regularization and thus is efficient for implementation. Experimental results on the MNIST and affNIST digit number datasets demonstrate that mCNN can match or outperform the original CNN with much fewer training samples. Moreover, mCNN also performs well for face recognition on the recently released largescale MS-Cele-1M dataset and outperforms stateof-the-arts. Moreover, compared with the traditional CNNs which use data augmentation to improve their performance, the computational cost of mCNN is reduced by a factor of 25.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Fang Zhao, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>BMVC 2017&nbsp;<a target="_blank" href="https://www.researchgate.net/publication/320223419_Marginalized_CNN_Learning_Deep_Invariant_Representations">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Marginalized+CNN%3A+Learning+Deep+Invariant+Representations&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2016</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Robust Face Recognition with Deep Multi-View Representation Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
This paper describes our proposed method targeting at the MSR Image Recognition Challenge MS-Celeb-1M. The challenge is to recognize one million celebrities from their face images captured in the real world. The challenge provides a large scale dataset crawled from the Web, which contains a large number of celebrities with many images for each subject. Given a new testing image, the challenge requires an identify for the image and the corresponding confidence score. To complete the challenge, we propose a two-stage approach consisting of data cleaning and multi-view deep representation learning. The data cleaning can effectively reduce the noise level of training data and thus improves the performance of deep learning based face recognition models. The multi-view representation learning enables the learned face representations to be more specific and discriminative. Thus the difficulties of recognizing faces out of a huge number of subjects are substantially relieved. Our proposed method achieves a coverage of 46.1% at 95% precision on the random set and a coverage of 33.0% at 95% precision on the hard set of this challenge.
                                                            <p>Jianshu Li, <b>Jian Zhao</b>, Fang Zhao, Hao Liu，Jing Li, Shengmei Shen, Jiashi Feng, and Terence Sim</p>
                                                            <p>ACM MM 2016 (Oral)&nbsp; <a target="_blank" href="https://www.researchgate.net/profile/Jian_Zhao68/publication/308833661_Robust_Face_Recognition_with_Deep_Multi-View_Representation_Learning/links/59d5a3560f7e9b7a7e4675fe/Robust-Face-Recognition-with-Deep-Multi-View-Representation-Learning.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Robust+Face+Recognition+with+Deep+Multi-View+Representation+Learning&btnG=" target="_blank">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2015</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">BE-SIFT: A More Brief and Efficient SIFT Image Matching Algorithm for Computer Vision</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Yiliu Feng, Shandong Yuan, and Wanzeng Cai</p>
                                                            <p>IEEE PICOM2015; <a target="_blank" href="http://ieeexplore.ieee.org/abstract/document/7363122/">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=BE-SIFT%3A+A+More+Brief+and+Efficient+SIFT+Image+Matching+Algorithm+for+Computer+Vision&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2014</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Realization and Design of A Pilot Assist Decision-Making System Based on Speech Recognition</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, and Zhengfa Liang</p>
                                                            <p>AIAA2014&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1406.2775.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Realization+and+Design+of+A+Pilot+Assist+Decision-Making+System+Based+on+Speech+Recognition&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">A New Efficient Key Technology for Space Telemetry Wireless Data Link: The Low-Complexity SC-CPM SC-FDE Algorithm</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, Botao Zhang, and Li Zhou</p>
                                                            <p>ICT2014&nbsp; <a target="_blank" href="http://ieeexplore.ieee.org/document/6913690/">Link</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=+A+New+Efficient+Key+Technology+for+Space+Telemetry+Wireless+Data+Link%3A+The+Low-Complexity+SC-CPM+SC-FDE+Algorithm&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">A New Technology for MIMO Detection: The μ Quantum Genetic Sphere Decoding Algorithm</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, and Ting Chen</p>
                                                            <p>ACA2014&nbsp; <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-662-44491-7_18">Link</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=A+New+Technology+for+MIMO+Detection%3A+The+%CE%BC+Quantum+Genetic+Sphere+Decoding+Algorithm&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">Research on A Kind of Optimization Scheme of MIMO-OFDM Sphere Equalization Technology for Unmanned Aerial Vehicle Wireless Image Transmission Data Link System</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, and Shandong Yuan</p>
                                                            <p>ACA2014&nbsp; <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-662-44491-7_19">Link</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=+Research+on+A+Kind+of+Optimization+Scheme+of+MIMO-OFDM+Sphere+Equalization+Technology+for+Unmanned+Aerial+Vehicle+Wireless+Image+Transmission+Data+Link+System&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">Design and Implementation for A New Kind of Extensible Digital Communication Simulation System Based on Matlab</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, Botao Zhang, and Ting Chen</p>
                                                            <p>Journal of Northerneastern University&nbsp;</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>

                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
			<div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Special Mention</h3>
                                        </div>
                                        <ul type=disc>
					     <li> I was invited by UBTECH to deliver a talk "Deep Learning for Human-Centric Image Understanding" on 08th January 2019.
					     <li> I was invited by OmniVision to deliver a talk "Facial Analytics" on 16th November 2018.
					     <li> I was invited by Jiang Men to deliver a talk "Deep Learning for Human-Centric Image Understanding" on 30th August 2018 (<a target="_blank" href="https://mp.weixin.qq.com/s/nZcGJZwXmJvvB72EnBtsCA">Link</a>, <a target="_blank" href="pub/JiangMen_Poster.jpeg">Poster</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/BZsopAYrUadQRdwj-JwZKA">Summary</a>).
					     <li> I delivered a VALSE Webinar talk "Deep Learning for Human-Centric Image Understanding" on 22nd August 2018 (<a target="_blank" href="https://mp.weixin.qq.com/s/ZKjR-l3pz06TbL5URHJGWw">Link</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/jEFv4mfimKZW5EIahPATIg">Summary</a>).	
					     <li> I represented our group to "Launch of NUS new Vision, Mission and Values" at University Culture Center on 15th August 2018, and presented our recent work on Facial Analytics to NUS President <a href="http://president.nus.edu.sg/biography.php" target="_blank">Prof. Tan Eng Chye</a>. <a href="https://www.instagram.com/p/BmhlvOvg3be/?utm_source=ig_share_sheet&igshid=56r2rgyrhc5v" target="_blank">NUS Instagram</a>, <a href="pub/Instagram.jpeg" target="_blank">NUS News</a>, <a href="pub/20180815.jpeg" target="_blank">Gallery</a>
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
			    
                        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Selected Awards</h3>
                                        </div>
                                        <ul type=disc>
					     <li> Excellent Graduate Award (<2%), National University of Defense Technology, 2019.
					     <li> Best Student Paper Award, 1st author, ACM MM 2018. <a href="pub/ACM MM18_Best Student Paper.pdf" target="_blank">Award Certificate</a>, <a target="_blank" href="http://www.acmmm.org/2018/awards/">ACM Official News</a>, <a target="_blank" href="https://www.eng.nus.edu.sg/2018/lv-team-wins-the-best-student-paper-award-at-acm-mm-2018/">NUS FOE News</a>, <a target="_blank" href="http://ece.nus.edu.sg/drupal/?q=node/25">NUS ECE Announcement</a>, <a target="_blank" href="http://ece.nus.edu.sg/drupal/node/243">NUS ECE News</a>
					     <li> 1st-Place Award, 1st author, MS-Celeb-1M face recognition <a target="_blank" href="http://www.msceleb.org/leaderboard/iccvworkshop-c1">hard set</a>/<a target="_blank" href="http://www.msceleb.org/leaderboard/iccvworkshop-c1">random set</a>/<a target="_blank" href="http://www.msceleb.org/leaderboard/c2">low-shot learning</a> challenges with ICCV 2017. <a href="http://mp.weixin.qq.com/s/-G94Mj-8972i2HtEcIZDpA" target="_blank">WeChat News</a>, <a target="_blank" href="http://ece.nus.edu.sg/drupal/?q=node/215">NUS ECE News</a>, <a href="pub/ECE_Poster.jpeg" target="_blank">NUS ECE Poster</a>, <a href="pub/MS-Track1.jpeg" target="_blank">Award Certificate for Track-1</a>, <a href="pub/MS-Track2.jpeg" target="_blank">Award Certificate for Track-2</a>, <a href="pub/MS-Awards.jpeg" target="_blank">Award Ceremony</a>
					     <li> 2nd-Place Award, 1st author, L.I.P human <a target="_blank" href="http://hcp.sysu.edu.cn/lip/leaderboard.php">parsing/pose</a> estimation challenges with CVPR 2017. <a href="pub/LIP_Certificate_Parsing.pdf" target="_blank">Award Certificate for Parsing</a>, <a href="pub/LIP_Certificate_Pose.pdf" target="_blank">Award Certificate for Pose</a>, <a href="pub/LIP-Awards.jpeg" target="_blank">Award Ceremony</a>
					     <li> 1st-Place Award, 1st author, NIST IJB-A unconstrained face <a href="pub/IJBA_11_report.pdf" target="_blank">verification</a>/<a href="pub/IJBA_1N_report.pdf" target="_blank">identification</a> challenges, 2017. <a href="https://mp.weixin.qq.com/s/s9H_OXX-CCakrTAQUFDm8g" target="_blank">WeChat News</a>
					     <li> 3rd-Place Award, 2nd author, MS-Celeb-1M face recognition <a target="_blank" href="https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/">hard set</a> challenge with ACM MM 2016.
                                             <li> Excellent Student Award (<10%), School of Computer, National University of Defense Technology, 2016.
                                             <li> Excellent Student Award (<10%), School of Computer, National University of Defense Technology, 2015.
　　                                          <li> Excellent Graduate Award (<2%), National University of Defense Technology, 2014.
                                             <li> Excellent Student Award (<2%), National University of Defense Technology, 2014. <a href="pub/14ExcellentStudentAward.jpeg" target="_blank">Award Certificate</a>
                                             <li> Guanghua Fellowship (<2%), National University of Defense Technology, 2014. <a href="pub/Guanghua.jpeg" target="_blank">Award Certificate</a>
                                             <li> Contribution Prize, for Engineering Implementation of Tianhe-2 supercomputer (No.1 on Top500, Jun, 2013), National University of Defense Technology, 2013. <a href="pub/13Tianhe.jpeg" target="_blank">Award Certificate</a>
					     <li> 3rd-Place Award, 13th "Great Wall Information Cup" challenge, National University of Defense Technology, 2013.
                                             <li> Excellent Student Award (<10%), School of Computer, National University of Defense Technology, 2013. <a href="pub/13ExcellentStudentAward.jpeg" target="_blank">Award Certificate</a>
                                             <li> 1st-Place Award, Big Data Processing and Information Sub-Forum of the 6th Graduate Innovation Forum, Provincial Education Department of Hunan Province, 2013. <a href="pub/13FirstPrize.jpeg" target="_blank">Award Certificate</a>
                                             <li> Excellent Graduate Award (<2%), Beihang University, 2012. <a href="pub/12ExcellentGraduate.jpeg" target="_blank">Award Certificate</a>
                                             <li> 2nd-Place Award, 5th Student Research Training Program (SRTP), Beihang University, 2012. <a href="pub/SRTP.jpeg" target="_blank">Award Certificate</a>
                                             <li> National Endeavor Fellowship, Central Government & Beijing Government of China, 2011.
                                             <li> 3rd-Place Award, "Feng Ru Cup" challenge, School of Automation Science and Electrical Engineering, Beihang University, 2011. <a href="pub/11Fengru.jpeg" target="_blank">Award Certificate</a>
                                             <li> SMC Fellowship, Beihang University, 2010.
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
                            
                        <div class="pagecontents">
                            <div class="section color-1">
                                <div class="section-container">
                                    <div id="contact" class="row">
                                        <div  class="col-md-10 col-md-offset-1">
                                            <div class="title text-center">
                                                <h3>Contact</h3> 
                                            </div>
                                            <p><b>Email</b>: zhaojian90 (at) u (dot) nus (dot) edu</p>
                                            <p><b>Phone</b>: (65) 9610 7176 </p>
                                            <p><b>Address</b>: Vision and Machine Learning Lab, E4-#08-24, 4 Engineering Drive 3, National University of Singapore, Singapore 117583 </p>
<p style="margin-top:25px;font-size:smaller;text-align:center">Modified: 06 March 2019</p>                            
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5o66f75yvk2&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33&amp;s=170" async="async"></script>
 </div>
                                    </div>
                                </div>

                            </div>
                    </div>
                </div>
        </div>
</div>        </div>

<script>
  mixpanel.track("Page zhaoj9014.github.io load");
</script>
</body>
</html>
