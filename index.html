<!DOCTYPE html>
<html lang="en" class=" js no-touch csstransitions" style=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>ZHAO Jian - Deep Learning & Computer Vision, NUS </title>
        <link href="images/favicon.png" rel="shortcut icon" type="image/x-icon" />
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        <meta name="google-site-verification" content="tXGinevmz43rJKRTfCj6v5f8zN91tcmg6fdrrYUn-gI" />
        <!--[if lt IE 9]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">
        <link rel="stylesheet" href="css/academicons.min.css">
        <link rel="stylesheet" href="css/style.css?v6">
        <script type="text/javascript" src="./js/jquery-1.10.2.js"></script>
        <script type="text/javascript" src="./js/TweenMax.min.js"></script>
        <script type="text/javascript" src="./js/modernizr.custom.63321.js"></script>
        <script type="text/javascript" src="./js/bootstrap.min.js"></script>
        <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
        <script type="text/javascript" src="./js/custom.js"></script>
        <!-- start Mixpanel --><script type="text/javascript">(function(e,a){if(!a.__SV){var b=window;try{var c,l,i,j=b.location,g=j.hash;c=function(a,b){return(l=a.match(RegExp(b+"=([^&]*)")))?l[1]:null};g&&c(g,"state")&&(i=JSON.parse(decodeURIComponent(c(g,"state"))),"mpeditor"===i.action&&(b.sessionStorage.setItem("_mpcehash",g),history.replaceState(i.desiredHash||"",e.title,j.pathname+j.search)))}catch(m){}var k,h;window.mixpanel=a;a._i=[];a.init=function(b,c,f){function e(b,a){var c=a.split(".");2==c.length&&(b=b[c[0]],a=c[1]);b[a]=function(){b.push([a].concat(Array.prototype.slice.call(arguments,
0)))}}var d=a;"undefined"!==typeof f?d=a[f]=[]:f="mixpanel";d.people=d.people||[];d.toString=function(b){var a="mixpanel";"mixpanel"!==f&&(a+="."+f);b||(a+=" (stub)");return a};d.people.toString=function(){return d.toString(1)+".people (stub)"};k="disable time_event track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config reset people.set people.set_once people.increment people.append people.union people.track_charge people.clear_charges people.delete_user".split(" ");
for(h=0;h<k.length;h++)e(d,k[h]);a._i.push([b,c,f])};a.__SV=1.2;b=e.createElement("script");b.type="text/javascript";b.async=!0;b.src="undefined"!==typeof MIXPANEL_CUSTOM_LIB_URL?MIXPANEL_CUSTOM_LIB_URL:"file:"===e.location.protocol&&"//cdn.mxpnl.com/libs/mixpanel-2-latest.min.js".match(/^\/\//)?"https://cdn.mxpnl.com/libs/mixpanel-2-latest.min.js":"//cdn.mxpnl.com/libs/mixpanel-2-latest.min.js";c=e.getElementsByTagName("script")[0];c.parentNode.insertBefore(b,c)}})(document,window.mixpanel||[]);
mixpanel.init("ebfda33a74d82c326f768bb95f1db5ab");</script><!-- end Mixpanel -->
    </head>
    <body>

        <div id="wrapper">
            <a class="mobilemenu"><i class="fa fa-bars"></i></a>

            <div id="sidebar">
                <div id="main-nav" class="ps-container">
                    <div id="nav-container">
                        <div id="profile" class="clearfix">
                            <div class="portrait hidden-xs"></div>
                            <div class="title">
                                <h2>ZHAO Jian</h2>
                                <h3>National University of Singapore</h3>
                            </div>
                            
                        </div>
                        <ul id="navigation">
                            <li class="currentmenu">
                              <a href="#biography">
                                  <div class="icon"><i class="fa fa-user"></i></div>
                                <div class="text">About Me</div>
                              </a>
                            </li>  
                            <li>
                              <a href="#education">
                                <div class="icon"><i class="fa fa-university"></i></div>
                                <div class="text">Education</div>
                              </a>
                            </li>

                            <li>
                              <a href="#work">
                                <div class="icon"><i class="fa fa-suitcase"></i></div>
                                <div class="text">Work Experience</div>
                              </a>
                            </li>

                            <li>
                                <a href="#publications">
                                    <div class="icon"><i class="fa fa-book"></i></div>
                                    <div class="text">Publications</div>
                                </a>
                            </li>

                            <li>
                              <a href="#contact">
                                  <div class="icon"><i class="fa fa-envelope"></i></div>
                                  <div class="text">Contact</div>
                              </a>
                            </li>

                            <li class="external CV">
                              <a target="_blank"  href="pub/CV.pdf">
                                  <div class="icon"><i class="fa fa-download"></i></div>
                                  <div class="text">Download CV</div>
                              </a>
                            </li>
                        </ul>
                    </div>        
                </div>
                    <div class="social-icons">
                        <ul>
                            <li><a href="https://github.com/ZhaoJ9014" target="_blank"><i class="fa fa-github"></i></a></li>
                            <li><a href="https://www.linkedin.com/in/jian-zhao-951089140" target="_blank"><i class="fa fa-linkedin"></i></a></li>
                            <li><a href="https://scholar.google.com.sg/citations?hl=en&user=zdhRJCkAAAAJ&view_op=list_works&gmla=AJsN-F4PURIx5GMQHVpprJJBjTsNC62YCHjxGsKOwVhrkZ1aJsLgBiuKPBbAgbdcE5_KNw3OnLQgOVSjlqmS6gc-6ti0M2K5o-klHgoOywFCbdaaGnpis130zvgoZFJkVfmoNKpo8Krp" target="_blank"><i class="ai ai-google-scholar"></i></a></li>
                        </ul>
                    </div>
            </div>

            <div id="main">
            
                <div class="page home" data-pos="home">
                    <div id="biography"  class="pageheader">
                        <div class="headercontent">
                            <div class="section-container">
                                
                                <div class="row">
                                    <div class="col-sm-2 visible-sm"></div>
                                    <div class="col-sm-8 col-md-5">
                                        <div class="biothumb">
                                            <img alt="ZHAO Jian, deep learning and computer vision Ph.D. candidate" title="ZHAO Jian, deep learning and computer vision Ph.D. candidate" src="images/Ilija_Ilievski.jpg" class="img-responsive">
                                            <div class="overlay">
                                                
                                                <h1 class="">ZHAO Jian</h1>
                                                <ul class="list-unstyled">
                                                    <li>Department of Electrical and Computer Engineering</li>
                                                    <li>Faculty of Engineering</li>
                                                    <li>National University of Singapore</li>
                                                </ul>
                                            </div> 
                                            
                                               
                                        </div>
                                        
                                    </div>
                                    <div class="clearfix visible-sm visible-xs"></div>
                                    <div  class="col-sm-12 col-md-7">
                                        <h3  class="title">Bio
                                         <i class="quote">"聚焦 Knowledge changes fate."</i></h3>

                                            <p>I am a Ph.D. candidate at <a href="http://www.lv-nus.org/" target="_blank">Learning and Vision Group</a>, Department of Electrical and Computer Engineering (ECE), Faculty of Engineering, National University of Singapore (NUS). My main supervisor is
                                                <a href="https://sites.google.com/site/jshfeng/" target="_blank">Assist. Prof. FENG Jiashi</a>
                                                and my co-supervisor is <a href="https://www.ece.nus.edu.sg/stfpage/eleyans/" target="_blank">Assoc. Prof. YAN Shuicheng</a>. I am
                                                generously supported by China Scholarship Council (CSC) and School of Computer, National University of Defense Technology (NUDT), China.
                                                My domestic supervisor of NUDT is Prof. LIU Hengzhu. Currently, I am working on developing Deep Neural Network models for fine-grained image understanding, applied to Face Recognition, Image Generation and Human Parsing.

                                            </p>
                                            <p> From May 2016 to now, I was an intern at Core Technology Group, Learning & Vision, Panasonic R&D Center Singapore, working on a cooperative face recognition project.
                                            </p>
                                            <p>
                                                I have an M.Eng. degree in Computer Science for Signal Processing with a thesis titled "Research on the Equalization Technologies for the Wireless Image Transmission Data Link System Based on the UAV Platform".
                                            </p>
                                            <p><b>Research interests:</b> Artificial Intelligence, Deep Learning and Computer Vision, Unconstrained/Large-Scale/Low-Shot Face Recognition, Image Generation with Adversarial Learning, and Human Parsing</p>
                                            <p><b>Past interests:</b> Signal Processing, Wireless Communication, System Modelling and Simulation</p>
                                            <p><b>Memberships:</b> IEEE student member, IEEE Computer Society student member, Computer Vision Foundation student member, CCF student member</p>
                                            <p><b style="color: red;">Latest: </b>
				            <div><b>06 Sep 2017  &mdash;</b> I have successfully passed my Ph.D. Comprehensive Qualifying Examination (CQE) and Oral Qualifying Examination (OQE), and I am a Ph.D. candidate now. My TAC members are <a href="https://sites.google.com/site/jshfeng/" target="_blank">Assist. Prof. FENG Jiashi</a>, <a href="https://www.ece.nus.edu.sg/stfpage/eleyans/" target="_blank">Assoc. Prof. YAN Shuicheng</a>, <a href="https://www.ece.nus.edu.sg/stfpage/eleclf/" target="_blank">Assoc. Prof. Cheong Loong Fah</a>, and <a href="http://tanrobby.github.io/" target="_blank">Assist. Prof. Robby T. Tan</a>.</div>
				            <div><b>05 Sep 2017  &mdash;</b> I have 1 paper accepted by NIPS 2017 as the 1st author. The 2nd author XIONG Lin will attend NIPS 2017 at Long Beach Convention Center, Long Beach, America on December 04th - December 09th 2017 and present our work there. <a target="_blank" href="pub/ZHAOJIAN_ID70.pdf">Poster</a></div>
					    <div><b>01 Sep 2017  &mdash;</b> I will join the Apple Computer Vision team as an intern from 04/06/2018 to 21/12/2018.</div>
				            <div><b>19 May 2017  &mdash;</b> We are the first to propose a novel and comprehensive Multi-Human Parsing (MHP) benchmark with fine-grained pixel-wise annotations and well designed baselines. I am the 1st author. See <a href="http://arxiv.org/abs/1705.07206" target="_blank">our paper</a> for details.</div>              
                                            </p>
                                            <p><b>Hobbies: </b>Travel, Rollar Skating, Fitness, Mixed Martial Arts (MMA) and Brazilian Jiu-Jitsu (BJJ)</p>
                                            <p><b>PS: </b> My English name is Jason. I am a three-strip white belt in BJJ under Evolve MMA Singapore. </a></p>
                                    </div>
                                    
                                </div>
                            </div>        
                        </div>
                    </div>

                    <div class="pagecontents">
                        <div class="section color-1">
                            <div class="section-container">
                                <div class="row">
                                    <div id="education" class="col-md-5 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Education</h3>
                                        </div>
                                        <ul class="ul-card">
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">Ph.D.</span>
                                                    <span class="year">2015 - 2019 (Expected)</span>
                                                </div>
                                                <div class="description">
                                                    <p>Department of Electrical and Computer Engineering, Faculty of Engineering</p>
                                                    <p class="where">National University of Singapore</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">Ph.D.</span>
                                                    <span class="year">2014 - 2015</span>
                                                </div>
                                                <div class="description">
                                                    <p>School of Computer</p>
                                                    <p class="where">National University of Defense Technology， China</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">M.Eng.</span>
                                                    <span class="year">2012 - 2014</span>
                                                </div>
                                                <div class="description">
                                                    <p>School of Computer</p>
                                                    <p class="where">Ss. National University of Defense Technology, China</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">B.Sc.</span><span class="year">2008 - 2012</span>
                                                </div>
                                                <div class="description">
                                                    <p >School of Automation Science and Electrical Engineering</p>
                                                    <p class="where">Ss. Beihang University, China</p>
                                                </div>
                                            </li>
                                        </ul>
                                    </div>
                                    <div id="work" class="col-md-5">
                                        <div class="title text-center">
                                            <h3>Work Experience</h3>
                                            <i class="quote">&nbsp;</i>
                                        </div>
                                        <ul class="ul-card">
                                            <li>
                                                <div class="dy">
                                                    <span class="year">2015 - 2017</span>
                                                </div>
                                                <div class="description">
                                                    <p>Graduate Assistant</p>
                                                    <p class="where">NUS Module: <a href="https://myaces.nus.edu.sg/cors/jsp/report/ModuleDetailedInfo.jsp?acad_y=2014/2015&sem_c=1&mod_c=EE2024" target="_blank">EE2024 PROGRAMMING FOR COMPUTER INTERFACES</a></p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="year">2016 - 2017</span>
                                                </div>
                                                <div class="description">
                                                    <p>Research Intern</p>
                                                    <p class="where">Core Technology Group, Learning & Vision, Panasonic R&D Center Singapore</p>
                                                </div>
                                            </li>
                                             <li>
                                                <div class="dy">
                                                    <span class="year">2011 - 2012</span>
                                                </div>
                                                <div class="description">
                                                    <p>Software Engineer</p>
                                                    <p class="where">China Aerospace Science and Industry Corporation (CASIC)</p>
                                                </div>
                                            </li>

                                        </ul>
                                    </div>
                                </div>    
                            </div>
                                
                        </div>

                        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Publications</h3>
                                        </div>
                                        <ul class="timeline">
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Synthesizing realistic profile faces is promising for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our submissions to NIST IJB-A 2017 face recognition competitions, where we won the 1st places on the tracks of verification and identification.
                                                            <p><b>Jian Zhao</b>, Lin Xiong, Karlekar Jayashree, Jianshu Li, Fang Zhao, Zhecan Wang, Sugiri Pranata, Shengmei Shen, Shuicheng Yan, and Jiashi Feng</p>
							    <p>NIPS 2017&nbsp;<a target="_blank" href="pub/NIPS_2017.pdf">PDF</a> <a target="_blank" href="pub/ZHAOJIAN_ID70.pdf">Poster</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Conditional Dual-Agent GANs for Photorealistic and Annotation Preserving Image Synthesis</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
        In this paper, we propose a novel Conditional Dual-Agent GAN (CDA-GAN) for photorealistic and annotation preserving image synthesis, which significantly benefits Deep Convolutional Neural Networks (DCNNs) learning. Instead of merely distinguishing real or fake, the proposed dual agents of the Discriminator are able to preserve both of realism and annotation information simultaneously through a standard adversarial loss and an annotation perception loss. During training, the Generator is conditioned on the desired image features learned by a pre-trained CNN sharing the same architecture of the Discriminator yet different weights. Thus, CDA-GAN is flexible in terms of scalability and able to generate photorealistic image with well preserved annotation information for learning DCNNs in specific domains. We perform detailed experiments to verify the effectiveness of CDA-GAN, which outperforms other state-of-the-arts on MNIST digits classification dataset and IJB-A face recognition dataset.
                                                            <p>Zhecan Wang, <b>Jian Zhao</b>, Yu Cheng, Shengtao Xiao, Jianshu Li, Fang Zhao, Jiashi Feng, and Ashraf Kassim</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
							    <p>BMVC 2017 FaceHUB Workshop&nbsp;<a target="_blank" href="pub/0004.pdf">PDF</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">High Performance Large Scale Face Recognition with Multi-Cognition Softmax and Feature Retrieval</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	To solve this large scale face recognition problem, a Multi-Cognition Softmax Model (MCSM) is proposed to distribute training data to several cognition units by a data shuffling strategy in this paper. Here we introduce one cognition unit as a group of independent softmax models, which is designed to increase the diversity of the one softmax model to boost the performance for models ensemble. Meanwhile, a template-based Feature Retrieval (FR) module is adopted to improve the performance of MCSM by a specific voting scheme. Moreover, a one-shot learning method is applied on collected extra 600K identities due to each identity has one image only. Finally, testing images with lower score from MCSM and FR are assigned new labels with higher score by merging one-shot learning results. Our solution ranks the first place in both two settings of the final evaluation and outperforms other teams by a large margin.
                                                            <p>Yan Xu, Yu Cheng, <b>Jian Zhao</b>, Zhecan Wang, Lin Xiong, Karlekar Jayashree, Sugiri Pranata, Shengmei Shen, and Jiashi Feng</p>
                                                            <p>ICCV 2017 MS-Celeb-1M Workshop (Oral)&nbsp; <a target="_blank" href="pub/12.pdf">PDF</a> <a target="_blank" href="http://www.msceleb.org/leaderboard/papers">Link</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Know You at One Glance: A Compact Vector Representation for Low-Shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	In this paper, we propose an enforced Softmax optimization approach which is able to improve the model's representational capacity by producing a “compact vector representation” for effectively solving the challenging low-shot learning face recognition problem. Compact vector representations are significantly helpful to overcome the underlying multi-modality variations and remain the primary key features as close to the mean face of the identity as possible in the high-dimensional feature space. Therefore, the gallery facial representations become more robust under various situations, leading to the overall performance improvement for low-shot learning. Comprehensive evaluations on the MNIST, LFW, and the challenging MS-Celeb-1M Low-Shot Learning Face Recognition benchmark datasets clearly demonstrate the superiority of our proposed method over state-of-the-arts.
                                                            <p>Yu Cheng, <b>Jian Zhao</b>, Zhecan Wang, Yan Xu, Karlekar Jayashree, Shengmei Shen, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
							    <p>ICCV 2017 MS-Celeb-1M Workshop (Oral)&nbsp; <a target="_blank" href="pub/0008.pdf">PDF</a> <a target="_blank" href="http://www.msceleb.org/leaderboard/papers">Link</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Integrated Face Analytics Networks through Cross-Dataset Hybrid Training</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	Face analytics benefits many multimedia applications. It consists of several tasks and most existing approaches generally treat these tasks independently, which limits their deployment in real scenarios. In this paper we propose an integrated Face Analytics Network (iFAN), which is able to perform multiple tasks jointly for face analytics with a novel carefully designed network architecture to fully facilitate the informative interaction among different tasks. The proposed integrated network explicitly models the interactions between tasks so that the correlations between tasks can be fully exploited for performance boost. In addition, to solve the bottleneck of the absence of datasets with comprehensive training data for various tasks, we propose a novel cross-dataset hybrid training strategy. It allows ``plug-in and play'' of multiple datasets annotated for different tasks without the requirement of a fully labeled common dataset for all the tasks. We experimentally show that the proposed iFAN achieves state-of-the-art performance on multiple face analytics tasks using a single integrated model. Specifically, iFAN achieves an overall F-score of 91.15% on the Helen dataset for face parsing, a normalized mean error of 5.81% on the MTFL dataset for facial landmark localization and an accuracy of 45.73% on the BNU dataset for emotion recognition with a single model.
                                                            <p>Jianshu Li, Shengtao Xiao, Fang Zhao, <b>Jian Zhao</b>, Jianan Li, Jiashi Feng, Shuicheng Yan, and Terence Sim</p>
                                                            <p>ACM MM 2017 (Oral) &nbsp;</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Towards Real World Human Parsing: Multiple-Human Parsing in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
The recent progress of human parsing techniques has been largely driven by the availability of rich data resources. In this work, we demonstrate some critical discrepancies between the current benchmark datasets and the real world human parsing scenarios. For instance, all the human parsing datasets only contain one person per image, while usually multiple persons appear simultaneously in a realistic scene. It is more practically demanded to simultaneously parse multiple persons, which presents a greater challenge to modern human parsing methods. Unfortunately, absence of relevant data resources severely impedes the development of multiple-human parsing methods. 
To facilitate future human parsing research, we introduce the Multiple-Human Parsing (MHP) dataset, which contains multiple persons in a real world scene per single image. The MHP dataset contains various numbers of persons (from 2 to 16) per image with 18 semantic classes for each parsing annotation. Persons appearing in the MHP images present sufficient variations in pose, occlusion and interaction. To tackle the multiple-human parsing problem, we also propose a novel Multiple-Human Parser (MH-Parser), which considers both the global context and local cues for each person in the parsing process. The model is demonstrated to outperform the naive "detect-and-parse" approach by a large margin, which will serve as a solid baseline and help drive the future research in real world human parsing.
                                                            <p>Jianshu Li, <b>Jian Zhao</b>, Yunchao Wei, Congyan Lang, Yidong Li, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
                                                            <p>Under review&nbsp; <a target="_blank" href="https://arxiv.org/abs/1705.07206">PDF</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:raPXYF8LiNQJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWaoQ3mE-yjssUo4B8FV10towvBmdPoQW&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Self-Supervised Neural Aggregation Networks for Human Parsing</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
In this paper, we present a Self-Supervised Neural Aggregation Network (SS-NAN) for human parsing. SS-NAN adaptively learns to aggregate the multi-scale features at each pixel "address". In order to further improve the feature discriminative capacity, a self-supervised joint loss is adopted as an auxiliary learning strategy, which imposes human joint structures into parsing results without resorting to extra supervision. The proposed SS-NAN is end-to-end trainable. SS-NAN can be integrated into any advanced neural networks to help aggregate features regarding the importance at different positions and scales and incorporate rich high-level knowledge regarding human joint structures from a global perspective, which in turn improve the parsing results. Comprehensive evaluations on the recent Look into Person (LIP) and the PASCAL-Person-Part benchmark datasets demonstrate the significant superiority of our method over other state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Xuecheng Nie, Yunpeng Chen, Zhecan Wang, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>CVPR 2017 Workshop on Visual Understanding of Human in Crowd Scene (Oral)&nbsp; <a target="_blank" href="http://ieeexplore.ieee.org/abstract/document/8014938/">Link</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:47ItyB17Wb4J:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWaoPCztyZelP8PcAN5s7eTTdKNvkm42K&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Estimation of Affective Level in the Wild with Multiple Memory Networks</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
This paper presents the proposed solution to the ''affect in the wild'' challenge, which aims to estimate the affective level, i.e. the valence and arousal values, of every frame in a video. A carefully designed deep convolutional neural network (a variation of residual network) for affective level estimation of facial expressions is first implemented as a baseline. Next we use multiple memory networks to model the temporal relations between the frames. Finally ensemble models are used to combine the predictions from multiple memory networks. Our proposed solution outperforms the baseline model by a factor of 10.62% in terms of mean square error (MSE).
                                                            <p>Jianshu Li, Yunpeng Chen, Shengtao Xiao, <b>Jian Zhao</b>, Sujoy Roy, Jiashi Feng, Shuicheng Yan, and Terencei Sim</p>
                                                            <p>CVPR Faces in-the-wild 2017 Workshop (Oral)&nbsp; <a target="_blank" href="http://ieeexplore.ieee.org/document/8014978/">Link</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:v3p50-ttCiQJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWaoP2_u4hfoWtBAufHXCUL42IZQK6W5d&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">A Good Practice Towards Top Performance of Face Recognition: Transferred Deep Feature Fusion</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Unconstrained face recognition performance evaluations have traditionally focused on Labeled Faces in the Wild (LFW) dataset for imagery and the YouTubeFaces (YTF) dataset for videos in the last couple of years. Spectacular progress in this field has resulted in a saturation on verification and identification accuracies for those benchmark datasets. In this paper, we propose a unified learning framework named transferred deep feature fusion targeting at the new IARPA Janus Bechmark A (IJB-A) face recognition dataset released by NIST face challenge. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the LFW and YTF datasets. Inspired by transfer learning, we train two advanced deep convolutional neural networks (DCNN) with two different large datasets in source domain, respectively. By exploring the complementarity of two distinct DCNNs, deep feature fusion is utilized after feature extraction in target domain. Then, template specific linear SVMs is adopted to enhance the discrimination of framework. Finally, multiple matching scores corresponding different templates are merged as the final results. This simple unified framework outperforms the state-of-the-art by a wide margin on IJB-A dataset. Based on the proposed approach, we have submitted our IJB-A results to National Institute of Standards and Technology (NIST) for official evaluation. 
                                                            <p>Lin Xiong, Jayashree Karlekar, <b>Jian Zhao</b>, Jiashi Feng, and Shengmei Shen</p>
                                                            <p><b>(The first three authors are with equal contributions.)</b></p>
                                                            <p>arXiv&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.00438">PDF</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:3UaUo8FiORsJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWaoQZZO3CzYhPiNhYwlYVc0kPgQkpUVQ&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Marginalized CNN: Learning Deep Invariant Representations</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Training a deep neural network usually requires sufficient annotated samples. The scarcity of supervision samples in practice thus becomes the major bottleneck on performance of the network. In this work, we propose a principled method to circumvent this difficulty through marginalizing all the possible transformations over samples, termed as Marginalized Convolutional Neural Network (mCNN). mCNN implicitly considers in- finitely many transformed copies of the training data in every training epoch and therefore is able to learn representations invariant for transformation in an end-to-end way. We prove that such marginalization can be understood as a classic CNN with a special form of regularization and thus is efficient for implementation. Experimental results on the MNIST and affNIST digit number datasets demonstrate that mCNN can match or outperform the original CNN with much fewer training samples. Moreover, mCNN also performs well for face recognition on the recently released largescale MS-Cele-1M dataset and outperforms stateof-the-arts. Moreover, compared with the traditional CNNs which use data augmentation to improve their performance, the computational cost of mCNN is reduced by a factor of 25.
                                                            <p><b>ZHAO Jian</b>, LI Jianshu, ZHAO Fang, YAN Shuicheng, and FENG Jiashi</p>
                                                            <p>BMVC 2017&nbsp;<a target="_blank" href="pub/0470.pdf">PDF</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-Prototype Networks for Unconstrained Set-based Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
In this paper, we consider the challenging unconstrainedset-based face recognition problem where each subject faceis  instantiated  by  a  set  of  media  (images  and  videos)  in-stead of a single image.   Traditional face recognition ap-proaches based on single image may not perform well insuch scenarios since they do not exploit media-set informa-tion effectively.  But naively aggregating information fromall the media within a set would suffer from the large intra-set variance caused by heterogeneous factors (e.g., varyingmedia modalities, poses and illuminations) and fail to learndiscriminative face representations.   To address this chal-lenging problem, we propose a novel Multi-Prototype Net-work (MPNet) model that adaptively learns multiple proto-type face representations from sets of media.  Each learnedprototype is representative for the subject face under certainconditions in terms of pose, illumination and media modal-ity.  Instead of handcrafting the set partition for prototypelearning, MPNet introduces a new Dense SubGraph (DSG)learning sub-net that implicitly untangles inconsistent me-dia  and  learns  a  number  of  prototypes  for  unconstrainedset-based face recognition.  The proposed MPNet with theDSG sub-net is end-to-end trainable. Comprehensive evalu-ations on the challenging IJB-A and large-scale MS-Celeb-1M benchmark datasets clearly demonstrate the superiorityof our proposed MPNet over state-of-the-arts.
                                                            <p><b>ZHAO Jian</b>, ZHAO Jiaojiao, LI Jianshu, ZHAO Fang, CHENG Yu, ZHAI Yongping, Jayashree Karlekar, Sugiri Pranata, SHEN Shengmei, YAN Shuicheng, and FENG Jiashi</p>
                                                            <p>Under review&nbsp;</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Weakly Supervised Phrase Localization with Multi-Scale Anchored Transformer Network</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Free-form textual phrase localization in images is extremely challenging when only image-level supervision is available. In this paper, we propose a novel weakly supervised localization model, namely Multi-scale Anchored Transformer Network (MATN) that can accurately localize textual phrases. Through taking region proposals of an image as localization anchors and computing multi-scale correspondence maps between a given phrase and the image spatial feature map, MATN learns to predict phrase location referring to the anchors. These anchors provide useful cues for MATN to reliably reason about the regions where objects most likely appear given only image-level supervision. MATN is trained by a novel strategy that simultaneously minimizes a contrastive reconstruction loss between different phrases from a single image and a set of triplet losses among multiple images with the similar phrases. Compared with other region proposal based methods, MATN searches for the optimal bounding box over the entire feature map instead of selecting a sub-optimal one from discrete region proposals and thus is more resistant to errors in the proposals. Besides, MATN explicitly leverages the shared knowledge across multiple images containing the similar objects and the discriminative information across different phrases from a single image in the learning process, which are absent in previous methods. We evaluate MATN on the Flickr30K Entities and ReferItGame datasets and the experimental results show that MATN significantly outperforms the state-of-the-art methods.
                                                            <p>ZHAO Fang (NUS), LI Jianshu (NUS), <b>ZHAO Jian</b> (NUS), FENG Jiashi (NUS), and YAN Shuicheng (Qihoo/360 AI Institute & NUS)</p>
                                                            <p>Under review&nbsp;</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li> 
                                            <li>
                                                <div class="date">2016</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Robust face recognition with deep multi-view representation learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
This paper describes our proposed method targeting at the MSR Image Recognition Challenge MS-Celeb-1M. The challenge is to recognize one million celebrities from their face images captured in the real world. The challenge provides a large scale dataset crawled from the Web, which contains a large number of celebrities with many images for each subject. Given a new testing image, the challenge requires an identify for the image and the corresponding confidence score. To complete the challenge, we propose a two-stage approach consisting of data cleaning and multi-view deep representation learning. The data cleaning can effectively reduce the noise level of training data and thus improves the performance of deep learning based face recognition models. The multi-view representation learning enables the learned face representations to be more specific and discriminative. Thus the difficulties of recognizing faces out of a huge number of subjects are substantially relieved. Our proposed method achieves a coverage of 46.1% at 95% precision on the random set and a coverage of 33.0% at 95% precision on the hard set of this challenge.
                                                            <p>LI Jianshu (NUS), <b>ZHAO Jian</b> (NUS), ZHAO Fang (NUS), LIU Hao (HeFei University of Technology)， LI Jing (NUS), SHEN Shengmei (Panasonic), FENG Jiashi (NUS), and Terence Sim (NUS)</p>
                                                            <p>ACM MM 2016&nbsp; <a target="_blank" href="pub/face-recog.pdf">PDF</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:WQuuWzzuQNAJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWN_C6XQQaJY-Lwq3ZkyIW-1tpBC23kbR&scisf=4&ct=citation&cd=-1&hl=zh-CN" target="_blank">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2016</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Robust LSTM-Autoencoders for Face De-Occlusion in the Wild</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers which is heavily desired in real-world applications concerning surveillance and security. Although much research effort has been devoted to developing face de-occlusion methods, most of them can only work well under constrained conditions, such as all the faces are from a pre-defined closed set. In this paper, we propose a robust LSTM-Autoencoders (RLA) model to effectively restore partially occluded faces even in the wild. The RLA model consists of two LSTM components, which aims at occlusion-robust face encoding and recurrent occlusion removal respectively. The first one, named multi-scale spatial LSTM encoder, reads facial patches of various scales sequentially to output a latent representation, and occlusion-robustness is achieved owing to the fact that the influence of occlusion is only upon some of the patches. Receiving the representation learned by the encoder, the LSTM decoder with a dual channel architecture reconstructs the overall face and detects occlusion simultaneously, and by feat of LSTM, the decoder breaks down the task of face de-occlusion into restoring the occluded part step by step. Moreover, to minimize identify information loss and guarantee face recognition accuracy over recovered faces, we introduce an identity-preserving adversarial training scheme to further improve RLA. Extensive experiments on both synthetic and real datasets of faces with occlusion clearly demonstrate the effectiveness of our proposed RLA in removing different types of facial occlusion at various locations. The proposed method also provides significantly larger performance gain than other deocclusion methods in promoting recognition performance over partially-occluded faces.
                                                            <p>ZHAO Fang, FENG Jiashi, <b>ZHAO Jian</b>, YANG Wenhan, and YAN Shuicheng</p>
                                                            <p>IEEE Transactions on Image Processing&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1612.08534.pdf">PDF</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:t6A7oHqvWl4J:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWN_CaCt3OMTKAP28okFcposSwuNkk_q3&scisf=4&ct=citation&cd=-1&hl=zh-CN" target="_blank">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2015</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">BE-SIFT: A More Brief and Efficient SIFT Image Matching Algorithm for Computer Vision</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
                                                            <p><b>ZHAO Jian</b> (NUDT), LIU Hengzhu (NUDT), FENG Yiliu (NUDT), YUAN Shandong (NUDT), and CAI Wanzeng (NUDT)</p>
                                                            <p>IEEE PICOM2015; <a target="_blank" href="http://ieeexplore.ieee.org/abstract/document/7363122/">PDF</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:THD9dkxxpTcJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWO-qQHxvsPvmbxD9o49d1nsD7GBis6Lr&scisf=4&ct=citation&cd=-1&hl=zh-CN&scfhb=1">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2014</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Realization and design of a pilot assist decision-making system based on speech recognition</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian ZHAO</b> (NUDT), Hengzhu LIU (NUDT), Xucan CHEN (NUDT), and Zhengfa LIANGANG (NUDT)</p>
                                                            <p>AIAA2014&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1406.2775.pdf">PDF</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:x_BJDKMyVR0J:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWaoRRdaFJY11YfKVkR66ayxjvlpk7UEv&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">A new efficient key technology for space telemetry wireless data link: The low-complexity SC-CPM SC-FDE algorithm</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian ZHAO</b> (NUDT), Hengzhu LIU (NUDT), Xucan CHEN (NUDT), Botao ZHANG (NUDT), and Li ZHOU (NUDT)</p>
                                                            <p>ICT2014&nbsp; <a target="_blank" href="http://ieeexplore.ieee.org/document/6913690/">Link</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:IpZN9yw7MbAJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWaoRfgRI4Kmxn9-9CupWrMpe5Zmq5yAa&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">A New Technology for MIMO Detection: The μ Quantum Genetic Sphere Decoding Algorithm</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian ZHAO</b> (NUDT), Hengzhu LIU (NUDT), Xucan CHEN (NUDT), and Ting CHEN (NUDT)</p>
                                                            <p>ACA2014&nbsp; <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-662-44491-7_18">Link</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:Uhc9ioKKM50J:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWaoRsYo0ozZc3zVc7uyh98MyBI9Ar3p4&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">Research on a kind of optimization scheme of MIMO-OFDM sphere equalization technology for unmanned aerial vehicle wireless image transmission data link system</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian ZHAO</b> (NUDT), Hengzhu LIU (NUDT), Xucan CHEN (NUDT), and Shandong YUAN (NUDT)</p>
                                                            <p>ACA2014&nbsp; <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-662-44491-7_19">Link</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:l-A3jQ6Pt2YJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWaoR7Px5xEXeT3hyhjjnxrMQUVPh6weh&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">Design and Implementation for a New Kind of Extensible Digital Communication Simulation System Based on Matlab</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian ZHAO</b> (NUDT), Hengzhu LIU (NUDT), Xucan CHEN (NUDT), Botao ZHANG (NUDT), and Ting CHEN (NUDT)</p>
                                                            <p>JOURNAL OF NORTHEASTERN UNIVERSITY&nbsp;</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>

                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
                            
                        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Selected Awards</h3>
                                        </div>
                                        <ul type=disc>
					     <li> 2017 No.1 on ICCV 2017 MS-Celeb-1M Large-Scale Face Recognition Hard Set Challenge, Random Set Challenge, and Low-Shot Learning Challenge, 1st author. <a target="_blank" href="http://www.msceleb.org/leaderboard/c2">Link</a>
					     <li> 2017 No.2 on CVPR 2017 Visual Understanding of Humans in Crowd Scene & the 1st Look into Person (L.I.P) Challenges on Human Parsing and Pose Estimation, 1st author. <a target="_blank" href="http://hcp.sysu.edu.cn/lip/leaderboard.php">Link</a>
					     <li> 2017 No.1 on National Institute of Standards and Technology (NIST) IARPA Janus Benchmark A (IJB-A) Unconstrained Face Verification challenge and Idenfication challenge, 1st author. Official reports: <a href="pub/IJBA_11_report.pdf" target="_blank">Verification</a>, <a href="pub/IJBA_1N_report.pdf" target="_blank">Identification</a>.
				             <li> 2017 No.1 on CVPR 2017 Faces in-the-wild challenge, 4th author. <a target="_blank" href="https://ibug.doc.ic.ac.uk/resources/first-faces-wild-workshop-challenge/">Link</a>
					     <li> 2016 No.3 on ACM MM 2016 MS-Celeb-1M Hard set challenge, 2nd author. <a target="_blank" href="https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/">Link</a>
                                             <li> 2016 "Excellent Student Award", School of Computer, National University of Defense Technology (<10%)
                                             <li> 2015 "Excellent Student Award", School of Computer, National University of Defense Technology (<10%)
　　                                          <li> 2014 "Excellent Graduate", National University of Defense Technology (<2%)
                                             <li> 2014 "Excellent Student Award", National University of Defense Technology (<2%)
                                             <li> 2014 "Guanghua Scholarship", National University of Defense Technology (<2%)
                                             <li> 3rd prize on 13th "Great Wall Information Cup" competition, National University of Defense Technology
                                             <li> 2013 "Excellent Student Award", School of Computer, National University of Defense Technology (<10%)
                                             <li> 1st prize on "Big Data Processing and Information Sub-Forum of the 6th Graduate Innovation Forum", The Education Ministry of Hunan Province
                                             <li> 2012 "Excellent Graduate", Beihang University (<2%)
                                             <li> 2nd prize on 5th "Student Research Training Program (SRTP)", Education Ministry of China
                                             <li> 2011 "National Endeavor Scholarship", Central Government & Beijing Government of China
                                             <li> 3rd prize on 21th "FENG RU Cup" Competition, Beihang University
                                             <li> 2010 "SMC Scholarship", Beihang University
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
                            
                        <div class="pagecontents">
                            <div class="section color-1">
                                <div class="section-container">
                                    <div id="contact" class="row">
                                        <div  class="col-md-10 col-md-offset-1">
                                            <div class="title text-center">
                                                <h3>Contact</h3>
                                            </div>
                                            <p><b>Email</b>: <a href="mailto:zhaojian90@u.nus.edu">zhaojian90@u.nus.edu</a></p>
                                            <p><b>Phone</b>: (65) 9610 7176 </p>
                                            <p><b>Address</b>: Vision and Machine Learning Lab, E4-#08-24, 4 Engineering Drive 3, National University of Singapore, Singapore 117583 </p>
<p style="margin-top:25px;font-size:smaller;text-align:center">Modified: 01 November 2017</p>
 </div>

                                    </div>
                                </div>

                            </div>
                    </div>
                </div>
        </div>
</div>        </div>

<script>
  mixpanel.track("Page zhaoj9014.github.io load");
</script>
</body>
</html>
