<!DOCTYPE html>
<html lang="en" class=" js no-touch csstransitions" style=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>Jian ZHAO (赵健)</title>
        <link href="images/favicon.png" rel="shortcut icon" type="image/x-icon" />
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        <meta name="google-site-verification" content="tXGinevmz43rJKRTfCj6v5f8zN91tcmg6fdrrYUn-gI" />
        <!--[if lt IE 9]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">
        <link rel="stylesheet" href="css/academicons.min.css">
        <link rel="stylesheet" href="css/style.css?v6">
        <script type="text/javascript" src="./js/jquery-1.10.2.js"></script>
        <script type="text/javascript" src="./js/TweenMax.min.js"></script>
        <script type="text/javascript" src="./js/modernizr.custom.63321.js"></script>
        <script type="text/javascript" src="./js/bootstrap.min.js"></script>
        <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
        <script type="text/javascript" src="./js/custom.js"></script>
        <!-- start Mixpanel --><script type="text/javascript">(function(e,a){if(!a.__SV){var b=window;try{var c,l,i,j=b.location,g=j.hash;c=function(a,b){return(l=a.match(RegExp(b+"=([^&]*)")))?l[1]:null};g&&c(g,"state")&&(i=JSON.parse(decodeURIComponent(c(g,"state"))),"mpeditor"===i.action&&(b.sessionStorage.setItem("_mpcehash",g),history.replaceState(i.desiredHash||"",e.title,j.pathname+j.search)))}catch(m){}var k,h;window.mixpanel=a;a._i=[];a.init=function(b,c,f){function e(b,a){var c=a.split(".");2==c.length&&(b=b[c[0]],a=c[1]);b[a]=function(){b.push([a].concat(Array.prototype.slice.call(arguments,
0)))}}var d=a;"undefined"!==typeof f?d=a[f]=[]:f="mixpanel";d.people=d.people||[];d.toString=function(b){var a="mixpanel";"mixpanel"!==f&&(a+="."+f);b||(a+=" (stub)");return a};d.people.toString=function(){return d.toString(1)+".people (stub)"};k="disable time_event track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config reset people.set people.set_once people.increment people.append people.union people.track_charge people.clear_charges people.delete_user".split(" ");
for(h=0;h<k.length;h++)e(d,k[h]);a._i.push([b,c,f])};a.__SV=1.2;b=e.createElement("script");b.type="text/javascript";b.async=!0;b.src="undefined"!==typeof MIXPANEL_CUSTOM_LIB_URL?MIXPANEL_CUSTOM_LIB_URL:"file:"===e.location.protocol&&"//cdn.mxpnl.com/libs/mixpanel-2-latest.min.js".match(/^\/\//)?"https://cdn.mxpnl.com/libs/mixpanel-2-latest.min.js":"//cdn.mxpnl.com/libs/mixpanel-2-latest.min.js";c=e.getElementsByTagName("script")[0];c.parentNode.insertBefore(b,c)}})(document,window.mixpanel||[]);
mixpanel.init("ebfda33a74d82c326f768bb95f1db5ab");</script><!-- end Mixpanel -->
    </head>
    <body>

        <div id="wrapper">
            <a class="mobilemenu"><i class="fa fa-bars"></i></a>

            <div id="sidebar">
                <div id="main-nav" class="ps-container">
                    <div id="nav-container">
                        <div id="profile" class="clearfix">
                            <div class="portrait hidden-xs"></div>
                            <div class="title">
                                <h2>Jian ZHAO (赵健)</h2>
                                <h3>China Telecom Institute of AI & Northwestern Polytechnical University</h3>
                            </div>
                            
                        </div>
                        <ul id="navigation">
                            <li class="currentmenu">
                              <a href="#biography">
                                  <div class="icon"><i class="fa fa-user"></i></div>
                                <div class="text">About Me</div>
                              </a>
                            </li>  
                            <li>
                              <a href="#education">
                                <div class="icon"><i class="fa fa-university"></i></div>
                                <div class="text">Education</div>
                              </a>
                            </li>

                            <li>
                              <a href="#work">
                                <div class="icon"><i class="fa fa-suitcase"></i></div>
                                <div class="text">Work Experience</div>
                              </a>
                            </li>

                            <li>
                                <a href="#publications">
                                    <div class="icon"><i class="fa fa-book"></i></div>
                                    <div class="text">Publications</div>
                                </a>
                            </li>

                            <li>
                              <a href="#contact">
                                  <div class="icon"><i class="fa fa-envelope"></i></div>
                                  <div class="text">Contact</div>
                              </a>
                            </li>

                            <li class="external CV">
                              <a target="_blank"  href="pub/CV.pdf">
                                  <div class="icon"><i class="fa fa-download"></i></div>
                                  <div class="text">Download CV</div>
                              </a>
                            </li>
                        </ul>
                    </div>        
                </div>
                    <div class="social-icons">
                        <ul>
                            <li><a href="https://github.com/ZhaoJ9014" target="_blank"><i class="fa fa-github"></i></a></li>
                            <li><a href="https://www.linkedin.com/in/%E5%81%A5-%E8%B5%B5-5546952a1/?locale=en_US" target="_blank"><i class="fa fa-linkedin"></i></a></li>
                            <li><a href="https://scholar.google.com.sg/citations?hl=en&user=zdhRJCkAAAAJ&view_op=list_works&gmla=AJsN-F4PURIx5GMQHVpprJJBjTsNC62YCHjxGsKOwVhrkZ1aJsLgBiuKPBbAgbdcE5_KNw3OnLQgOVSjlqmS6gc-6ti0M2K5o-klHgoOywFCbdaaGnpis130zvgoZFJkVfmoNKpo8Krp" target="_blank"><i class="ai ai-google-scholar"></i></a></li>
                        </ul>
                    </div>
            </div>

            <div id="main">
            
                <div class="page home" data-pos="home">
                    <div id="biography"  class="pageheader">
                        <div class="headercontent">
                            <div class="section-container">
                                
                                <div class="row">
                                    <div class="col-sm-2 visible-sm"></div>
                                    <div class="col-sm-8 col-md-5">
                                        <div class="biothumb">
                                            <img alt="Jian ZHAO (赵健), Researcher on Multimedia Analytics, Vicinagearth Security, and Embodied AI" title="Jian ZHAO (赵健), Researcher on Multimedia Analytics, Vicinagearth Security, and Embodied AI" src="images/Ilija_Ilievski.jpg" class="img-responsive">
                                            <div class="overlay">
                                                
                                                <h1 class="">Jian ZHAO (赵健)</h1>
                                                <ul class="list-unstyled">
                                                    <li>China Telecom Institute of AI & Northwestern Polytechnical University</li>
                                                </ul>
                                            </div> 
                                            
                                               
                                        </div>
                                        
                                    </div>
                                    <div class="clearfix visible-sm visible-xs"></div>
                                    <div  class="col-sm-12 col-md-7">
                                        <h3  class="title">Bio
                                         <i class="quote">"行稳致远 Knowledge changes fate."</i></h3>

                                            <p>I am currently a Young Scientist with China Telecom Institute of AI, Beijing China and a Researcher with School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, Xi'an China. Previously, I was an Assistant Researcher with Academy of Military Sciences, Beijing, China. I received my Ph.D. degree from National University of Singapore (NUS) in 2019 under the supervision of Assist. Prof. <a href="https://sites.google.com/site/jshfeng/" target="_blank">Jiashi Feng</a>, Assoc. Prof. <a href="https://yanshuicheng.ai/" target="_blank">Shuicheng Yan</a>, and Prof. Hengzhu Liu, Master degree from National University of Defense Technology (NUDT) in 2014 under the supervision of Prof. Xucan Chen, and Bachelor degree from Beihang University (BUAA) in 2012 under the supervision of Dr. Shaopeng Dong and Prof. Mei Yuan. I was supported by China Scholarship Council (CSC) and School of Computer, NUDT to pursue my Ph.D. degree at <a href="http://www.lv-nus.org/" target="_blank">Learning and Vision Group</a>, Faculty of Engineering (FOE), Department of Electrical and Computer Engineering (ECE), NUS, Singapore.</p>
                                            <p><b>Research Interests:</b> Multimedia Analytics, Vicinagearth Security, and Embodied AI.</p>
                                            <p><b>Past Interests:</b> Signal Processing, Wireless Communication, System Modeling, and Simulation.</p>
                                            <p><b>Professional Activities:</b> 
					       <div>Senior Area Chair (<a target="_blank" href="https://mp.weixin.qq.com/s/pQrtUaWtO_ep_DlXUKOc2g">list</a>) of VALSE.</div>
					       <div>Committee member of CCF-CV. (<a href="pub/ccfcv21.jpeg" target="_blank">Thank-You Letter</a>)</div>
					       <div>Member of CSIG Committee for Elite Young Professionals. (<a target="_blank" href="http://youth.csig.org.cn/%e5%85%b3%e4%ba%8e/">list</a>)</div>
					       <div>Committee member of CSIG-BVD. (<a href="pub/CSIG.pdf" target="_blank">Invitation Letter</a>, <a href="pub/csigbvd21.jpg" target="_blank">Thank-You Letter</a>)</div>
					       <div>Member of the board of directors of BSIG. (<a href="pub/BSIG.jpg" target="_blank">Certificate</a>)</div>
					       <div>Member of BSIG Committee for Elite Young Professionals. (<a href="pub/bsig.jpeg" target="_blank">Certificate</a>)</div>
					       <div>PaddlePaddle Developer Expert of Baidu. <a target="_blank" href="https://www.paddlepaddle.org.cn/ppdemd?n=/ppdemd/%E8%B5%B5%E5%81%A5">Official Announcement</a></div>
					       <div>Editorial Board Member of <a target="_blank" href="hhttps://elspublishing.com/journals/electronics-and-signal-processing/editorial_board/">Electronics and Signal Processing</a> (<a href="pub/ESP.png" target="_blank">Certificate</a>), <a target="_blank" href="https://journals.bilpubgroup.com/index.php/aia/about/editorialTeam">Artificial Intelligence Advances</a> (<a href="pub/AIA.pdf" target="_blank">Certificate</a>), and <a target="_blank" href="https://ietresearch.onlinelibrary.wiley.com/hub/journal/17519640/homepage/editorial-board">IET Computer Vision</a> (<a href="pub/CVI.pdf" target="_blank">Certificate</a>)</div>
					       <div>Guest Editor of Pattern Recognition Letters special issue on Recent Advances in Deep Learning Model Security (<a target="_blank" href="https://www.journals.elsevier.com/pattern-recognition-letters/call-for-papers/recent-advances-in-deep-learning-model-security">CFP</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/3VxLYJLXolY2D8pADyjYKw">WeChat News1</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/R8SKqLZ8VCLmjGUJbDwhAw">WeChat News2</a>, <a target="_blank" href="https://www.jiqizhixin.com/articles/2021-07-05">WeChat News3</a>), and Electronics special issue on Multimedia Content Analysis, Management and Retrieval: Trends and Challenges. (<a target="_blank" href="https://www.mdpi.com/journal/electronics/special_issues/multimedia_AMR">CFP</a>)</div>
					       <div>Member of Expert Arbitration of "<a target="_blank" href="https://www.tiaozhanbei.net/">Challenge Cup</a>" National Undergraduate Curricular Academic Science and Technology Works by Race. (<a href="pub/challenge-cup.jpeg" target="_blank">Certificate</a>)</div>
					       <div>Member of Expert Committee/Arbitration of China Artificial Intelligence Competition. (<a target="_blank" href="https://ai.xm.gov.cn/">link</a>)</div>
					       <div>Technical Consultant of <a href="pub/jiqizhixin-24.jpg" target="_blank">Synced</a> (2023-Now), Baidu Institute of AI (2020-2022), OPPO Institute (2021-2022), PENSEES (2020-2021).</div>
					       <div>Invited reviewer of NSFC, T-PAMI, IJCV, T-MM, T-IFS, T-CSVT, Neurocomputing, T-CDS, T-AFFC, T-HMS, KNOSYS, IET Computer Vision, CSSP, JVCI, GVC, IMAVIS, ESWA, INFFUS, CVIU, ITS, NeurIPS (one of the <a href="pub/NeurIPS2018.png" target="_blank">top 30%</a> highest-scoring reviewers of NeurIPS 2018), CVPR, ICCV, ACM MM, AAAI, ICLR, ICML, ICME, ACCV, UAI, FG, CICAI, ICASSP.</div>
					       <div>General Chair of the <a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIzMDE3OTE4OQ==&mid=2247483837&idx=1&sn=ae545300cd0eb1c2433677bc0abf38e5&source=41#wechat_redirect">challenge</a> on unconstrained face recognition, suspicious object recognition, and special behavior recognition.</div>
					       <div>Chair of the 1st & 2nd & 3rd Anti-UAV Workshop & Challenge with CVPR 2020, ICCV 2021, and CVPR 2023. (<a target="_blank" href="https://anti-uav.github.io/">Official Website</a>, <a href="pub/anti-uav.pdf" target="_blank">Poster</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/BuK9Lba4taFgEprlbzhAnA">WeChat News1</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/YILT9CWKXg5dVuj-ZqVJJg">WeChat News2</a>, <a target="_blank" href="https://community.modelscope.cn/63e0d79d406cc115977189e5.html">ModelScope News</a>, <a target="_blank" href="https://blog.csdn.net/sunbaigui/article/details/128900807">CSDN News</a>)</div>
					       <div>Chair of the 1st SkatingVerse Workshop & Challenge with FG 2024. (<a target="_blank" href="https://skatingverse.github.io/">Official Website</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/mN839_sR2wsyLT5VhkaYLw">BSIG News</a>, <a target="_blank" href="https://www.jiqizhixin.com/articles/2024-03-21">Synced News</a>)</div>
					       <div>Chair of the "知识与语义驱动的视觉理解" Workshop with VALSE 2021. (<a target="_blank" href="http://valser.org/2021/#/workshopde?id=12">Official Website</a>)</div>
					       <div>Area Chair of the <a target="_blank" href="https://cicai.caai.cn/">CAAI International Conference on Artificial Intelligence (CICAI)</a> 2022 and 2023.</div>
					       <div>Presentation Chair of the <a target="_blank" href="https://cicai.caai.cn/">CAAI International Conference on Artificial Intelligence 2021</a>.</div>
					       <div>Session Chair of the ACM MM 2021. (<a href="pub/MM21SecChair.jpg" target="_blank">Certificate</a>)</div>
					       <div>Session Chair of the <a target="_blank" href="https://event.baai.ac.cn/event/53">ECCV 2020 China Pre-Conference</a>. (<a href="pub/SC-ECCV2020.jpeg" target="_blank">Certificate</a>)</div>
					       <div>Workshop Chair of the <a target="_blank" href="https://ccbr99.cn/index.html">Chinese Conference on Biometric Recognition (CCBR)</a> 2024 (<a target="_blank" href="https://mp.weixin.qq.com/s/cHg6ldEfk--K1NLsbGL9LA">Official Notification</a>).</div>
					       <div>Co-Chair of the CAIIAM 2023 <a target="_blank" href="https://mp.weixin.qq.com/s/kZEx0osUS3hN07qGNH-hnw">Visual Target Perception: Present and Future</a> Workshop.</div>
					       <div>Co-Chair of the ECCV 2020 <a target="_blank" href="https://rlq-tod.github.io/index.html">RLQ-TOD</a> Workshop and Challenge.</div>
					       <div>Co-Chair of the VALSE 2022 <a target="_blank" href="http://valser.org/2022/#/workshopde?id=8">Explainable Deep Learning Algorithms and Vision Applications</a> Workshop.</div>
					       <div>Co-Chair of the VALSE 2021 <a target="_blank" href="http://valser.org/2021/#/workshopde?id=12">Knowledge and Semantics Driven Visual Understanding</a> Workshop.</div> 
					       <div>Co-Chair of the CVPR 2018 L.I.P Workshop and <a target="_blank" href="https://lv-mhp.github.io/">MHP</a> Challenge.</div> 
					       <div>Senior Member of CAAI, CSIG (<a target="_blank" href="https://mp.weixin.qq.com/s/9YLcoyuTfp0XVsmpWbgrSA">Official Notification</a>).</div>
					       <div>Member of IEEE, CCF (<a href="pub/CCF.pdf" target="_blank">Invitation Letter</a>), CAAI, CSIG (<a href="pub/CSIG.pdf" target="_blank">Invitation Letter</a>), CSIG-BVD (<a href="pub/CSIG.pdf" target="_blank">Invitation Letter</a>), CSIG-MM, BSIG (<a href="pub/BSIG.pdf" target="_blank">Invitation Letter</a>).</div></p>
					    <p><b style="color: red;">Latest: </b>
					    <div><b>13 Mar 2024  &mdash;</b> One ICME 2024 accepted as the advisor & corresponding author.</div>
					    <div><b>27 Feb 2024  &mdash;</b> Two CVPR 2024 accepted as the advisor.</div>
					    <div><b>17 Feb 2024  &mdash;</b> One T-OMM accepted as the advisor & corresponding author.</div>
					    <div><b>25 Jan 2024  &mdash;</b> Received the 2023 CAAI WU WEN JUN AI Outstanding Youth Award. <a target="_blank" href="https://mp.weixin.qq.com/s/yKmZ5qwepRmdlw6v58I3UA">Official Announcement</a></div>
					    <div><b>23 Jan 2024  &mdash;</b> One IJCV accepted as the 1st & corresponding author.</div>
				            </p>
                                            <p><b>Hobbies: </b>Travel, Cooking, Rollar Skating, Fitness, Mixed Martial Arts (MMA) and Brazilian Jiu-Jitsu (BJJ). <a href="pub/Gym.jpg" target="_blank">Pic</a></p>
                                            <p><b>PS: </b> My English name is Jason. I am a 2-Stripe Blue Belt under Prof. Luiz Fernando Mexicano of Top Brother (Competition Records: <a href="pub/champ-23.jpg" target="_blank">Championship</a>, Gi-Blue Belt Master1 Male 75kg, Beijing Magnificent Sports Meet 2023, Beijing China; <a href="pub/Rouzhen23.jpg" target="_blank">Sliver Medal</a>, Gi-Blue Belt Master1 Male 69kg, Rouzhen Competition 2023, Beijing China; <a href="pub/asjjf2024.jpg" target="_blank">Sliver/Bronze Medal</a>, Gi-Blue Belt Master1 Male 76kg/Open Heavy, ASJJF Beijing Jiu Jitsu Championship 2024, Beijing China). I received my <a href="pub/BJJ_BlueBelt_Certificate.jpg" target="_blank">Blue Belt</a> in BJJ from Prof. <a href="pub/BJJ_BlueBelt_Ceremony.jpeg" target="_blank">Leandro Thomas Issa da Silva</a> of <a href="https://evolve-mma.com/" target="_blank">Evolve MMA</a> Singapore.</a> <a href="pub/BJJ.jpg" target="_blank">Pic</a></p>
                                    </div>
                                    
                                </div>
                            </div>        
                        </div>
                    </div>

                    <div class="pagecontents">
                        <div class="section color-1">
                            <div class="section-container">
                                <div class="row">
                                    <div id="education" class="col-md-5 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Education</h3>
					    <i class="quote"> </i>
                                        </div>
                                        <ul class="ul-card">
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">Ph.D.</span>
                                                    <span class="year">2016 - 2019</span>
                                                </div>
                                                <div class="description">
                                                    <p>Department of Electrical and Computer Engineering, Faculty of Engineering</p>
                                                    <p class="where">National University of Singapore, Singapore</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">Ph.D.</span>
                                                    <span class="year">2014 - 2015</span>
                                                </div>
                                                <div class="description">
                                                    <p>School of Computer</p>
                                                    <p class="where">National University of Defense Technology, China</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">M.Eng.</span>
                                                    <span class="year">2012 - 2014</span>
                                                </div>
                                                <div class="description">
                                                    <p>School of Computer</p>
                                                    <p class="where">National University of Defense Technology, China</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="degree">B.Eng.</span><span class="year">2008 - 2012</span>
                                                </div>
                                                <div class="description">
                                                    <p >School of Automation Science and Electrical Engineering</p>
                                                    <p class="where">Beihang University, China</p>
                                                </div>
                                            </li>
                                        </ul>
                                    </div>
                                    <div id="work" class="col-md-5">
                                        <div class="title text-center">
                                            <h3>Work Experience</h3>
                                            <i class="quote"> </i>
                                        </div>
                                        <ul class="ul-card">
					     <li>
                                                <div class="dy">
                                                    <span class="year">2024 - Now</span>
                                                </div>
                                                <div class="description">
                                                    <p>Young Scientist</p>
                                                    <p class="where">China Telecom Institute of AI, Beijing, China</p>
                                                </div>
                                            </li>
					     <li>
                                                <div class="dy">
                                                    <span class="year">2024 - Now</span>
                                                </div>
                                                <div class="description">
                                                    <p>Researcher</p>
                                                    <p class="where">School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, Xi'an, China</p>
                                                </div>
                                            </li>
					     <li>
                                                <div class="dy">
                                                    <span class="year">2019 - 2023</span>
                                                </div>
                                                <div class="description">
                                                    <p>Assistant Researcher</p>
                                                    <p class="where">Academy of Military Sciences, China</p>
                                                </div>
                                            </li>
					    <li>
                                                <div class="dy">
                                                    <span class="year">2022 - 2023</span>
                                                </div>
                                                <div class="description">
                                                    <p>Visiting Scholar</p>
                                                    <p class="where">Peng Cheng Laboratory, Shenzhen, China</p>
                                                </div>
                                            </li>
					     <li>
                                                <div class="dy">
                                                    <span class="year">2019 - 2020</span>
                                                </div>
                                                <div class="description">
                                                    <p>Rhino-Bird Visiting Scholar</p>
                                                    <p class="where">Tencent AI Lab, Shenzhen, China</p>
                                                </div>
                                            </li>
					     <li>
                                                <div class="dy">
                                                    <span class="year">2018 - 2019</span>
                                                </div>
                                                <div class="description">
                                                    <p>"Texpert" Research Scientist</p>
                                                    <p class="where">FiT DeepSea AI Lab, Tencent, Shenzhen, China</p>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="dy">
                                                    <span class="year">2016 - 2018</span>
                                                </div>
                                                <div class="description">
                                                    <p>Research Intern</p>
                                                    <p class="where">Core Technology Group, Learning & Vision, Panasonic R&D Center, Singapore</p>
                                                </div>
                                            </li>
					    <li>
                                                <div class="dy">
                                                    <span class="year">2016 - 2017</span>
                                                </div>
                                                <div class="description">
                                                    <p>Graduate Assistant</p>
                                                    <p class="where">NUS Module: <a href="https://myaces.nus.edu.sg/cors/jsp/report/ModuleDetailedInfo.jsp?acad_y=2014/2015&sem_c=1&mod_c=EE2024" target="_blank">EE2024 PROGRAMMING FOR COMPUTER INTERFACES</a></p>
                                                </div>
                                            </li>
					    <li>
                                                <div class="dy">
                                                    <span class="year">2011 - 2012</span>
                                                </div>
                                                <div class="description">
                                                    <p>Research Intern</p>
                                                    <p class="where">China Aerospace Science and Industry Corporation, Beijing, China</p>
                                                </div>
                                            </li>

                                        </ul>
                                    </div>
                                </div>    
                            </div>
                                
                        </div>

                        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Publications</h3>
                                        </div>
                                        <ul class="timeline">
					   <li class="open">
                                                <div class="date">2024</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Review and Analysis of RGBT Single Object Tracking Methods: A Fusion Perspective</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Visual tracking is a fundamental task in computer vision with significant practical applications in various domains, including surveillance, security, robotics, and human-computer interaction. However, it may face limitations in visible light data, such as lowlight environments, occlusion, and camouflage, which can significantly reduce its accuracy. To cope with these challenges, researchers have explored the potential of combining the visible and infrared modalities to improve tracking performance. By leveraging the complementary strengths of visible and infrared data, RGB-infrared fusion tracking has emerged as a promising approach to address these limitations and improve tracking accuracy in challenging scenarios. In this paper, we present a review on RGB-infrared fusion tracking. Specifically, we categorize existing RGBT tracking methods into four categories based on their underlying architectures, feature representations, and fusion strategies, namely feature decoupling based method, feature selecting based method, collaborative graph tracking method, and traditional fusion method. Furthermore, we provide a critical analysis of their strengths, limitations, representative methods, and future research directions. To further demonstrate the advantages and disadvantages of these methods, we present a review of publicly available RGBT tracking datasets and analyze the main results on public datasets. Moreover,we discuss some limitations in RGBT tracking at present and provide some opportunities and future directions for RGBT visual tracking, such as dataset diversity, unsupervised and weakly supervised applications. In conclusion, our survey aims to serve as a useful resource for researchers and practitioners interested in the emerging field of RGBT tracking, and to promote further progress and innovation in this area.
                                                            <p>Zhihao Zhang, Jun Wang, Zhuli Zang, Lei Jin, Shengjie Li, Hao Wu, <b>Jian Zhao</b>, Bo Zhang</p>
							    <p><b>(Corresponding Authors: Hao Wu, Jian Zhao, Bo Zhang)</b></p>
							    <p>T-OMM&nbsp; <a target="_blank" href="https://dl.acm.org/doi/pdf/10.1145/3651308">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Review+and+Analysis+of+RGBT+Single+Object+Tracking+Methods%3A+A+Fusion+Perspective&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2024</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations. In addition, we generate Virtual Negatives (VNs) based on GDP to involve the contrastive learning process. Extensive experiments on two public benchmarks demonstrate the superiority of our PRCL framework.
                                                            <p>Haoyu Xie, Changqi Wang, <b>Jian Zhao</b>, Yang Liu, Jun Dan, Chong Fu, and Baigui Sun</p>
							    <p><b>(The first three authors are with equal contributions.)</b></p>
							    <p><b>(Corresponding Authors: Jian Zhao, Baigui Sun)</b></p>
							    <p>IJCV&nbsp; <a target="_blank" href="https://arxiv.org/html/2402.18117v1">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=PRCL%3A+Probabilistic+Representation+Contrastive+Learning+for+Semi-Supervised+Semantic+Segmentation&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/AmDtsMSlDb5Ac_PBBeAEOQ">BSIG News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/Vf743t0kNXLxaJA2OCYxHA">WeChat News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Adversarial Attacks on Video Object Segmentation with Hard Region Discovery</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Video object segmentation has been applied to various computer vision tasks, such as video editing, autonomous driving, and human-robot interaction. However, the methods based on deep neural networks are vulnerable to adversarial examples, which are the inputs attacked by almost humanimperceptible perturbations, and the adversary (i.e., attacker) will fool the segmentation model to make incorrect pixel-level predictions. This will rise the security issues in highly-demanding tasks because small perturbations to the input video will result in potential attack risks. Though adversarial examples have been extensively used for classification, it is rarely studied in video object segmentation. Existing related methods in computer vision either require prior knowledge of categories or cannot be directly applied due to the special design for certain tasks, failing to consider the pixel-wise region attack. Hence, this work develops an object-agnostic adversary that has adversarial impacts on VOS by first-frame attacking via hard region discovery. Particularly, the gradients from the segmentation model are exploited to discover the easily confused region, in which it is difficult to identify the pixel-wise objects from the background in a frame. This provides a hardness map that helps to generate perturbations with a stronger adversarial power for attacking the first frame. Empirical studies on three benchmarks indicate that our attacker significantly degrades the performance of several state-of-the-art video object segmentation models.
                                                            <p>Ping Li, Yu Zhang, Li Yuan, <b>Jian Zhao</b>, Xianghua Xu, and Xiaoqin Zhang</p>
							    <p>T-CSVT&nbsp; <a target="_blank" href="https://arxiv.org/pdf/2309.13857.pdf">PDF</a>, <a href="https://scholar.google.com.hk/scholar?q=adversarial+attacks+on+video+object+segmentation+with+hard+region+discovery&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Anti-UAV410: A Thermal Infrared Benchmark and Customized Scheme for Tracking Drones in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       The perception of drones, also known as Unmanned Aerial Vehicles (UAVs), particularly in infrared videos, is crucial for effective anti-UAV tasks. However, existing datasets for UAV tracking have limitations in terms of target size and attribute distribution characteristics, which do not fully represent complex realistic scenes. To address this issue, we introduce a generalized infrared UAV tracking benchmark called Anti-UAV410. The benchmark comprises a total of 410 videos with over 438 K manually annotated bounding boxes. To tackle the challenges of UAV tracking in complex environments, we propose a novel method called Siamese drone tracker (SiamDT). SiamDT incorporates a dual-semantic feature extraction mechanism that explicitly models targets in dynamic background clutter, enabling effective tracking of small UAVs. The SiamDT method consists of three key steps: Dual-Semantic RPN Proposals (DS-RPN), Versatile R-CNN (VR-CNN), and Background Distractors Suppression. These steps are responsible for generating candidate proposals, refining prediction scores based on dual-semantic features, and enhancing the discriminative capacity of the trackers against dynamic background clutter, respectively. Extensive experiments conducted on the Anti-UAV410 dataset and three other large-scale benchmarks demonstrate the superior performance of the proposed SiamDT method compared to recent state-of-the-art trackers. The benchmark of Anti-UAV410 is available at https://github.com/HwangBo94/Anti-UAV410.
                                                            <p>Bo Huang, Jianan Li, Junjie Chen, Gang Wang, <b>Jian Zhao</b>, and Tingfa Xu</p>
							    <p><b>(Corresponding Authors: Jianan Li, Gang Wang, Jian Zhao)</b></p>
							    <p>T-PAMI&nbsp; <a target="_blank" href="https://www.computer.org/csdl/journal/tp/5555/01/10325629/1Shv1VcSW5O">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Anti-UAV410%3A+A+Thermal+Infrared+Benchmark+and+Customized+Scheme+for+Tracking+Drones+in+the+Wild&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/Q9zoyQXPEdydLoNLUmOEZw">BSIG News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/DUilwrteD3zbCwaS5cnclw">WeChat News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Modality Meets Long-term Tracker: A Siamese Dual Fusion Framework for Tracking UAV</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Tracking an Unmanned Aerial Vehicle (UAV) to obtain its locations and trajectory is a crucial task to avoid the unlawful use of UAVs. However, most existing UAV tracking methods fail when facing cluster environments, out-of-view, and occlusions because of their insufficient representation of global context information capacity. To mitigate these issues, we propose a new tracker, namely SiamFusion, to innovate a dual fusion procedure that leverages the advantages in both the feature and decision levels. In particular, we propose a novel feature fusion module named Modality-Fusion to utilize multi-modal information, enhancing the perception of the target. From the decision level, we further develop a local-global converter based on a multi-modal fusion decision-making mechanism to reduce the accumulation during tracking, which significantly increases the robustness of the tracking process. Extensive experiments demonstrate the superiority of the proposed SiamFusion, which achieves the best performance on Anti-UAV in terms of accuracy and speed. In particular, we exceed the state-of-the-art tracking algorithm in the tracking accuracy by 4.2% at a similar frame rate. Our source codes, pre-trained models, and online demos will be released upon acceptance.
                                                            <p>Zhihao Zhang, Lei Jin, Shengjie Li, Jianqiang Xia, Jun Wang, Zun Li, Zheng Zhu, Wenhan Yang, Pengfei Zhang, <b>Jian Zhao</b>, and Bo Zhang</p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>ICIP 2023&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10222679/metrics#metrics">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Modality+Meets+Long-term+Tracker%3A+A+Siamese+Dual+Fusion+Framework+for+Tracking+UAV&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Heterogeneous Diversity Driven Active Learning for Multi-Object Tracking</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       The existing one-stage multi-object tracking (MOT) al-gorithms have achieved satisfactory performance benefit-ing from a large amount of labeled data. However, acquir-ing plenty of laborious annotated frames is not practicalin real applications. To reduce the cost of human anno-tations, we propose Heterogeneous Diversity driven ActiveMulti-object Tracking (HD-AMOT), to infer the most infor-mative frames for any MOT tracker by observing the hetero-geneous cues of samples. HD-AMOT defines the diversifiedinformative representation by encoding the geometric andsemantic information, and formulates the frame inferencestrategy as a Markov decision process to learn an optimalsampling policy based on the designed informative repre-sentation. Specifically, HD-AMOT consists of a diversifiedinformative representation module as well as an informa-tive frame selection network. The former produces the sig-nal characterizing the diversity and distribution of frames,and the latter receives the signal and conducts multi-framecooperation to enable batch frame sampling. Extensive ex-periments conducted on the MOT15, MOT17, MOT20, andDancetrack datasets demonstrate the efficacy and effective-ness of HD-AMOT. Experiments show that under 50% bud-get our HD-AMOT can achieve similar or even higher per-formance as fully-supervised learning 
                                                            <p>Rui Li, Baopeng Zhang, Jun Liu, Wei Liu, <b>Jian Zhao</b>, and Zhu Teng</p>
							    <p>ICCV 2023&nbsp; <a target="_blank" href="https://www.researchgate.net/publication/372827084_Heterogeneous_Diversity_Driven_Active_Learning_for_Multi-Object_Tracking">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Heterogeneous+Diversity+Driven+Active+Learning+for+Multi-Object+Tracking&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">3D Implicit Transporter for Temporally Consistent Keypoint Discovery</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Keypoint-based representation has proven advantageousin various visual and robotic tasks. However, the exist-ing 2D and 3D methods for detecting keypoints mainly relyon geometric consistency to achieve spatial alignment, ne-glecting temporal consistency. To address this issue, theTransporter method was introduced for 2D data, which re-constructs the target frame from the source frame to in-corporate both spatial and temporal information. However, the direct application of the Transporter to 3D pointclouds is infeasible due to their structural differences from2D images. Thus, we propose the first 3D version ofthe Transporter, which leverages hybrid 3D representation,cross attention, and implicit reconstruction. We apply thisnew learning system on 3D articulated objects and non-rigid animals (humans and rodents) and show that learnedkeypoints are spatio-temporally consistent. Additionally, we propose a closed-loop control strategy that utilizes thelearned keypoints for 3D object manipulation and demon-strate its superior performance. Codes are available athttps://github.com/zhongcl-thu/3D-Implicit-Transporter. 
                                                            <p>Chengliang Zhong, Yuhang Zheng, Yupeng Zheng, Hao Zhao, Li Yi, Xiaodong Mu, Ling Wang, Pengfei Li, Guyue Zhou, Chao Yang, Xinliang Zhang, and <b>Jian Zhao</b></p>
							    <p>ICCV 2023&nbsp; <a target="_blank" href="https://www.researchgate.net/publication/373328882_3D_Implicit_Transporter_for_Temporally_Consistent_Keypoint_Discovery">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=3D+Implicit+Transporter+for+Temporally+Consistent+Keypoint+Discovery&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/IgjXggzaMjCVHrsTB5cUfA">WeChat News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">DecenterNet: Bottom-Up Human Pose Estimation Via Decentralized Pose Representation</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       This Multi-person pose estimation in crowded scenes remains a verychallenging task. This paper finds that most previous methods failto estimate or group visible keypoints in crowded scenes ratherthan reasoning invisible keypoints. We thus categorize the crowdedscenes into entanglement and occlusion based on the visibilityof human parts and observe that entanglement is a significantproblem in crowded scenes. With this observation, we proposeDecenterNet, an end-to-end deep architecture to perform robust andefficient pose estimation in crowded scenes. Within DecenterNet,we introduce a decentralized pose representation that uses all visiblekeypoints as the root points to represent human poses, which ismore robust in the entanglement area. We also propose a decoupledpose assessment mechanism, which introduces a location map toadaptively select optimal poses in the offset map. In addition, wehave constructed a new dataset named SkatingPose, containingmore entangled scenes. The proposed DecenterNet surpasses thebest method on SkatingPose by 1.8 AP. Furthermore, DecenterNetobtains 71.2 AP and 71.4 AP on the COCO and CrowdPose datasets,respectively, demonstrating the superiority of our method. We willrelease our source code, trained models, and dataset to facilitatefurther studies in this research direction. Our code and dataset areavailable in https://github.com/InvertedForest/DecenterNet. 
                                                            <p>Tao Wang, Lei Jin, Zhang Wang, Xiaojin Fan, Yu Cheng, Yinglei Teng, Junliang Xing, and <b>Jian Zhao</b></p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>ACM MM 2023&nbsp; <a target="_blank" href="https://www.researchgate.net/publication/373328982_DecenterNet_Bottom-Up_Human_Pose_Estimation_Via_Decentralized_Pose_Representation">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=DecenterNet%3A+Bottom-Up+Human+Pose+Estimation+Via+Decentralized+Pose+Representation&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/zT3pGkgCUt0ZPAV5mMGtMw">WeChat News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Uncovering the Unseen: Discover Hidden Intentions by Micro-Behavior Graph Reasoning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       This paper introduces a new and challenging Hidden IntentionDiscovery (HID) task. Unlike existing intention recognition tasks,which are based on obvious visual representations to identify com-mon intentions for normal behavior, HID focuses on discoveringhidden intentions when humans try to hide their intentions forabnormal behavior. HID presents a unique challenge in that hiddenintentions lack the obvious visual representations to distinguishthem from normal intentions. Fortunately, from a sociological andpsychological perspective, we find that the difference between hid-den and normal intentions can be reasoned from multiple micro-behaviors, such as gaze, attention, and facial expressions. Therefore,we first discover the relationship between micro-behavior and hid-den intentions and use graph structure to reason about hiddenintentions. To facilitate research in the field of HID, we also con-structed a seminal dataset containing a hidden intention annotationof a typical theft scenario for HID. Extensive experiments showthat the proposed network improves performance on the HID taskby 9.9% over the state-of-the-art method SBP.
                                                            <p>Zhuo Zhou, Wenxuan Liu, Danni Xu, Zheng Wang, and <b>Jian Zhao</b></p>
							    <p>ACM MM 2023&nbsp; <a target="_blank" href="https://www.researchgate.net/publication/373329037_Uncovering_the_Unseen_Discover_Hidden_Intentions_by_Micro-Behavior_Graph_Reasoning">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Uncovering+the+Unseen%3A+Discover+Hidden+Intentions+by+Micro-Behavior+Graph+Reasoning&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Single-stage Multi-human Parsing via Point Sets and Center-based Offsets</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       This work studies the multi-human parsing problem. Existing methods, either following top-down or bottom-up two-stage paradigms, usually involve expensive computational costs. We instead present a high-performance Single-stage Multi-human Parsing (SMP) deep architecture that decouples the multi-human parsing problem into two fine-grained sub-problems, i.e., locating the human body and parts. SMP leverages the point features in the barycenter positions to obtain their segmentation and then generates a series of offsets from the barycenter of the human body to the barycenters of parts, thus performing human body and parts matching without the grouping process. Within the SMP architecture, we propose a Refined Feature Retain module to extract the global feature of instances through generated mask attention and a Mask of Interest Reclassify module as a trainable plug-in module to refine the classification results with the predicted segmentation. Extensive experiments on the MHPv2.0 dataset demonstrate the best effectiveness and efficiency of the proposed method, surpassing the state-of-the-art method by 2.1% in AP50p, 1.0% in APvolp, and 1.2% in PCP50. In particular, the proposed method requires fewer training epochs and a less complex model architecture. We will release our source codes, pretrained models, and online demos to facilitate further studies.
                                                            <p>Jiaming Chu, Lei Jin, Junliang Xing, and <b>Jian Zhao</b></p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>ACM MM 2023&nbsp; <a target="_blank" href="https://arxiv.org/pdf/2304.11356.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Single-stage+Multi-human+Parsing+via+Point+Sets+and+Center-based+Offsets&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/tfmcK4LOkDhFBVILlQluKQ">WeChat News1</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/IljjUcbxAiqwsh8kjRlstA">WeChat News2</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Rethinking the Person Localization for Single-Stage Multi-Person Pose Estimation</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Single-stage models for multi-person pose estimation have garnered significant attention due to their streamlined approach in generating person position localization and body structure perception in a single pass. These two parts, however, are processed individually by existing methods, leading to suboptimal results, e.g. , candidates with high confidences for person localization while poor structure estimations. To this end, we propose a simple yet effective approach, namely Structure-guided Person Localization (SPL), jointly leveraging the advantages of the two aspects to solve the multi-person pose estimation problem, with two complementary novelties. First, we propose to incorporate body structure perception to guide person position localization, consequently, we introduce the Structure-guided Center Learning (SCL) to unify the quality of the body structure perception in the displacement map with the confidence of the person existence in the center map, thus achieving more accurate keypoint position localization results even with extreme poses. Second, to facilitate the end-to-end training of SPL, we propose the efficient Agency-based Scale-adaptive Learning (ASL). Specifically, we predict an agency map of the same size as the center map, which focuses on the foreground area and can adaptively adjust the scale size for each central area with the body structure perception confidence. Comprehensive experiments on challenging benchmarks including COCO and CrowdPose clearly verify the superiority of our framework, which achieves new state-of-the-art single-stage multi-person pose estimation results. Specifically, SPL obtains 72.1 AP scores and 69.5 AP scores in COCO test-dev2017 and CrowdPose test set, respectively.
                                                            <p>Lei Jin, Xiaojuan Wang, Xuecheng Nie, Wendong Wang, Yandong Guo, Shuicheng Yan, and <b>Jian Zhao</b></p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>T-MM&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10141888">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Rethinking+the+Person+Localization+for+Single-Stage+Multi-Person+Pose+Estimation&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Evidential Detection and Tracking Collaboration: New Problem, Benchmark and Algorithm for Robust Anti-UAV System</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Unmanned Aerial Vehicles (UAVs) have been widely used in many areas, including transportation, surveillance, and military. However, their potential for safety and privacy violations is an increasing issue and highly limits their broader applications, underscoring the critical importance of UAV perception and defense (anti-UAV). Still, previous works have simplified such an anti-UAV task as a tracking problem, where the prior information of UAVs is always provided; such a scheme fails in real-world anti-UAV tasks (i.e. complex scenes, indeterminate-appear and -reappear UAVs, and real-time UAV surveillance). In this paper, we first formulate a new and practical anti-UAV problem featuring the UAVs perception in complex scenes without prior UAVs information. To benchmark such a challenging task, we propose the largest UAV dataset dubbed AntiUAV600 and a new evaluation metric. The AntiUAV600 comprises 600 video sequences of challenging scenes with random, fast, and small-scale UAVs, with over 723K
                                                            <p>Xuefeng Zhu, Tianyang Xu, <b>Jian Zhao</b>, Jiawei Liu, Kai Wang, Gang Wang, Jianan Li, Qiang Wang, Lei Jin, Zheng Zhu, Junliang Xing, and Xiaojun Wu</p>
							    <p><b>(The first three authors are with equal contributions.)</b></p>
							    <p><b>(Corresponding Authors: Tianyang Xu, Jian Zhao, Gang Wang, Jianan Li, Xiaojun Wu)</b></p>
							    <p>ArXiv&nbsp; <a target="_blank" href="https://arxiv.org/pdf/2306.15767.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Evidential+Detection+and+Tracking+Collaboration%3A+New+Problem%2C+Benchmark+and+Algorithm+for+Robust+Anti-UAV+System&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">The 3rd Anti-UAV Workshop & Challenge: Methods and Results</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       The 3rd Anti-UAV Workshop & Challenge aims to encourage research in developing novel and accurate methods for multi-scale object tracking. The Anti-UAV dataset used for the Anti-UAV Challenge has been publicly released. There are two main differences between this year's competition and the previous two. First, we have expanded the existing dataset, and for the first time, released a training set so that participants can focus on improving their models. Second, we set up two tracks for the first time, i.e., Anti-UAV Tracking and Anti-UAV Detection & Tracking. Around 76 participating teams from the globe competed in the 3rd Anti-UAV Challenge. In this paper, we provide a brief summary of the 3rd Anti-UAV Workshop & Challenge including brief introductions to the top three methods in each track. The submission leaderboard will be reopened for researchers that are interested in the Anti-UAV challenge. The benchmark dataset and other information can be found at https://anti-uav.github.io/.
                                                            <p><b>Jian Zhao</b>, Jianan Li, Lei Jin, Jiaming Chu, Zhihao Zhang, Jun Wang, Jiangqiang Xia, Kai Wang, Yang Liu, Sadaf Gulshad, Jiaojiao Zhao, Tianyang Xu, Xuefeng Zhu, Shihan Liu, Zheng Zhu, Guibo Zhu, Zechao Li, Zheng Wang, Baigui Sun, Yandong Guo, Shin ichi Satoh, Junliang Xing, and Jane Shen Shengmei</p>
							    <p>ArXiv&nbsp; <a target="_blank" href="https://arxiv.org/pdf/2305.07290.pdf">PDF</a>, , <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=The+3rd+Anti-UAV+Workshop+%26+Challenge%3A+Methods+and+Results&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/pWocyWpVzv10TTneKgqYXg">WeChat News1</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/O23aJL1GJJyRXxPjSOxq9g">WeChat News2</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Neural Architecture Search (NAS) has been increasingly appealing to the society of object Re-Identification (ReID), for that task-specific architectures significantly improve the retrieval performance. Previous works explore new optimizing targets and search spaces for NAS ReID, yet they neglect the difference of training schemes between image classification and ReID. In this work, we propose a novel Twins Contrastive Mechanism (TCM) to provide more appropriate supervision for ReID architecture search. TCM reduces the category overlaps between the training and validation data, and assists NAS in simulating real-world ReID training schemes. We then design a Multi-Scale Interaction (MSI) search space to search for rational interaction operations between multi-scale features. In addition, we introduce a Spatial Alignment Module (SAM) to further enhance the attention consistency confronted with images from different sources. Under the proposed NAS scheme, a specific architecture is automatically searched, named as MSINet. Extensive experiments demonstrate that our method surpasses state-of-the-art ReID methods on both indomain and cross-domain scenarios. Source code available in https://github.com/vimar-gu/MSINet.
                                                            <p>Jianyang Gu, Kai Wang, Hao Luo, Chen Chen, Wei Jiang, Yuqiang Fang, Shanghang Zhang, Yang You, and <b>Jian Zhao</b></p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>CVPR 2023&nbsp; <a href="pub/cvpr23.pdf" target="_blank">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=MSINet%3A+Twins+Contrastive+Search+of+Multi-Scale+Interaction+for+Object+ReID&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/3pkvS0TuBif3vWx_qjQ_ew">BSIG News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/dINOVPbWugtt3Tlfj0xd9Q">VALSE News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2023</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">A Compact and Powerful Single-Stage Network for Multi-Person Pose Estimation</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Multi-person pose estimation generally follows top-down and bottom-up paradigms. The top-down paradigm detects all human boxes and then performs single-person pose estimation on each ROI. The bottom-up paradigm locates identity-free keypoints and then groups them into individuals. Both of them use an extra stage to build the relationship between human instance and corresponding keypoints (e.g., human detection in a top-down manner or a grouping process in a bottom-up manner). The extra stage leads to a high computation cost and a redundant two-stage pipeline. To address the above issue, we introduce a fine-grained body representation method. Concretely, the human body is divided into several local parts and each part is represented by an adaptive point. The novel body representation is able to sufficiently encode the diverse pose information and effectively model the relationship between human instance and corresponding keypoints in a single-forward pass. With the proposed body representation, we further introduce a compact single-stage multi-person pose regression network, called AdaptivePose++, which is the extended version of AAAI-22 paper AdaptivePose. During inference, our proposed network only needs a single-step decode operation to estimate the multi-person pose without complex post-processes and refinements. Without any bells and whistles, we achieve the most competitive performance on representative 2D pose estimation benchmarks MS COCO and CrowdPose in terms of accuracy and speed. In particular, AdaptivePose++ outperforms the state-of-the-art SWAHR-W48 and CenterGroup-W48 by 3.2 AP and 1.4 AP on COCO mini-val with faster inference speed. Furthermore, the outstanding performance on 3D pose estimation datasets MuCo-3DHP and MuPoTS-3D further demonstrates its effectiveness and generalizability on 3D scenes.
                                                            <p>Yabo Xiao, Xiaojuan Wang, Mingshu He, Lei Jin, Mei Song, and <b>Jian Zhao</b></p>
							    <p>Electronics&nbsp; <a target="_blank" href="https://www.mdpi.com/2079-9292/12/4/857">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=A+Compact+and+Powerful+Single-Stage+Network+for+Multi-Person+Pose+Estimation&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Joint Coupled Representation and Homogeneous Reconstruction for Multi-Resolution Small Sample Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Off-the-shelf dictionary learning algorithms have achieved satisfactory results in small sample face recognition applications. However, the achieved results depend on the facial images obtained at a single resolution. In practice, the resolution of the images captured on the same target is different because of the different shooting equipment and different shooting distances. These images of the same category at different resolutions will pose a great challenge to these algorithms. In this paper, we propose a Joint Coupled Representation and Homogeneous Reconstruction (JCRHR) for multi-resolution small sample face recognition. In JCRHR, an analysis dictionary is introduced and combined with the synthetic dictionary for coupled representation learning, which better reveals the relationship between coding coefficients and samples. In addition, a coherence enhancement term is proposed to improve the coherent representation of the coding coefficients at different resolutions, which facilitates the reconstruction of the sample by its homogeneous atoms. Moreover, each sample at different resolutions is assigned a different coding coefficient in the multi-dictionary learning process, so that the learned dictionary is more in line with the actual situation. Furthermore, a regularization term based on the fractional norm is drawn into the dictionary coupled learning to remove the redundant information in the dictionary, which can reduce the negative impacts of the redundant information. Comprehensive results demonstrate that the proposed JCRHR method achieves better results than the state-of-the-art methods, on several small sample face databases.
                                                            <p>Xiaojin Fan, Mengmeng Liao, Jingfeng Xue, Hao Wu, Lei Jin, <b>Jian Zhao</b>, and Liehuang Zhu</p>
							    <p>Neurocomputing&nbsp; <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231222015156">PDF</a>, <a href="https://scholar.google.com.tw/scholar?q=Joint+coupled+representation+and+homogeneous+reconstruction+for+multiresolution+small+sample+face+recognition&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Zero-shot learning (ZSL) tackles the novel class recognition problem by transferring semantic knowledge from seen classes to unseen ones. Semantic knowledge is typically represented by attribute descriptions shared between different classes, which act as strong priors for localizing object attributes that represent discriminative region features, enabling significant and sufficient visualsemantic interaction for advancing ZSL. Existing attention-based models have struggled to learn inferior region features in a single image by solely using unidirectional attention, which ignore the transferable and discriminative attribute localization of visual features for representing the key semantic knowledge for effective knowledge transfer in ZSL. In this paper, we propose a cross attribute-guided Transformer network, termed TransZero++, to refine visual features and learn accurate attribute localization for key semantic knowledge representations in ZSL. Specifically, TransZero++ employs an attribute→visual Transformer sub-net (AVT) and a visual→attribute Transformer sub-net (VAT) to learn attribute-based visual features and visual-based attribute features, respectively. By further introducing feature-level and prediction-level semantical collaborative losses, the two attribute-guided transformers teach each other to learn semantic-augmented visual embeddings for key semantic knowledge representations via semantical collaborative learning. Finally, the semantic-augmented visual embeddings learned by AVT and VAT are fused to conduct desirable visual-semantic interaction cooperated with class semantic vectors for ZSL classification. Extensive experiments show that TransZero++ achieves the new stateof-the-art results on three golden ZSL benchmarks and on the large-scale ImageNet dataset. The project website is available at: https://shiming-chen.github.io/TransZero-pp/TransZero-pp.html.
                                                            <p>Shiming Chen, Ziming Hong, Wenjin Hou, Guosen Xie, Yibing Song, <b>Jian Zhao</b>, Xinge You, Shuicheng Yan, and Ling Shao</p>
							    <p>T-PAMI&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9987664">PDF</a>, <a href="https://scholar.google.com.sg/scholar?oi=bibs&hl=en&cluster=11186771001301414077">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Rethinking Sampling Strategies for Unsupervised Person Re-identification</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Unsupervised person re-identification (re-ID) remains a challenging task. While extensive research has focused on the framework design and loss function, this paper shows that sampling strategy plays an equally important role. We analyze the reasons for the performance differences between various sampling strategies under the same framework and loss function. We suggest that deteriorated over-fitting is an important factor causing poor performance, and enhancing statistical stability can rectify this problem. Inspired by that, a simple yet effective approach is proposed, termed group sampling, which gathers samples from the same class into groups. The model is thereby trained using normalized group samples, which helps alleviate the negative impact of individual samples. Group sampling updates the pipeline of pseudo-label generation by guaranteeing that samples are more efficiently classified into the correct classes. It regulates the representation learning process, enhancing statistical stability for feature representation in a progressive fashion. Extensive experiments on Market-1501, DukeMTMC-reID and MSMT17 show that group sampling achieves performance comparable to state-of-the-art methods and outperforms the current techniques under purely camera-agnostic settings. Code has been available at https://github.com/ucas-vg/GroupSampling.
                                                            <p>Xumeng Han, Xuehui Yu, Guorong Li, <b>Jian Zhao</b>, Gang Pan, Qixiang Ye, Jianbin Jiao, and Zhenjun Han</p>
							    <p>T-IP&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/document/9969623">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Rethinking+Sampling+Strategies+for+Unsupervised+Person+Re-identification&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Waveform Level Adversarial Example Generation for Joint Attacks Against both Automatic Speaker Verification and Spoofing Countermeasures</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Adversarial examples crafted to deceive Automatic Speaker Verification (ASV) systems have attracted a lot of attention when studying the vulnerability of ASV. However, real-world ASV systems usually work together with spoofing countermeasures (CM) to exclude fake voices generated by text-to-speech (TTS) or voice conversion (VC). The deployment of CM would reduce the capability of the adversarial samples on deceiving ASV. Although additional perturbations against CM may be generated and put on the crafted adversarial examples against ASV to yield new adversarial examples against both ASV and CM, those additional perturbations would however hinder the examples’ adversarial effectiveness on ASV. In this paper, a novel joint approach is proposed to generate adversarial examples by considering attacking ASV and CM simultaneously. For any voice from TTS, VC or a real-world speaker, our crafted adversarial perturbations will turn its original labels on CM and speaker ID to bonafide and some target speaker ID, correspondingly. In our approach, a differentiable front-end is introduced to replace the conventional hand-crafted time–frequency feature extractor. Perturbations can thus be estimated by updating the gradients of the joint objective of ASV and CM on the waveform variables. The proposed method has demonstrated a 99.3% success rate on white-box logical access attacks to deceive ASV and CM simultaneously, which outperforms the baselines of 65.3% and 36.7%. Furthermore, transferability on black-box and physical settings has also been validated.
                                                            <p>Xingyu Zhang, Xiongwei Zhang, Wei Liu, Xia Zou, Meng Sun, and <b>Jian Zhao</b></p>
							    <p>EAAI&nbsp; <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0952197622004596">PDF</a>, <a href="https://scholar.google.com.tw/scholar?cluster=4436830294355478123&hl=zh-CN&as_sdt=0,5&as_vis=1">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">3D-Guided Frontal Face Generation for Pose-Invariant Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Although deep learning techniques have achieved extraordinary accuracy in recognizing human faces, the pose variances of images captured in real-world scenarios still hinder reliable model appliance. To mitigate this gap, we propose to recognize faces via generation frontal face images with a 3D-Guided Deep Pose-Invariant Face Recognition Model (3D-PIM) consisted of a simulator and a refiner module. The simulator employs a 3D Morphable Model (3D MM) to fit the shape and appearance features and recover primary frontal images with less training data. The refiner further enhances the image realism on both global facial structure and local details with adversarial training, while keeping the discriminative identity information consistent with original images. An Adaptive Weighting (AW) metric is then adopted to leverage the complimentary information from recovered frontal faces and original profile faces and to obtain credible similarity scores for recognition. Extended experiments verify the superiority of the proposed "recognition via generation" framework over state-of-the-arts.
                                                            <p>Hao Wu, Jianyang Gu, Xiaojin Fan, He Li, Lidong Xie, and <b>Jian Zhao</b></p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>T-IST&nbsp; <a target="_blank" href="https://dl.acm.org/doi/10.1145/3572035">PDF</a>, <a href="https://scholar.google.com.tw/scholar?hl=zh-CN&as_sdt=0%2C5&as_vis=1&q=3D-Guided+Frontal+Face+Generation+for+Pose-Invariant+Recognition&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Point-to-Box Network for Accurate Object Detection via Single Point Supervision</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Subspace clustering aims to fit each category of data points by learning an underlying subspace and then conduct clustering according to the learned subspace. Ideally, the learned subspace is expected to be block diagonal such that the similarities between clusters are zeros. In this paper, we provide the explicit theoretical connection between spectral clustering and the subspace clustering based on block diagonal representation. We propose Enforced Block Diagonal Subspace Clustering (EBDSC) and show that the spectral clustering with the Radial Basis Function kernel can be regarded as EBDSC. Compared with the exiting subspace clustering methods, an analytical, nonnegative and symmetrical solution can be obtained by EBDSC. An important difference with respect to the existing ones is that our model is a more general case. EBDSC directly uses the obtained solution as the similarity matrix, which can avoid the complex computation of the optimization program. Then the solution obtained by the proposed method can be used for the final clustering. Finally, we provide the experimental analysis to show the efficiency and effectiveness of our method on the synthetic data and several benchmark data sets in terms of different metrics.
                                                            <p>Pengfei Chen, Xuehui Yu, Xumeng Han, Najmul Hassan, Kai Wang, Jiachen Li, <b>Jian Zhao</b>, Humphrey Shi, Zhenjun Han, and Qixiang Ye</p>
							    <p>ECCV 2022&nbsp; <a target="_blank" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690053.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Point-to-Box+Network+for+Accurate+Object+Detection+via+Single+Point+Supervision&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/TF6mZ5sQHcX_d_Rla9iD3A">WeChat News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
				           <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Enforced Block Diagonal Subspace Clustering with Closed Form Solution</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Subspace clustering aims to fit each category of data points by learning an underlying subspace and then conduct clustering according to the learned subspace. Ideally, the learned subspace is expected to be block diagonal such that the similarities between clusters are zeros. In this paper, we provide the explicit theoretical connection between spectral clustering and the subspace clustering based on block diagonal representation. We propose Enforced Block Diagonal Subspace Clustering (EBDSC) and show that the spectral clustering with the Radial Basis Function kernel can be regarded as EBDSC. Compared with the exiting subspace clustering methods, an analytical, nonnegative and symmetrical solution can be obtained by EBDSC. An important difference with respect to the existing ones is that our model is a more general case. EBDSC directly uses the obtained solution as the similarity matrix, which can avoid the complex computation of the optimization program. Then the solution obtained by the proposed method can be used for the final clustering. Finally, we provide the experimental analysis to show the efficiency and effectiveness of our method on the synthetic data and several benchmark data sets in terms of different metrics.
                                                            <p>Yalan Qin, Hanzhou Wu, <b>Jian Zhao</b>, and Guorui Feng</p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>PR&nbsp; <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322002722">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Enforced+Block+Diagonal+Subspace+Clustering+with+Closed+Form+Solution&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Semantic Compression Embedding for Generative Zero-Shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Generative methods have been successfully applied in zero-shot learning (ZSL) by learning an implicit mapping to alleviate the visual-semantic domain gaps and synthesizing unseen samples to handle the data imbalance between seen and unseen classes. However, existing generative methods simply use visual features extracted from the pre-trained CNN backbone, which lack attribute-level semantic information. Thus, seen classes are indistinguishable and the knowledge transfer from seen to unseen classes is limited. To tackle this issue, we propose a novel Semantic Compression Embedding Guided Generation (SC-EGG) model, which cascades a semantic compression embedding network (SCEN) and an embedding guided generative network (EGGN). The SCEN extracts a group of attribute-level local features, which are further compressed into the new low-dimension visual feature for each sample, thus a dense-semantic visual space is obtained. The EGGN learns a mapping from the class-level semantic space to the densesemantic visual space, thus improving the discriminability of the synthesized dense-semantic unseen visual features. Extensive experiments on three benchmark datasets, i.e., CUB, SUN and AWA2, demonstrate the significant performance gains of SC-EGG over current state-of-the-art methods and its baselines.
                                                            <p>Ziming Hong, Shiming Chen, Guosen Xie, Wenhan Yang, <b>Jian Zhao</b>, Yuanjie Shao, Qinmu Peng, and Xinge You</p>
							    <p>IJCAI 2022&nbsp; <a target="_blank" href="https://www.ijcai.org/proceedings/2022/0134.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Semantic+Compression+Embedding+for+Generative+Zero-Shot+Learning&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">GrOD: Deep Learning with Gradients Orthogonal Decomposition for Knowledge Transfer, Distillation, and Adversarial Training</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Regularization that incorporates the linear combination of empirical loss and explicit regularization terms as the loss function has been frequently used for many machine learning tasks. The explicit regularization term is designed in different types, depending on its applications. While regularized learning often boost the performance with higher accuracy and faster convergence, the regularization would sometimes hurt the empirical loss minimization and lead to poor performance. To deal with such issues in this work, we propose a novel strategy, namely Gradients Orthogonal Decomposition (GrOD), that improves the training procedure of regularized deep learning. Instead of linearly combining gradients of the two terms, GrOD re-estimates a new direction for iteration that does not hurt the empirical loss minimization while preserving the regularization affects, through orthogonal decomposition. We have performed extensive experiments to use GrOD improving the commonly-used algorithms of transfer learning, knowledge distillation, adversarial learning. The experiment results based on large datasets, including Caltech 256, MIT indoor 67, CIFAR-10 and ImageNet, show significant improvement made by GrOD for all three algorithms in all cases.
                                                            <p>Haoyi Xiong, Ruosi Wan, <b>Jian Zhao</b>, Zeyu Chen, Xingjian Li, Zhanxing Zhu, and Jun Huan</p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>T-KDD&nbsp; <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3530836">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=en&as_sdt=2005&sciodt=0%2C5&cites=2769336589334104733&scipsc=&q=GrOD%3A+Deep+Learning+with+Gradients+Orthogonal+Decomposition+for+Knowledge+Transfer%2C+Distillation%2C+and+Adversarial+Training&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       The key challenge of zero-shot learning (ZSL) is how to infer the latent semantic knowledge between visual and attribute features on seen classes, and thus achieving a desirable knowledge transfer to unseen classes. Prior works either simply align the global features of an image with its associated class semantic vector or utilize unidirectional attention to learn the limited latent semantic representations, which could not effectively discover the intrinsic semantic knowledge (e.g., attribute semantics) between visual and attribute features. To solve the above dilemma, we propose a Mutually Semantic Distillation Network (MSDN), which progressively distills the intrinsic semantic representations between visual and attribute features for ZSL. MSDN incorporates an attribute!visual attention sub-net that learns attribute-based visual features, and a visual attribute attention sub-net that learns visual-based attribute features. By further introducing a semantic distillation loss, the two mutual attention sub-nets are capable of learning collaboratively and teaching each other throughout the training process. The proposed MSDN yields significant improvements over the strong baselines, leading to new state-of-the-art performances on three popular challenging benchmarks, i.e., CUB, SUN, and AWA2. Our codes have been available at: https://github.com/shimingchen/MSDN.
                                                            <p>Shiming Chen, Ziming Hong, Guosen Xie, Wenhan Yang, Qinmu Peng, Kai Wang, <b>Jian Zhao</b>, and Xinge You</p>
							    <p>CVPR 2022&nbsp; <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_MSDN_Mutually_Semantic_Distillation_Network_for_Zero-Shot_Learning_CVPR_2022_paper.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=MSDN%3A+Mutually+Semantic+Distillation+Network+for+Zero-Shot+Learning&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/EpzaYMd49QZk8qE0Q271iA">WeChat News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Single-Stage is Enough: Multi-Person Absolute 3D Pose Estimation</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       The existing multi-person absolute 3D pose estimation methods are mainly based on two-stage paradigm, i.e., topdown or bottom-up, leading to redundant pipelines with high computation cost. We argue that it is more desirable to simplify such two-stage paradigm to a single-stage one to promote both efficiency and performance. To this end, we present an efficient single-stage solution, Decoupled Regression Model (DRM), with three distinct novelties. First, DRM introduces a new decoupled representation for 3D pose, which expresses the 2D pose in image plane and depth information of each 3D human instance via 2D center point (center of visible keypoints) and root point (denoted as pelvis), respectively. Second, to learn better feature representation for the human depth regression, DRM introduces a 2D Poseguided Depth Query Module (PDQM) to extract the features in 2D pose regression branch, enabling the depth regression branch to perceive the scale information of instances. Third, DRM leverages a Decoupled Absolute Pose Loss (DAPL) to facilitate the absolute root depth and root-relative depth estimation, thus improving the accuracy of absolute 3D pose. Comprehensive experiments on challenging benchmarks including MuPoTS-3D and Panoptic clearly verify the superiority of our framework, which outperforms the state-of-the-art bottom-up absolute 3D pose estimation methods.
                                                            <p>Lei Jin, Chenyang Xu, Xiaojuan Wang, Yabo Xiao, Yandong Guo, Xuecheng Nie, and <b>Jian Zhao</b></p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>CVPR 2022&nbsp; <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jin_Single-Stage_Is_Enough_Multi-Person_Absolute_3D_Pose_Estimation_CVPR_2022_paper.pdf">PDF</a>, <a target="_blank" href="pub/cvpr22.pdf">Poster</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Single-Stage+is+Enough%3A+Multi-Person+Absolute+3D+Pose+Estimation&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/WKIcWhl3sW_MlJb6IUkrgg">CSIG News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Toward High-Quality Face-Mask Occluded Restoration</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Face-mask occluded restoration aims to restore the masked region of a human face, which has attracted increasing attention in the context of the COVID-19 pandemic. One major challenge of this task is the large visual variance of masks in the real world. To solve it we first construct a large-scale Face-mask Occluded Restoration (FMOR) dataset, which contains 5,500 unmasked images and 5,500 face-mask occluded images with various illuminations, and involves 1,100 subjects of different races, face orientations and mask types. Moreover we propose a Face-Mask Occluded Detection and Restoration (FMODR) framework, which can detect face-mask regions with large visual variations and restore them to realistic human faces. In particular, our FMODR contains a self-adaptive contextual attention module specifically designed for this task, which is able to exploit the contextual information and correlations of adjacent pixels for achieving high realism of the restored faces, which are however often neglected in existing contextual attention models. Our framework achieves state-of-the-art results of face restoration on three datasets, including CelebA, AR and our FMOR datasets. Moreover, experimental results on AR and FMOR datasets demonstrate that our framework can significantly improve masked face recognition and verification performance.
                                                            <p>Feihong Lu, Hang Chen, Kang Li, Qiliang Deng, <b>Jian Zhao</b>, Kaipeng Zhang, and Hong Han</p>
							    <p>T-OMM&nbsp; <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3524137">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Toward+High-quality+Face-Mask+Occluded+Restoration&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Grouping by Center: Predicting Centripetal Offsets for the Bottom-up Human Pose Estimation</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       We introduce Grouping by Center, a novel grouping approach for the bottom-up human pose estimation, which detects human joint first and then does grouping. The grouping strategy is the critical factor for the bottom-up pose estimation. To increase the conciseness and accuracy, we propose to use the center of the body as a grouping clue. More concretely, we predict the offsets from the keypoints to the body centers. Keypoints with aligned shifted results will be grouped as one person. However, the multi-scale variance of people can affect the prediction of the grouping clue, which has been neglected in previous research. To resolve the scale variance of the offset, we put forward a Multiscale Translation Layer and an iterative refinement. Furthermore, we scheme a greedy grouping strategy with a dynamic threshold due to the various scales of instances. Through a comprehensive comparison, our framework is validated to be effective and practical. We also lay out the state-of-the-art performance revolving the bottom-up multi-person pose estimation on the MS-COCO dataset and the CrowdPose dataset.
                                                            <p>Lei Jin, Xiaojuan Wang, Xuecheng Nie, Luoqi Liu, Yandong Guo, and <b>Jian Zhao</b></p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>T-MM&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9735381">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Grouping+by+Center%3A+Predicting+Centripetal+Offsets+for+the+Bottom-up+Human+Pose+Estimation&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/WKIcWhl3sW_MlJb6IUkrgg">CSIG News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2022</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Diverse Complementary Part Mining for Weakly Supervised Object Localization</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Weakly Supervised Object Localization (WSOL) aims to localize objects with only image-level labels, which has better scalability and practicability than fully supervised methods in the actual deployment. However, a common limitation for available techniques based on classification networks is that they only highlight the most discriminative part of the object, not the entire object. To alleviate this problem, we propose a novel end-to-end part discovery model (PDM) to learn multiple discriminative object parts in a unified network for accurate object localization and classification. The proposed PDM enjoys several merits. First, to the best of our knowledge, it is the first work to directly model diverse and robust object parts by exploiting part diversity, compactness, and importance jointly for WSOL. Second, three effective mechanisms including diversity, compactness, and importance learning mechanisms are designed to learn robust object parts. Therefore, our model can exploit complementary spatial information and local details from the learned object parts, which help to produce precise bounding boxes and discriminate different objects. Extensive experiments on two standard benchmarks demonstrate that our PDM performs favorably against state-of-the-art WSOL approaches.
                                                            <p>Meng Meng, Tianzhu Zhang, Wenfei Yang, <b>Jian Zhao</b>, Yongdong Zhang, and Feng Wu</p>
							    <p>T-IP&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9700771">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=en&as_sdt=0%2C5&q=Diverse+Complementary+Part+Mining+for+Weakly+Supervised+Object+Localization&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
				           <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">The 2nd Anti-UAV Workshop & Challenge: Methods and Results</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       The 2nd Anti-UAV Workshop & Challenge aims to encourage research in developing novel andaccurate methods for multi-scale object tracking. The Anti-UAV dataset was used for the Anti-UAVChallenge and is publicly released. There are two subsets in the dataset,i.e., the test-dev subset andtest-challenge subset. Both subsets consist of 140 thermal infrared video sequences, spanning multipleoccurrences of multi-scale UAVs. Around 24 participating teams from the globe competed in the 2ndAnti-UAV Challenge. In this paper, we provide a brief summary of the 2nd Anti-UAV Workshop &Challenge including brief introductions to the top three methods.The submission leaderboard will bereopened for researchers that are interested in the Anti-UAV challenge. The benchmark dataset andother information can be found at: https://anti-uav.github.io/.
                                                            <p><b>Jian Zhao</b>, Gang Wang, Jianan Li, Lei Jin, Nana Fan, Min Wang, Xiaojuan Wang, Ting Yong, Yafeng Deng, Yandong Guo, and Shiming Ge</p>
							    <p>ArXiv&nbsp; <a target="_blank" href="https://arxiv.org/pdf/2108.09909.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=THE+2ND+ANTI-UAV+WORKSHOP+%26+CHALLENGE%3A+METHODS+ANDRESULTS&btnG=">BibTeX</a>, <a target="_blank" href="https://www.geekpark.net/news/282330">Qihoo 360 Summary News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/ZGGjUzn3TDeAV_JeI2j5ZQ">CJIG Summary News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/Z5Qk4QxxRMqPWcbXupXiRQ">BSIG Summary News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Dense Attentive Feature Enhancement for Salient Object Detection</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Attention mechanisms have been proven highly effective for salient object detection. Most previous works utilize attention as a self-gated module to reweigh the feature maps at different levels independently. However, they are limited to certain-level guidance and could not satisfy the need of both accurately detecting intact objects and maintaining their detailed boundaries. In this paper, we build dense attention upon features from multiple levels simultaneously and propose a novel Dense Attentive Feature Enhancement (DAFE) module for efficient feature enhancement in saliency detection. DAFE stacks several attentional units and densely connects attentive feature output from current unit to its all subsequent units. This allows feature maps at deep units to absorb attentive information from shallow units, thus more discriminative information can be efficiently selected at the final output. Note that DAFE is plug and play, which can be effortlessly inserted into any saliency or video saliency models for their performance improvements. We further instantiate a highly effective Dense Attentive Feature Enhancement Network (DAFE-Net) for accurate salient object detection. DAFE-Net constructs DAFE over the aggregation feature that contains both semantics and saliency details, the entire salient objects and their boundaries can be well retained through dense attentions. Extensive experiments demonstrate that the proposed DAFE module is highly effective, and the DAFE-Net performs favorably compared with state-of-the-art approaches.
                                                            <p>Zun Li, Congyan Lang, Liqian Liang, <b>Jian Zhao</b>, Songhe Feng, Qibin Hou, and Jiashi Feng</p>
							    <p>T-CSVT&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9508442">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Dense+Attentive+Feature+Enhancement+for+Salient+Object+Detection&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-caption Text-to-Face Synthesis: Dataset and Algorithm</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Text-to-Face synthesis with multiple captions is still an important yet less addressed problem because of the lack of effective algorithms and large-scale datasets. We accordingly propose a Semantic Embedding and Attention (SEA-T2F) network that allows multiple captions as input to generate highly semantically related face images. With a novel Sentence Features Injection Module, SEA-T2F can integrate any number of captions into the network. In addition, an attention mechanism named Attention for Multiple Captions is proposed to fuse multiple word features and synthesize fine-grained details. Considering text-to-face generation is an ill-posed problem, we also introduce an attribute loss to guide the network to generate sentence-related attributes. Existing datasets for text-to-face are either too small or roughly generated according to attribute labels, which is not enough to train deep learning based methods to synthesize natural face images. Therefore, we build a large-scale dataset named CelebAText-HQ, in which each image is manually annotated with 10 captions. Extensive experiments demonstrate the effectiveness of our algorithm.
                                                            <p>Jianxin Sun, Qi Li, Weining Wang, <b>Jian Zhao</b>, and Zhenan Sun</p>
                                                            <p>ACM MM 2021&nbsp; <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3474085.3475391">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Multi-caption+Text-to-Face+Synthesis%3A+Dataset+and+Algorithm&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/pBYa6hEOawBfVlKl5y1Gmw">CSIG News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/amSoe2tIYG4D_HvSvVcNzg">BSIG News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/DC3Bn6FE-VktmUmzncNtkg">WeChat News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Seeing Crucial Parts: Vehicle Model Verification via A Discriminative Representation Model</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Widely-used surveillance cameras have promoted large amounts of street scene data, which contains one important but long-neglected object: vehicle. Here we focus on the challenging problem of vehicle model verification. Most previous works usually employ global features (e.g., fully-connected features) to further perform vehicle-level deep metric learning (e.g., triplet-based network). However, we argue that it is noteworthy to investigate the distinctiveness of local features and consider vehicle-part-level metric learning by reducing the intra-class variance as much as possible. In this paper, we introduce a simple yet powerful deep model, i.e., enforced intra-class alignment network (EIA-Net), which can learn a more discriminative image representation by localizing key vehicle parts and jointly incorporating two distance metrics: vehicle-level embedding and vehicle-part-sensitive embedding. For learning features, we propose an effective feature extraction module which is composed of two components: Regional Proposal Network (RPN)-based network and Part-based CNN. RPN-based network is used to define key vehicle regions and aggregate local features on these regions, while Part-based CNN offers supplementary global features for RPN-based network. The fusion features learned by feature extraction module are cast into deep metric learning module. Especially, we derived an enforced intraclass alignment loss (EIAL) by re-utilizing key vehicle part information to enhance reducing intra-class variance. Furthermore, we modify the coupled cluster loss (CCL) to model the vehicle-level embedding by enlarging the inter-class variance while shortening intra-class variance. Extensive experiments over benchmark datasets VehicleID and CompCars have shown that the proposed EIA-Net significantly outperforms the state-of-the-art approaches for vehicle model verification. Furthermore, we also conduct comprehensive experiments on vehicle Re-ID datasets, i.e., VehicleID and VeRi776, to validate the generalization ability effectiveness of our proposed method.
                                                            <p>Liqian Liang, Congyan Lang, Zun Li, <b>Jian Zhao</b>, Tao Wang, and Songhe Feng</p>
							    <p>T-OMM&nbsp; <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3474596">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Seeing+Crucial+Parts%3A+Vehicle+Model+Verification+via+A+Discriminative+Representation+Model&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Face.evoLVe: A High-Performance Face Recognition Library</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       While face recognition has drawn much attention, a large number of algorithms and models have been proposed with applications to daily life, such as authentication for mobile payments, etc. Recently, deep learning methods have dominated in the field of face recognition with advantages in comparisons to conventional approaches and even the human perception. Despite the popular adoption of deep learning-based methods to the field, researchers and engineers frequently need to reproduce existing algorithms with unified implementations (i.e., the identical deep learning framework with standard implementations of operators and trainers) and compare the performance of face recognition methods under fair settings (i.e., the same set of evaluation metrics and preparation of datasets with tricks on/off ), so as to ensure the reproducibility of experiments. To the end, we develop face.evoLVe — a comprehensive library that collects and implements a wide range of popular deep learningbased methods for face recognition. First of all, face.evoLVe is composed of key components that cover the full process of face analytics, including face alignment, data processing, various backbones, losses, and alternatives with bags of tricks for improving performance. Later, face.evoLVe supports multi-GPU training on top of different deep learning platforms, such as PyTorch and PaddlePaddle, which facilitates researchers to work on both large-scale datasets with millions of images and and low-shot counterparts with limited well-annotated data. More importantly, along with face.evoLVe, images before & after alignment in the common benchmark datasets are released with source codes and trained models provided. All these efforts lower the technical burdens in reproducing the existing methods for comparison, while users of our library could focus on developing advanced approaches more efficiently. Last but not least, face.evoLVe is well designed and vibrantly evolving, so that new face recognition approaches can be easily plugged into our framework. Note that we have used face.evoLVe to participate in a number of face recognition competitions and secured the first place. The version that supports PyTorch is publicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the PaddlePaddle version is available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle. Face.evoLVe has been widely used for face analytics, receiving 2.4K stars and 622 forks.
                                                            <p>Qingzhong Wang, Pengfei Zhang, Haoyi Xiong, and <b>Jian Zhao</b></p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>Neurocomputing&nbsp; <a target="_blank" href="https://arxiv.org/pdf/2107.08621.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Face.evoLVe%3A+A+High-Performance+Face+Recognition+Library&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					   <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Image-to-Video Generation via 3D Facial Dynamics</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       We present a versatile model, FaceAnime, for various video generation tasks from still images. Video generation from a single face image is an interesting problem and usually tackled by utilizing Generative Adversarial Networks (GANs) to integrate information from the input face image and a sequence of sparse facial landmarks. However, the generated face images usually suffer from quality loss, image distortion, identity change, and expression mismatching due to the weak representation capacity of the facial landmarks. In this paper, we propose to “imagine” a face video from a single face image according to the reconstructed 3D face dynamics, aiming to generate a realistic and identity-preserving face video, with precisely predicted pose and facial expression. The 3D dynamics reveal changes of the facial expression and motion, and can serve as a strong prior knowledge for guiding highly realistic face video generation. In particular, we explore face video prediction and exploit a well-designed 3D dynamic prediction network to predict a 3D dynamic sequence for a single face image. The 3D dynamics are then further rendered by the sparse texture mapping algorithm to recover structural details and sparse textures for generating face frames. Our model is versatile for various AR/VR and entertainment applications, such as face video retargeting and face video prediction. Superior experimental results have well demonstrated its effectiveness in generating high-fidelity, identitypreserving, and visually pleasant face video clips from a single source face image.
                                                            <p>Xiaoguang Tu, Yingtian Zou, <b>Jian Zhao</b>, Wenjie Ai, Jian Dong, Yuan Yao, Zhikang Wang, Guodong Guo, Zhifeng Li, Wei Liu, and Jiashi Feng</p>
							    <p>T-CSVT&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9439899">PDF</a>, <a href="https://scholar.google.com.hk/scholar?q=Image-to-Video+Generation+via+3D+Facial+Dynamics&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/jWitXdzkMhnsDLGfxR26vg">BSIG News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/Ab9YEoTTSzBrpblWZjJwsw">WeChat News1</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/lqGqYeuiM4fplZv4LBsdaA">WeChat News2</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Joint Face Image Restoration and Frontalization for Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       In real-world scenarios, many factors may harm face recognition performance, e.g., large pose, bad illumination, low resolution, blur and noise. To address these challenges, previous efforts usually first restore the low-quality faces to high-quality ones and then perform face recognition. However, most of these methods are stage-wise, which is sub-optimal and deviates from the reality. In this paper, we address all these challenges jointly for unconstrained face recognition. We propose an Multi-Degradation Face Restoration (MDFR) model to restore frontalized high-quality faces from the given low-quality ones under arbitrary facial poses, with three distinct novelties. First, MDFR is a well-designed encoder-decoder architecture which extracts feature representation from an input face image with arbitrary low-quality factors and restores it to a high-quality counterpart. Second, MDFR introduces a pose residual learning strategy along with a 3D-based Pose Normalization Module (PNM), which can perceive the pose gap between the input initial pose and its real-frontal pose to guide the face frontalization. Finally, MDFR can generate frontalized high-quality face images by a single unified network, showing a strong capability of preserving face identity. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of MDFR over state-of-the-art methods on both face frontalization and face restoration.
                                                            <p>Xiaoguang Tu, <b>Jian Zhao</b>, Qiankun Liu, Wenjie Ai, Guodong Guo, Zhifeng Li, Wei Liu, and Jiashi Feng</p>
							    <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>T-CSVT&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9427073">PDF</a>, <a href="https://scholar.google.com.hk/scholar?q=Joint+Face+Image+Restoration+and+Frontalization+for+Recognition&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/FHxuQ370bRk35QuwSEbj6w">CSIG News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/CHZXFbk4KJuWCaurJHbqTg">BSIG News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Robust Video-based Person Re-Identification by Hierarchical Mining</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Video-based person re-identification (Re-ID) aims at retrieving the person through the video sequences across non- overlapping cameras. Some characteristics of pedestrians are not consecutive across frames due to the variations of viewpoints, postures, and occlusions over time. However, existing methods ignore such data peculiarity and the networks tend to only learn those salient consecutive characteristics among frames in video sequences. As a result, the learned representations fail to cover all the characteristics of pedestrians, thus lacking integrity and discrimination. To tackle this problem, we present a novel deep architecture termed Hierarchical Mining Network (HMN), which mines as many pedestrians’ characteristics by referring to the temporal and intra-class knowledge. It consists of a novel Attentive Temporal Module (ATM) and a Dynamic Supervising Branch (DSB), with a Balancing Triplet Loss (BTL) assisting the training. The proposed ATM, with pedestrian perceiving capacity, is capable of evaluating each activation of features through temporal analysis, so that the temporally scattered characteristics of pedestrians can be better aggregated and the contaminated ones can be eliminated. Then, the DSB along with the BTL further enhances the integrity of representations by multiple supervision. Specifically, the DSB perceives the diversities of intra-class samples in each mini-batch and generates targeted supervising signals for them, in which process the BTL guarantees the signals with smaller intra-class variations and larger inter-class variations. Comprehensive experiments on two video-based datasets, i.e., MARS, and DukeMTMC- VideoReID, demonstrate the contribution of each component and the superiority of the proposed HMN over the state-of-the-arts. Benchmarking our model on three popular image-based datasets, i.e., Market1501, DukeMTMC-Reid, and MSMT17 additionally verifies the promising generalizability of the proposed DSB and BTL.
                                                            <p>Zhikang Wang, Lihuo He, Xiaoguang Tu, <b>Jian Zhao</b>, Xinbo Gao, Shengmei Shen, and Jiashi Feng</p>
							    <p>T-CSVT&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/document/9416694">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Robust+Video-based+Person+Re-Identification+by+Hierarchical+Mining&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Images Inpainting via Structure Guidance</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Aiming at the problem of obvious visual artifacts in the content of rough network with less prior knowledge, a two-stage image inpainting method based on an edge structure generator is proposed. The edge structure generator is used to perform feature learning on the input image edge and color smoothing information, and generate the missing structural contents so as to guide the fine network to reconstruct high-quality semantic images. The mentioned method has been tested on the public benchmark datasets such as Paris Street-View. The experimental results show that the proposed approach can complete the hole images with the mask rate of 50%. The quantitative evaluation indicators: PSNR, SSIM, L1 and L2 errors respectively surpass current images inpainting algorithms with excellent performance, such as EC, GC, SF, etc. Among them, when the mask rate is 0%-20%, the PSNR index reaches 33.40 dB, which is an increase of 2.37-6.57 dB compared to other methods; the SSIM index is increased by 0.006-0.138. Meanwhile, the completed images get clearer texture and higher visual quality.
                                                            <p>Kai Hu, <b>Jian Zhao</b>, Yu Liu, Yukai Niu, and Gang Ji</p>
                                                            <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>Journal of BUAA&nbsp; <a target="_blank" href="https://bhxb.buaa.edu.cn/bhzk/en/article/doi/10.13700/j.bh.1001-5965.2021.0004?viewType=HTML">PDF</a>, <a href="https://bhxb.buaa.edu.cn/bhzk/en/article/doi/10.13700/j.bh.1001-5965.2021.0004?viewType=HTML">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Anti-UAV: A Large Multi-Modal Benchmark for UAV Tracking</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Unmanned Aerial Vehicle (UAV) offers lots of applications in both commerce and recreation. Therefore, perception of the status of UAVs is crucially important. In this paper, we consider the task of tracking UAVs, providing rich information such as location and trajectory. To facilitate research on this topic, we introduce a new benchmark, referred to as Anti-UAV, opening up a promising direction for UAV tracking in a long distance with more than 300 video pairs containing over 580k manually annotated bounding boxes. Furthermore, the advancement of addressing research challenges in Anti-UAV can help the design of anti-UAV systems, leading to better surveillance of UAVs. Accordingly, a simple yet effective approach named dual-flow semantic consistency (DFSC) is proposed for UAV tracking. Modulated by the semantic flow across video sequences, the tracker learns more robust class-level semantic information and obtains more discriminative instance-level features. Experiments show the significant performance gain of our proposed approach over state-of-the-art trackers, and the challenging aspects of Anti-UAV. The Anti-UAV benchmark and the code of the proposed approach will be publicly available at https://github.com/ucas-vg/Anti-UAV.
                                                            <p>Nan Jiang, Kuiran Wang, Xiaoke Peng, Xuehui Yu, Qiang Wang, Junliang Xing, Guorong Li, <b>Jian Zhao</b>, and Zhenjun Han</p>
                                                            <p><b>(Corresponding Author: Jian Zhao)</b></p>
							    <p>T-MM&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/document/9615243">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Anti-UAV%3A+A+Large+Multi-Modal+Benchmark+for+UAV+Tracking&btnG=">BibTeX</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/4OSlvDykh-vIcwNkJfZW3w">CSIG News</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/9s_ds-QAvBaDfa5LTy1MNQ">BSIG News</a>, <a href="pub/Anti-UAV.pdf" target="_blank">WeChat News</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2021</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Effective Fusion Factor in FPN for Tiny Object Detection</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       FPN-based detectors have made significant progress in general object detection, e.g., MS COCO and PASCAL VOC. However, these detectors fail in certain application scenarios, e.g., tiny object detection. In this paper, we argue that the top-down connections between adjacent layers in FPN bring two-side influences for tiny object detection, not only positive. We propose a novel concept, fusion factor, to control information that deep layers deliver to shallow layers, for adapting FPN to tiny object detection. After series of experiments and analysis, we explore how to estimate an effective value of fusion factor for a particular dataset by a statistical method. The estimation is dependent on the number of objects distributed in each layer. Comprehensive experiments are conducted on tiny object detection datasets, e.g., TinyPerson and Tiny CityPersons. Our results show that when configuring FPN with a proper fusion factor, the network is able to achieve significant performance gains over the baseline on tiny object detection datasets. Codes and models will be released.
                                                            <p>Yuqi Gong, Xuehui Yu, Yao Ding, Xiaoke Peng, <b>Jian Zhao</b>, and Zhenjun Han</p>
                                                            <p>WACV 2021&nbsp; <a target="_blank" href="https://openaccess.thecvf.com/content/WACV2021/papers/Gong_Effective_Fusion_Factor_in_FPN_for_Tiny_Object_Detection_WACV_2021_paper.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Effective+Fusion+Factor+in+FPN+for+Tiny+Object+Detection&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2020</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">The 1st Tiny Object Detection Challenge: Methods and Results</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       The 1st Tiny Object Detection (TOD) Challenge aims to encourage research in developing novel and accurate methods for tiny object detection in images which have wide views, with a current focus on tiny person detection. The TinyPerson dataset was used for the TOD Challenge and is publicly released. It has 1610 images and 72651 box-level annotations. Around 36 participating teams from the globe competed in the 1st TOD Challenge. In this paper, we provide a brief summary of the 1st TOD Challenge including brief introductions to the top three methods.The submission leaderboard will be reopened for researchers that are interested in the TOD challenge. The benchmark dataset and other information can be found at: https://github.com/ucas-vg/TinyBenchmark.
                                                            <p>Xuehui Yu, Zhenjun Han, Yuqi Gong, Nan Jan, <b>Jian Zhao</b>, Qixiang Ye, Jie Chen, Yuan Feng, Bin Zhang, Xiaodi Wang, Ying Xin, Jingwei Liu, Mingyuan Mao, Sheng Xu, Baochang Zhang, Shumin Han, Cheng Gao, Wei Tang, Lizuo Jin, Mingbo Hong, Yuchao Yang, Shuiwang Li, Huan Luo, Qijun Zhao, and Humphrey Shi</p>
                                                            <p>ArXiv&nbsp; <a target="_blank" href="https://arxiv.org/pdf/2009.07506.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=The+1st+Tiny+Object+Detection+Challenge%3A+Methods+and+Results&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2020</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-Human Parsing With a Graph-based Generative Adversarial Model</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Human parsing is an important task in human-centric image understanding in computer vision and multimedia systems. However, most existing works on human parsing mainly tackle the single-person scenario, which deviates from real-world applications where multiple persons are present simultaneously with interaction and occlusion. To address such a challenging multi-human parsing problem, we introduce a novel multi-human parsing model named MH-Parser, which uses a graph-based generative adversarial model to address the challenges of close person interaction and occlusion in multi-human parsing. To validate the effectiveness of the new model, we collect a new dataset named Multi-Human Parsing (MHP), which contains multiple persons with intensive person interaction and entanglement. Experiments on the new MHP dataset and existing datasets demonstrate that the proposed method is effective in addressing the multi-human parsing problem compared with existing solutions in the literature.
                                                            <p>Jianshu Li, <b>Jian Zhao</b>, Congyan Lang, Yidong Li, Yunchao Wei, Gudong Guo, Terence Sim, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
                                                            <p>T-OMM&nbsp; <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3418217">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Multi-Human+Parsing+With+a+Graph-based+Generative+Adversarial+Model&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2020</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Towards Age-Invariant Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, effective and novel training strategies are developed for end-to-end learning of the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we construct a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR dataset and several other cross-age datasets (MORPH, CACD, and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on the popular unconstrained face recognition datasets YTF and IJB-C additionally verifies its promising generalization ability in recognizing faces in the wild.
                                                            <p><b>Jian Zhao</b>, Shuicheng Yan, and Jiashi Feng</p>
							    <p><b>(ESI Highly Cited Paper)</b></p>
                                                            <p>T-PAMI&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/document/9146699">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Towards+Age-Invariant+Face+Recognition&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2020</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Unsupervised Domain Adaptation with Noise Resistible Mutual-Training for Person Re-identification</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Unsupervised domain adaptation (UDA) in the task of person re-identification (re-ID) is highly challenging due to large domain divergence and no class overlap between domains. Pseudo-label based self-training is one of the representative techniques to address UDA. However, label noise caused by unsupervised clustering is always a trouble to self-training methods. To depress noises in pseudo-labels, this paper proposes a Noise Resistible Mutual-Training (NRMT) method, which maintains two networks during training to perform collaborative clustering and mutual instance selection. On one hand, collaborative clustering eases the fitting to noisy instances by allowing the two networks to use pseudo-labels provided by each other as an additional supervision. On the other hand, mutual instance selection further selects reliable and informative instances for training according to the peer-confidence and relationship disagreement of the networks. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art UDA methods for person re-ID.
                                                            <p>Fang Zhao, Shengcai Liao, Guosen Xie, <b>Jian Zhao</b>, Kaihao Zhang, and Ling Shao</p>
                                                            <p>ECCV 2020&nbsp; <a target="_blank" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560511.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Unsupervised+Domain+Adaptation+with+Noise+Resistible+Mutual-Training+for+Person+Re-identification&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2020</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Fine-Grained Facial Expression Recognition in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Over the past decades, researches on facial expression recognition have been restricted within six basic expressions (anger, fear, disgust, happiness, sadness and surprise). However, these six words can not fully describe the richness and diversity of human beings' emotions. To enhance the recognitive capabilities for computers, in this paper, we focus on fine-grained facial expression recognition in the wild and build a brand new benchmark FG-Emotions to push the research frontiers on this topic, which extends the original six classes to more elaborate thirty-three classes. Our FG-Emotions contains 10,371 images and 1,491 video clips annotated with corresponding fine-grained facial expression categories and landmarks. FG-Emotions also provides several features (e.g., LBP features and dense trajectories features) to facilitate related research. Moreover, on top of FG-Emotions, we propose a new end-to-end Multi-Scale Action Unit (AU)-based Network (MSAU-Net) for facial expression recognition with image which learns a more powerful facial representation by directly focusing on locating facial action units and utilizing "zoom in" operation to aggregate distinctive local features. As for recognition with video, we further extend the MSAU-Net to a two-stream model (TMSAU-Net ) by adding a module with attention mechanism and a temporal stream branch to jointly learn spatial and temporal features. (T)MSAU-Net consistently outperforms existing state-of-the-art solutions on our FG-Emotions and several other datasets, and serves as a strong baseline to drive the future research towards fine-grained facial expression recognition in the wild.
                                                            <p>Liqian Liang, Congyan Lang, Yidong Li, Songhe Feng, and <b>Jian Zhao</b></p>
                                                            <p>T-IFS&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9133437">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Fine-Grained+Facial+Expression+Recognition+in+the+Wild&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2020</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Face anti-spoofing aims to detect presentation attack to face recognition based authentication systems. It has drawn growing attention due to the high security demand. The widely adopted CNN-based methods usually well recognize the spoofing faces when training and testing spoofing samples display similar patterns, but their performance would drop drastically on testing spoofing faces of novel patterns or unseen scenes, leading to poor generalization performance. Furthermore, almost all current methods treat face anti-spoofing as a prior step to face recognition, which prolongs the response time and makes face authentication inefficient. In this paper, we try to boost the generalizability and applicability of face anti-spoofing methods by designing a new Generalizable Face Authentication CNN (GFA-CNN) model with three novelties. First, GFA-CNN introduces a simple yet effective Total Pairwise Confusion (TPC) loss for CNN training which properly balances contributions of all the spoofing patterns for recognizing the spoofing faces. Secondly, it incorporate a Fast Domain Adaptation (FDA) component to alleviate negative effect brought by domain variation. Thirdly, it deploys the Filter Diversification Learning (FDL) to make the learned representations more adaptable to new scenes. Besides, the proposed GFA-CNN works in a multi-task manner—it performs face anti-spoofing and face recognition simultaneously. Experimental results on five popular face anti-spoofing and face recognition benchmarks show that GFA-CNN outperforms previous face anti-spoofing methods on cross-test protocol significantly and also well preserves the identity information of input face images.
                                                            <p>Xiaoguang Tu, Zheng Ma, <b>Jian Zhao</b>, Guodong Du, Mei Xie, and Jiashi Feng</p>
                                                            <p>T-IST&nbsp; <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3402446">PDF</a>, <a href="https://scholar.google.com.hk/scholar?cluster=10647949525077973142&hl=zh-CN&newwindow=1&as_sdt=0,5&sciodt=0,5">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2020</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">3D Face Reconstruction from A Single Image Assisted by 2D Face Images in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       3D face reconstruction from a single image is an important task in many multimedia applications. Recent works typically learn a CNN-based 3D face model that regresses coefficients of a 3D Morphable Model (3DMM) from 2D images to perform 3D face reconstruction. However, the shortage of training data with 3D annotations considerably limits performance of these methods. To alleviate this issue, we propose a novel 2D-Assisted Learning (2DAL) method that can effectively use “in the wild” 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmark heatmaps as additional information, 2DAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the landmark self-prediction consistency for 2D and 3D faces respectively, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark prediction. Using these four self-supervision schemes, 2DAL significantly relieves the demands for the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on AFLW2000-3D, AFLW-LFPA and Florence benchmarks show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.
                                                            <p>Xiaoguang Tu, <b>Jian Zhao</b>, Mei Xie, Zihang Jiang, Akshaya Balamurugan, Yao Luo, Yang Zhao, Lingxiao He, Zheng Ma, and Jiashi Feng</p>
                                                            <p>T-MM&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9091237">PDF</a>, <a href="https://scholar.google.com.hk/scholar?hl=zh-CN&newwindow=1&as_sdt=0%2C5&q=3D+Face+Reconstruction+from+A+Single+Image+Assisted+by+2D+Face+Images+in+the+Wild&btnG=">BibTeX</a>, <a target="_blank" href="https://github.com/XgTu/2DASL">Code</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2020</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real- world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-art by a significant margin.
                                                            <p>Zhecan Wang, <b>Jian Zhao</b>, Cheng Lu, Han Huang, Fan Yang, Lianji Li, and Yandong Guo</p>
                                                            <p>WACV 2020&nbsp; <a target="_blank" href="https://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Learning_to_Detect_Head_Movement_in_Unconstrained_Remote_Gaze_Estimation_WACV_2020_paper.pdf">PDF</a>, <a href="http://openaccess.thecvf.com/content_WACV_2020/html/Wang_Learning_to_Detect_Head_Movement_in_Unconstrained_Remote_Gaze_Estimation_WACV_2020_paper.html">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Recognizing Profile Faces by Imagining Frontal View</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Extreme pose variation is one of the key obstacles to accurate face recognition in practice. Compared with current techniques for pose-invariant face recognition, which either expect pose invariance from hand-crafted features or data-driven deep learning solutions, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose-Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporating an unsupervised cross-domain adversarial training and a meta-learning (“learning to learn”) strategy using siamese discriminator with dynamic convolution for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representations with large intra-class affinity and inter-class separability. Qualitative and quantitative experiments on both controlled and in-the-wild benchmark datasets demonstrate the superiority of the proposed model over the state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Junliang Xing, Lin Xiong, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>IJCV&nbsp; <a target="_blank" href="http://link.springer.com/article/10.1007/s11263-019-01252-7">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Recognizing+Profile+Faces+by+Imagining+Frontal+View&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Cross-Resolution Face Recognition via Prior-Aided Face Hallucination and Residual Knowledge Distillation</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Recent deep learning based face recognition methods have achieved great performance, but it still remains challenging to recognize very low-resolution query face like 28×28 pixels when CCTV camera is far from the captured subject. Such face with very low-resolution is totally out of detail information of the face identity compared to normal resolution in a gallery and hard to find corresponding faces therein. To this end, we propose a Resolution Invariant Model (RIM) for addressing such cross-resolution face recognition problems, with three distinct novelties. First, RIM is a novel and unified deep architecture, containing a Face Hallucination sub-Net (FHN) and a Heterogeneous Recognition sub-Net (HRN), which are jointly learned end to end. Second, FHN is a well-designed tri-path Generative Adversarial Network (GAN) which simultaneously perceives facial structure and geometry prior information, i.e. landmark heatmaps and parsing maps, incorporated with an unsupervised cross-domain adversarial training strategy to super-resolve very low-resolution query image to its 8× larger ones without requiring them to be well aligned. Third, HRN is a generic Convolutional Neural Network (CNN) for heterogeneous face recognition with our proposed residual knowledge distillation strategy for learning discriminative yet generalized feature representation. Quantitative and qualitative experiments on several benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts. Codes and models are available at https://github.com/HyoKong/Cross-Resolution-Face-Recognition.
                                                            <p>Hanyang Kong, <b>Jian Zhao</b>, Xiaoguang Tu, Junliang Xing, Shengmei Shen, and Jiashi Feng</p>
                                                            <p>ArXiv&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1905.10777.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Cross-Resolution+Face+Recognition+via+Prior-Aided+Face+Hallucination+and+Residual+Knowledge+Distillation&btnG=">BibTeX</a>, <a target="_blank" href="https://github.com/HyoKong/Cross-Resolution-Face-Recognition">Code</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Fine-Grained Multi-Human Parsing</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification, e-commerce, media editing, video surveillance, autonomous driving and virtual reality, etc. To perform well, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we first present a new large-scale database "Multi-Human Parsing (MHP v2.0)" for algorithm development and evaluation to advance the research on understanding humans in crowded scenes. MHP v2.0 contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels and 16 dense pose key point labels, involving 2-26 persons per image captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, including MHP v1.0, PASCAL-Person-Part and Buffy. NAN serves as a strong baseline to shed light on generic instance-level semantic part prediction and drive the future research on multi-human parsing. With the above innovations and contributions, we have organized the CVPR 2018 Workshop on Visual Understanding of Humans in Crowd Scene (VUHCS 2018) and the Fine-Grained Multi-Human Parsing and Pose Estimation Challenge. These contributions together significantly benefit the community. Code and pre-trained models are available at https://github.com/ZhaoJ9014/Multi-Human-Parsing_MHP.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Hengzhu Liu, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>IJCV &nbsp; <a target="_blank" href="https://link.springer.com/epdf/10.1007/s11263-019-01181-5?author_access_token=qdcnTMMpNqoBKljeVKT0xPe4RwlQNchNByi7wbcMAY4gfv63yR6ONcJguASlD5_eqQor_07VGN5sSgAsudzKut87eH8GGFpg3PGGWV2LNmw9clFK4qXKz4S_s8qz7pm-rfslIr9Ydfe73oZnYKgvsw%3D%3D">PDF</a>, <a href="https://link.springer.com/article/10.1007%2Fs11263-019-01181-5#citeas">BibTeX</a>, MHP Dataset v2.0 & v1.0, annotation tools, and source codes for NAN and evaluation metrics <a href="https://github.com/ZhaoJ9014/Multi-Human-Parsing_MHP" target="_blank">Download</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-Prototype Networks for Unconstrained Set-based Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       In this paper, we study the challenging unconstrained set-based face recognition problem where each subject face is instantiated by a set of media (images and videos) instead of a single image. Naively aggregating information from all the media within a set would suffer from the large intraset variance caused by heterogeneous factors (e.g., varying media modalities, poses and illuminations) and fail to learn discriminative face representations. A novel MultiPrototype Network (MPNet) model is thus proposed to learn multiple prototype face representations adaptively from the media sets. Each learned prototype is representative for the subject face under certain condition in terms of pose, illumination and media modality. Instead of handcrafting the set partition for prototype learning, MPNet introduces a Dense SubGraph (DSG) learning sub-net that implicitly untangles inconsistent media and learns a number of representative prototypes. Qualitative and quantitative experiments clearly demonstrate superiority of the proposed model over state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Xiaoguang Tu, Fang Zhao, Yuan Xin, Junliang Xing, Hengzhu Liu, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>IJCAI 2019 (Oral)&nbsp; <a target="_blank" href="https://www.ijcai.org/Proceedings/2019/0611.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Multi-Prototype+Networks+for+Unconstrained+Set-based+Face+Recognition&btnG=">BibTeX</a>, Foundation of <a target="_blank" href="pub/Tencent_Face.jpeg">Tencent Face Scan Payment</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Task Relation Networks</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Multi-task learning is popular in machine learning and computer vision. In multitask learning, properly modeling task relations is important for boosting the performance of jointly learned tasks. Task covariance modeling has been successfully used to model the relations of tasks but is limited to homogeneous multi-task learning. In this paper, we propose a feature based task relation modeling approach, suitable for both homogeneous and heterogeneous multi-task learning. First, we propose a new metric to quantify the relations between tasks. Based on the quantitative metric, we then develop the task relation layer, which can be combined with any deep learning architecture to form task relation networks to fully exploit the relations of different tasks in an online fashion. Benefiting from the task relation layer, the task relation networks can better leverage the mutual information from the data. We demonstrate our proposed task relation networks are effective in improving the performance in both homogeneous and heterogeneous multi-task learning settings through extensive experiments on computer vision tasks.
                                                            <p>Jianshu Li, Pan Zhou, Yunpeng Chen, <b>Jian Zhao</b>, Sujoy Roy, Yan Shuicheng, Jiashi Feng, and Terence Sim</p>
                                                            <p>WACV 2019&nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/document/8658407">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Task+Relation+Networks&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2019</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recognizing faces in the wild.
                                                            <p><b>Jian Zhao</b>, Yu Cheng, Yi Cheng, Yang Yang, Haochong Lan, Fang Zhao, Lin Xiong, Yan Xu, Jianshu Li, Sugiri Pranata, Shengmei Shen, Junliang Xing, Hengzhu Liu, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>AAAI 2019 (Oral)&nbsp; <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/download/4961/4834">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Look+Across+Elapse%3A+Disentangled+Representation+Learning+and+Photorealistic+Cross-Age+Face+Synthesis+for+Age-Invariant+Face+Recognition&btnG=#d=gs_cit&p=&u=%2Fscholar%3Fq%3Dinfo%3AkfCI2TPozcgJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Dzh-CN">BibTeX</a>, <a target="_blank" href="https://github.com/ZhaoJ9014/High_Performance_Face_Recognition/tree/master/src/Look%20Across%20Elapse-%20Disentangled%20Representation%20Learning%20and%20Photorealistic%20Cross-Age%20Face%20Synthesis%20for%20Age-Invariant%20Face%20Recognition.TensorFlow">Code</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Object Relation Detection Based on One-shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Detecting the relations among objects, such as "cat on sofa" and "person ride horse", is a crucial task in image understanding, and beneficial to bridging the semantic gap between images and natural language. Despite the remarkable progress of deep learning in detection and recognition of individual objects, it is still a challenging task to localize and recognize the relations between objects due to the complex combinatorial nature of various kinds of object relations. Inspired by the recent advances in one-shot learning, we propose a simple yet effective Semantics Induced Learner (SIL) model for solving this challenging task. Learning in one-shot manner can enable a detection model to adapt to a huge number of object relations with diverse appearance effectively and robustly. In addition, the SIL combines bottom-up and top-down attention mech- anisms, therefore enabling attention at the level of vision and semantics favorably. Within our proposed model, the bottom-up mechanism, which is based on Faster R-CNN, proposes objects regions, and the top-down mechanism selects and integrates visual features according to semantic information. Experiments demonstrate the effectiveness of our framework over other state-of-the-art methods on two large-scale data sets for object relation detection.
                                                            <p>Li Zhou, <b>Jian Zhao</b>, Jianshu Li, Li Yuan, and Jiashi Feng</p>
                                                            <p>ArXiv&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1807.05857.pdf">PDF</a>, <a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Object+Relation+Detection+Based+on+One-shot+Learning&btnG=#d=gs_cit&p=&u=%2Fscholar%3Fq%3Dinfo%3A4I7gw3DQI9gJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Dzh-CN">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">3D-Aided Dual-Agent GANs for Unconstrained Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Synthesizing realistic profile faces is beneficial for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by augmenting the number of samples with extreme poses and avoiding costly annotation work. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator’s output using unlabeled real faces while preserving the identity information during the realism refinement. The dual agents are specially designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose, texture as well as identity, and stabilize the training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only achieves outstanding perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A and CFP unconstrained face recognition benchmarks. In addition, the proposed DA-GAN is also a promising new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our winning entry to the NIST IJB-A face recognition competition in which we secured the 1st places on the tracks of verification and identification.
                                                            <p><b>Jian Zhao</b>, Lin Xiong, Jianshu Li, Junliang Xing, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>T-PAMI &nbsp; <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/8417439">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=3D-Aided+Dual-Agent+GANs+for+Unconstrained+Face+Recognition&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Dynamic Conditional Networks for Few-Shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       This paper proposes a novel Dynamic Conditional Convolutional Network (DCCN) to handle conditional few-shot learning, i.e, only a few training samples are available for each condition. DCCN consists of dual subnets: DyConvNet contains a dynamic convolutional layer with a bank of basis filters; CondiNet predicts a set of adaptive weights from conditional inputs to linearly combine the basis filters. In this manner, a specific convolutional kernel can be dynamically obtained for each conditional input. The filter bank is shared between all conditions thus only a low-dimension weight vector needs to be learned. This significantly facilitates the parameter learning across different conditions when training data are limited. We evaluate DCCN on four tasks which can be formulated as conditional model learning, including specific object counting, multi-modal image classification, phrase grounding and identity based face generation. Extensive experiments demonstrate the superiority of the proposed model in the conditional few-shot learning setting.
                                                            <p>Fang Zhao, <b>Jian Zhao</b>, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
                                                            <p>ECCV 2018&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper.pdf">PDF</a>, <a target="_blank" href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Dynamic+Conditional+Networks+for+Few-Shot+Learning&btnG=">BibTeX</a>, <a target="_blank" href="pub/ECCV2018_POSTER.pdf">Poster</a>, <a target="_blank" href="https://github.com/ZhaoJ9014/Dynamic-Conditional-Networks-for-Few-Shot-Learning.pytorch">Code</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database “Multi-Human Parsing (MHP)” for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Yu Cheng, Li Zhou, Terence Sim, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>ACM MM 2018 (<a target="_blank" href="pub/ACM MM18_Best Student Paper.pdf">Best Student Paper</a>)&nbsp; <a target="_blank" href="https://web.archive.org/web/20200602102739id_/https://dl.acm.org/doi/pdf/10.1145/3240508.3240509?download=true">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Understanding+Humans+in+Crowded+Scenes%3A+Deep+Nested+Adversarial+Learning+and+A+New+Benchmark+for+Multi-Human+Parsing&btnG=">BibTeX</a>, <a href="https://mp.weixin.qq.com/s/0W3AOMTeOyngnM1WA-8C5w" target="_blank">WeChat News</a>, MHP Dataset v2.0 & v1.0, annotation tools, and source codes for NAN and evaluation metrics <a href="https://github.com/ZhaoJ9014/Multi-Human-Parsing_MHP" target="_blank">Download</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-Human Parsing Machines</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Human parsing is an important task in human-centric analysis. Despite the remarkable progress in single-human parsing, the more realistic case of multi-human parsing remains challenging in terms of the data and the model. Compared with the considerable number of available single-human parsing datasets, the datasets for multi-human parsing are very limited in number mainly due to the huge annotation effort required. Besides the data challenge to multi-human parsing, the persons in real-world scenarios are often entangled with each other due to close interaction and body occlusion, making it difficult to distinguish body parts from different person instances. In this paper we propose the Multi-Human Parsing Machines (MHPM), which contains an MHP Montage model and an MHP Solver, to address both challenges in multi-human parsing. Specifically, the MHP Montage model in MHPM generates realistic images with multiple persons together with the parsing labels. It intelligently composes single persons onto background scene images while maintaining the structural information between persons and the scene. The generated images can be used to train better multi-human parsing algorithms. On the other hand, the MHP Solver in MHPM solves the bottleneck of distinguishing multiple entangled persons with close interaction. It employs a Group-Individual Push and Pull (GIPP) loss function, which can effectively separate persons with close interaction. We experimentally show that the proposed MHPM can achieve state-of-the-art performance on the multi-human parsing benchmark and the person individualization benchmark, which distinguishes closely entangled person instances.
                                                            <p>Jianshu Li, <b>Jian Zhao</b>, Yunpeng Chen, Sujoy Roy, Shuicheng Yan, Jiashi Feng, and Terence Sim</p>
                                                            <p>ACM MM 2018&nbsp; <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3240508.3240515">PDF</a>, <a href="https://scholar.google.com.hk/scholar?q=Multi-Human+Parsing+Machines&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">3D-Aided Deep Pose-Invariant Face Recognition</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Learning from synthetic faces, though perhaps appealing for high data efficiency, may not bring satisfactory performance due to the distribution discrepancy of the synthetic and real face images. To mitigate this gap, we propose a 3D-Aided Deep Pose-Invariant Face Recognition Model (3D-PIM), which automatically recovers realistic frontal faces from arbitrary poses through a 3D face model in a novel way. Specifically, 3D-PIM incorporates a simulator with the aid of a 3D Morphable Model (3D MM) to obtain shape and appearance prior for accelerating face normalization learning, requiring less training data. It further leverages a global-local Generative Adversarial Network (GAN) with multiple critical improvements as a refiner to enhance the realism of both global structures and local details of the face simulator’s output using unlabelled real data only, while preserving the identity information. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks clearly demonstrate superiority of the proposed model over state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Lin Xiong, Yu Cheng, Yi Cheng, Jianshu Li, Li Zhou, Yan Xu, Karlekar Jayashree, Sugiri Pranata, Shengmei Shen, Junliang Xing, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>IJCAI 2018 (Oral)&nbsp; <a target="_blank" href="https://www.ijcai.org/proceedings/2018/0165.pdf">PDF</a>, <a href="https://www.ijcai.org/proceedings/2018/165">BibTeX</a>, Foundation of Panasonic FacePRO (YouTube <a href="https://www.youtube.com/watch?v=Z5vxhhM0JGQx">News1</a>, <a href="https://www.youtube.com/watch?v=J_-iRx7z1qQ">News2</a>)</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Towards Pose Invariant Face Recognition in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a "learning to learn" strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Yu Cheng, Yan Xu, Lin Xiong, Jianshu Li, Fang Zhao, Karlekar Jayashree, Sugiri Pranata, Shengmei Shen, Junliang Xing, Shuicheng Yan, and Jiashi Feng</p>
							    <p>CVPR 2018&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.pdf">PDF</a>, <a target="_blank" href="pub/CVPR18.pdf">Poster</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Towards+Pose+Invariant+Face+Recognition+in+the+Wild&btnG=">BibTeX</a>, Foundation of Panasonic FacePRO (YouTube <a href="https://www.youtube.com/watch?v=Z5vxhhM0JGQx">News1</a>, <a href="https://www.youtube.com/watch?v=J_-iRx7z1qQ">News2</a>)</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2018</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Weakly Supervised Phrase Localization with Multi-Scale Anchored Transformer Network</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       In this paper, we propose a novel weakly supervised model, Multi-scale Anchored Transformer Network (MATN), to accurately localize free-form textual phrases with only image-level supervision. The proposed MATN takes region proposals as localization anchors, and learns a multi-scale correspondence network to continuously search for phrase regions referring to the anchors. In this way, MATN can exploit useful cues from these anchors to reliably reason about locations of the regions described by the phrases given only image-level supervision. Through differentiable sampling on image spatial feature maps, MATN introduces a novel training objective to simultaneously minimize a contrastive reconstruction loss between different phrases from a single image and a set of triplet losses among multiple images with similar phrases. Superior to existing region proposal based methods, MATN searches for the optimal bounding box over the entire feature map instead of selecting a sub-optimal one from discrete region proposals. We evaluate MATN on the Flickr30K Entities and ReferItGame datasets. The experimental results show that MATN significantly outperforms the state-of-the-art methods.
                                                            <p>Fang Zhao, Jianshu Li, <b>Jian Zhao</b>, and Jiashi Feng</p>
							    <p>CVPR 2018&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Weakly_Supervised_Phrase_CVPR_2018_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Weakly+Supervised+Phrase+Localization+With+Multi-Scale+Anchored+Transformer+Network&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
       Synthesizing realistic profile faces is promising for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our submissions to NIST IJB-A 2017 face recognition competitions, where we won the 1st places on the tracks of verification and identification.
                                                            <p><b>Jian Zhao</b>, Lin Xiong, Karlekar Jayashree, Jianshu Li, Fang Zhao, Zhecan Wang, Sugiri Pranata, Shengmei Shen, Shuicheng Yan, and Jiashi Feng</p>
							    <p>NeurIPS 2017&nbsp;<a target="_blank" href="http://papers.nips.cc/paper/6612-dual-agent-gans-for-photorealistic-and-identity-preserving-profile-face-synthesis.pdf">PDF</a>, <a target="_blank" href="pub/ZHAOJIAN_ID70.pdf">Poster</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Dual-agent+gans+for+photorealistic+and+identity+preserving+profile+face+synthesis&btnG=">BibTeX</a>, Foundation of Panasonic FacePRO (YouTube <a href="https://www.youtube.com/watch?v=Z5vxhhM0JGQx">News1</a>, <a href="https://www.youtube.com/watch?v=J_-iRx7z1qQ">News2</a>)</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Robust LSTM-Autoencoders for Face De-Occlusion in the Wild</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers, which is heavily desired in real-world applications concerning surveillance and security. Although much research effort has been devoted to developing face de-occlusion methods, most of them can only work well under constrained conditions, such as all of faces are from a pre-defined closed set of subjects. In this paper, we propose a robust LSTM-Autoencoders (RLA) model to effectively restore partially occluded faces even in the wild. The RLA model consists of two LSTM components, which aims at occlusion-robust face encoding and recurrent occlusion removal respectively. The first one, named multi-scale spatial LSTM encoder, reads facial patches of various scales sequentially to output a latent representation, and occlusion-robustness is achieved owing to the fact that the influence of occlusion is only upon some of the patches. Receiving the representation learned by the encoder, the LSTM decoder with a dual channel architecture reconstructs the overall face and detects occlusion simultaneously, and by feat of LSTM, the decoder breaks down the task of face de-occlusion into restoring the occluded part step by step. Moreover, to minimize identify information loss and guarantee face recognition accuracy over recovered faces, we introduce an identity-preserving adversarial training scheme to further improve RLA. Extensive experiments on both synthetic and real data sets of faces with occlusion clearly demonstrate the effectiveness of our proposed RLA in removing different types of facial occlusion at various locations. The proposed method also provides significantly larger performance gain than other de-occlusion methods in promoting recognition performance over partially-occluded faces.
                                                            <p>Fang Zhao, Jiashi Feng, <b>Jian Zhao</b>, Wenhan Yang, and Shuicheng Yan</p>
                                                            <p>T-IP&nbsp; <a target="_blank" href="http://ieeexplore.ieee.org/document/8101544/">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Robust+LSTM-Autoencoders+for+Face+De-Occlusion+in+the+Wild&btnG=" target="_blank">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Conditional Dual-Agent GANs for Photorealistic and Annotation Preserving Image Synthesis</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
        In this paper, we propose a novel Conditional Dual-Agent GAN (CDA-GAN) for photorealistic and annotation preserving image synthesis, which significantly benefits Deep Convolutional Neural Networks (DCNNs) learning. Instead of merely distinguishing real or fake, the proposed dual agents of the Discriminator are able to preserve both of realism and annotation information simultaneously through a standard adversarial loss and an annotation perception loss. During training, the Generator is conditioned on the desired image features learned by a pre-trained CNN sharing the same architecture of the Discriminator yet different weights. Thus, CDA-GAN is flexible in terms of scalability and able to generate photorealistic image with well preserved annotation information for learning DCNNs in specific domains. We perform detailed experiments to verify the effectiveness of CDA-GAN, which outperforms other state-of-the-arts on MNIST digits classification dataset and IJB-A face recognition dataset.
                                                            <p>Zhecan Wang, <b>Jian Zhao</b>, Yu Cheng, Shengtao Xiao, Jianshu Li, Fang Zhao, Jiashi Feng, and Ashraf Kassim</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
							    <p>BMVC 2017 FaceHUB Workshop (Oral)&nbsp;<a target="_blank" href="https://www.researchgate.net/profile/Jian-Zhao-48/publication/320223753_Conditional_Dual-Agent_GANs_for_Photorealistic_and_Annotation_Preserving_Image_Synthesis/links/59db73a00f7e9b2f587fef31/Conditional-Dual-Agent-GANs-for-Photorealistic-and-Annotation-Preserving-Image-Synthesis.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Conditional+Dual-Agent+GANs+for+Photorealistic+and+Annotation+Preserving+Image+Synthesis&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">High Performance Large Scale Face Recognition with Multi-Cognition Softmax and Feature Retrieval</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	To solve this large scale face recognition problem, a Multi-Cognition Softmax Model (MCSM) is proposed to distribute training data to several cognition units by a data shuffling strategy in this paper. Here we introduce one cognition unit as a group of independent softmax models, which is designed to increase the diversity of the one softmax model to boost the performance for models ensemble. Meanwhile, a template-based Feature Retrieval (FR) module is adopted to improve the performance of MCSM by a specific voting scheme. Moreover, a one-shot learning method is applied on collected extra 600K identities due to each identity has one image only. Finally, testing images with lower score from MCSM and FR are assigned new labels with higher score by merging one-shot learning results. Our solution ranks the first place in both two settings of the final evaluation and outperforms other teams by a large margin.
                                                            <p>Yan Xu, Yu Cheng, <b>Jian Zhao</b>, Zhecan Wang, Lin Xiong, Karlekar Jayashree, Hajime Tamura, Tomoyuki Kagaya, Sugiri Pranata, Shengmei Shen, Jiashi Feng, and Junliang Xing</p>
                                                            <p>ICCV 2017 MS-Celeb-1M Workshop (Oral)&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w27/Xu_High_Performance_Large_ICCV_2017_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=High+Performance+Large+Scale+Face+Recognition+with+Multi-Cognition+Softmax+and+Feature+Retrieval&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
					    <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Know You at One Glance: A Compact Vector Representation for Low-Shot Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	In this paper, we propose an enforced Softmax optimization approach which is able to improve the model's representational capacity by producing a “compact vector representation” for effectively solving the challenging low-shot learning face recognition problem. Compact vector representations are significantly helpful to overcome the underlying multi-modality variations and remain the primary key features as close to the mean face of the identity as possible in the high-dimensional feature space. Therefore, the gallery facial representations become more robust under various situations, leading to the overall performance improvement for low-shot learning. Comprehensive evaluations on the MNIST, LFW, and the challenging MS-Celeb-1M Low-Shot Learning Face Recognition benchmark datasets clearly demonstrate the superiority of our proposed method over state-of-the-arts.
                                                            <p>Yu Cheng, <b>Jian Zhao</b>, Zhecan Wang, Yan Xu, Karlekar Jayashree, Shengmei Shen, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
							    <p>ICCV 2017 MS-Celeb-1M Workshop (Oral)&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w27/Cheng_Know_You_at_ICCV_2017_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Know+You+at+One+Glance%3A+A+Compact+Vector+Representation+for+Low-Shot+Learning&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Integrated Face Analytics Networks through Cross-Dataset Hybrid Training</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
	Face analytics benefits many multimedia applications. It consists of several tasks and most existing approaches generally treat these tasks independently, which limits their deployment in real scenarios. In this paper we propose an integrated Face Analytics Network (iFAN), which is able to perform multiple tasks jointly for face analytics with a novel carefully designed network architecture to fully facilitate the informative interaction among different tasks. The proposed integrated network explicitly models the interactions between tasks so that the correlations between tasks can be fully exploited for performance boost. In addition, to solve the bottleneck of the absence of datasets with comprehensive training data for various tasks, we propose a novel cross-dataset hybrid training strategy. It allows ``plug-in and play'' of multiple datasets annotated for different tasks without the requirement of a fully labeled common dataset for all the tasks. We experimentally show that the proposed iFAN achieves state-of-the-art performance on multiple face analytics tasks using a single integrated model. Specifically, iFAN achieves an overall F-score of 91.15% on the Helen dataset for face parsing, a normalized mean error of 5.81% on the MTFL dataset for facial landmark localization and an accuracy of 45.73% on the BNU dataset for emotion recognition with a single model.
                                                            <p>Jianshu Li, Shengtao Xiao, Fang Zhao, <b>Jian Zhao</b>, Jianan Li, Jiashi Feng, Shuicheng Yan, and Terence Sim</p>
                                                            <p>ACM MM 2017 (Oral) &nbsp; <a target="_blank" href="https://arxiv.org/pdf/1711.06055.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Integrated+Face+Analytics+Networks+through+Cross-Dataset+Hybrid+Training&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Multi-Human Parsing in the Wild</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Human parsing is attracting increasing research attention. In this work, we aim to push the frontier of human parsing by introducing the problem of multi-human parsing in the wild. Existing works on human parsing mainly tackle single-person scenarios, which deviates from real-world applications where multiple persons are present simultaneously with interaction and occlusion. To address the multi-human parsing problem, we introduce a new multi-human parsing (MHP) dataset and a novel multi-human parsing model named MH-Parser. The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained semantic annotations in an instance-aware setting. The MH-Parser generates global parsing maps and person instance masks simultaneously in a bottom-up fashion with the help of a new Graph-GAN model. We envision that the MHP dataset will serve as a valuable data resource to develop new multi-human parsing models, and the MH-Parser offers a strong baseline to drive future research for multi-human parsing in the wild.
                                                            <p>Jianshu Li, <b>Jian Zhao</b>, Yunchao Wei, Congyan Lang, Yidong Li, Terence Sim, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p><b>(The first two authors are with equal contributions.)</b></p>
                                                            <p>ArXiv&nbsp; <a href="https://mp.weixin.qq.com/s/tfiPHvkhPW4HDEUzBMseGQ" target="_blank">WeChat News</a>, <a target="_blank" href="https://arxiv.org/pdf/1705.07206.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Towards+Real+World+Human+Parsing%3A+Multiple-Human+Parsing+in+the+Wild&btnG=">BibTeX</a>, MHP Dataset v1.0 <a href="https://github.com/ZhaoJ9014/Multi-Human-Parsing_MHP" target="_blank">Download</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Self-Supervised Neural Aggregation Networks for Human Parsing</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
In this paper, we present a Self-Supervised Neural Aggregation Network (SS-NAN) for human parsing. SS-NAN adaptively learns to aggregate the multi-scale features at each pixel "address". In order to further improve the feature discriminative capacity, a self-supervised joint loss is adopted as an auxiliary learning strategy, which imposes human joint structures into parsing results without resorting to extra supervision. The proposed SS-NAN is end-to-end trainable. SS-NAN can be integrated into any advanced neural networks to help aggregate features regarding the importance at different positions and scales and incorporate rich high-level knowledge regarding human joint structures from a global perspective, which in turn improve the parsing results. Comprehensive evaluations on the recent Look into Person (LIP) and the PASCAL-Person-Part benchmark datasets demonstrate the significant superiority of our method over other state-of-the-arts.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Xuecheng Nie, Yunpeng Chen, Zhecan Wang, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>CVPR 2017 Visual Understanding of Human in Crowd Scene Workshop (Oral)&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w19/papers/Zhao_Self-Supervised_Neural_Aggregation_CVPR_2017_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=+Self-Supervised+Neural+Aggregation+Networks+for+Human+Parsing&btnG=">BibTeX</a>, <a target="_blank" href="https://github.com/ZhaoJ9014/SS-NAN">Code</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Estimation of Affective Level in the Wild with Multiple Memory Networks</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
This paper presents the proposed solution to the ''affect in the wild'' challenge, which aims to estimate the affective level, i.e. the valence and arousal values, of every frame in a video. A carefully designed deep convolutional neural network (a variation of residual network) for affective level estimation of facial expressions is first implemented as a baseline. Next we use multiple memory networks to model the temporal relations between the frames. Finally ensemble models are used to combine the predictions from multiple memory networks. Our proposed solution outperforms the baseline model by a factor of 10.62% in terms of mean square error (MSE).
                                                            <p>Jianshu Li, Yunpeng Chen, Shengtao Xiao, <b>Jian Zhao</b>, Sujoy Roy, Jiashi Feng, Shuicheng Yan, and Terencei Sim</p>
                                                            <p>CVPR 2017 Faces in-the-wild Workshop (Oral)&nbsp; <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Li_Estimation_of_Affective_CVPR_2017_paper.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Estimation+of+Affective+Level+in+the+Wild+with+Multiple+Memory+Networks&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li class="open">
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">A Good Practice Towards Top Performance of Face Recognition: Transferred Deep Feature Fusion</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Unconstrained face recognition performance evaluations have traditionally focused on Labeled Faces in the Wild (LFW) dataset for imagery and the YouTubeFaces (YTF) dataset for videos in the last couple of years. Spectacular progress in this field has resulted in a saturation on verification and identification accuracies for those benchmark datasets. In this paper, we propose a unified learning framework named transferred deep feature fusion targeting at the new IARPA Janus Bechmark A (IJB-A) face recognition dataset released by NIST face challenge. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the LFW and YTF datasets. Inspired by transfer learning, we train two advanced deep convolutional neural networks (DCNN) with two different large datasets in source domain, respectively. By exploring the complementarity of two distinct DCNNs, deep feature fusion is utilized after feature extraction in target domain. Then, template specific linear SVMs is adopted to enhance the discrimination of framework. Finally, multiple matching scores corresponding different templates are merged as the final results. This simple unified framework outperforms the state-of-the-art by a wide margin on IJB-A dataset. Based on the proposed approach, we have submitted our IJB-A results to National Institute of Standards and Technology (NIST) for official evaluation. 
                                                            <p>Lin Xiong, Jayashree Karlekar, <b>Jian Zhao</b>, Jiashi Feng, and Shengmei Shen</p>
                                                            <p><b>(The first three authors are with equal contributions.)</b></p>
                                                            <p>arXiv&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.00438">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=A+Good+Practice+Towards+Top+Performance+of+Face+Recognition%3A+Transferred+Deep+Feature+Fusion&btnG=">BibTeX</a>, Foundation of Panasonic FacePRO (YouTube <a href="https://www.youtube.com/watch?v=Z5vxhhM0JGQx">News1</a>, <a href="https://www.youtube.com/watch?v=J_-iRx7z1qQ">News2</a>)</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2017</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Marginalized CNN: Learning Deep Invariant Representations</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
Training a deep neural network usually requires sufficient annotated samples. The scarcity of supervision samples in practice thus becomes the major bottleneck on performance of the network. In this work, we propose a principled method to circumvent this difficulty through marginalizing all the possible transformations over samples, termed as Marginalized Convolutional Neural Network (mCNN). mCNN implicitly considers in- finitely many transformed copies of the training data in every training epoch and therefore is able to learn representations invariant for transformation in an end-to-end way. We prove that such marginalization can be understood as a classic CNN with a special form of regularization and thus is efficient for implementation. Experimental results on the MNIST and affNIST digit number datasets demonstrate that mCNN can match or outperform the original CNN with much fewer training samples. Moreover, mCNN also performs well for face recognition on the recently released largescale MS-Cele-1M dataset and outperforms stateof-the-arts. Moreover, compared with the traditional CNNs which use data augmentation to improve their performance, the computational cost of mCNN is reduced by a factor of 25.
                                                            <p><b>Jian Zhao</b>, Jianshu Li, Fang Zhao, Shuicheng Yan, and Jiashi Feng</p>
                                                            <p>BMVC 2017&nbsp;<a target="_blank" href="http://www.bmva.org/bmvc/2017/papers/paper127/index.html">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Marginalized+CNN%3A+Learning+Deep+Invariant+Representations&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2016</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Robust Face Recognition with Deep Multi-View Representation Learning</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
This paper describes our proposed method targeting at the MSR Image Recognition Challenge MS-Celeb-1M. The challenge is to recognize one million celebrities from their face images captured in the real world. The challenge provides a large scale dataset crawled from the Web, which contains a large number of celebrities with many images for each subject. Given a new testing image, the challenge requires an identify for the image and the corresponding confidence score. To complete the challenge, we propose a two-stage approach consisting of data cleaning and multi-view deep representation learning. The data cleaning can effectively reduce the noise level of training data and thus improves the performance of deep learning based face recognition models. The multi-view representation learning enables the learned face representations to be more specific and discriminative. Thus the difficulties of recognizing faces out of a huge number of subjects are substantially relieved. Our proposed method achieves a coverage of 46.1% at 95% precision on the random set and a coverage of 33.0% at 95% precision on the hard set of this challenge.
                                                            <p>Jianshu Li, <b>Jian Zhao</b>, Fang Zhao, Hao Liu，Jing Li, Shengmei Shen, Jiashi Feng, and Terence Sim</p>
                                                            <p>ACM MM 2016 (Oral)&nbsp; <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/2964284.2984061">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Robust+Face+Recognition+with+Deep+Multi-View+Representation+Learning&btnG=" target="_blank">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2015</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">BE-SIFT: A More Brief and Efficient SIFT Image Matching Algorithm for Computer Vision</div>
                                                    <div class="text row">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Yiliu Feng, Shandong Yuan, and Wanzeng Cai</p>
                                                            <p>IEEE PICOM2015; <a target="_blank" href="http://ieeexplore.ieee.org/abstract/document/7363122/">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=BE-SIFT%3A+A+More+Brief+and+Efficient+SIFT+Image+Matching+Algorithm+for+Computer+Vision&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>
                                            <li>
                                                <div class="date">2014</div>
                                                <div class="circle"></div>
                                                <div class="data">
                                                    <div class="subject" style="padding-left: 0px;">Realization and Design of A Pilot Assist Decision-Making System Based on Speech Recognition</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, and Zhengfa Liang</p>
                                                            <p>AIAA2014&nbsp; <a target="_blank" href="https://arxiv.org/pdf/1406.2775.pdf">PDF</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=Realization+and+Design+of+A+Pilot+Assist+Decision-Making+System+Based+on+Speech+Recognition&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">A New Efficient Key Technology for Space Telemetry Wireless Data Link: The Low-Complexity SC-CPM SC-FDE Algorithm</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, Botao Zhang, and Li Zhou</p>
                                                            <p>ICT2014&nbsp; <a target="_blank" href="http://ieeexplore.ieee.org/document/6913690/">Link</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=+A+New+Efficient+Key+Technology+for+Space+Telemetry+Wireless+Data+Link%3A+The+Low-Complexity+SC-CPM+SC-FDE+Algorithm&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">A New Technology for MIMO Detection: The μ Quantum Genetic Sphere Decoding Algorithm</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, and Ting Chen</p>
                                                            <p>ACA2014&nbsp; <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-662-44491-7_18">Link</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=A+New+Technology+for+MIMO+Detection%3A+The+%CE%BC+Quantum+Genetic+Sphere+Decoding+Algorithm&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">Research on A Kind of Optimization Scheme of MIMO-OFDM Sphere Equalization Technology for Unmanned Aerial Vehicle Wireless Image Transmission Data Link System</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, and Shandong Yuan</p>
                                                            <p>ACA2014&nbsp; <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-662-44491-7_19">Link</a>, <a href="https://scholar.google.com.sg/scholar?hl=zh-CN&as_sdt=0%2C5&q=+Research+on+A+Kind+of+Optimization+Scheme+of+MIMO-OFDM+Sphere+Equalization+Technology+for+Unmanned+Aerial+Vehicle+Wireless+Image+Transmission+Data+Link+System&btnG=">BibTeX</a></p>
                                                        </div>
                                                    </div>
                                                    <div class="subject" style="padding-left: 0px;">Design and Implementation for A New Kind of Extensible Digital Communication Simulation System Based on Matlab</div>
                                                    <div class="text row ">
                                                        <div class="col-md-11">
                                                            <p><b>Jian Zhao</b>, Hengzhu Liu, Xucan Chen, Botao Zhang, and Ting Chen</p>
                                                            <p>Journal of Northerneastern University&nbsp;</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </li>

                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
			    
		        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Ph.D. Dissertation</h3>
                                        </div>
                                        <ul type=disc>
					     <li> DEEP LEARNING FOR HUMAN-CENTRIC IMAGE ANALYSIS: FROM FACE RECOGNITION TO HUMAN PARSING. 
				             <p> National University of Singapore, Singapore, 2019. <a target="_blank" href="https://scholarbank.nus.edu.sg/handle/10635/153741">Link</a>, <a href="https://scholar.google.com/scholar?as_q=&as_epq=DEEP%20LEARNING%20FOR%20HUMAN-CENTRIC%20IMAGE%20ANALYSIS&as_occt=any">BibTeX</a></p>
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
			<div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Special Mention</h3>
                                        </div>
                                        <ul type=disc>
					     <li> I was <a href="pub/NX-2023.jpg" target="_blank">invited</a> by Ningxia "Eastern Numbers and Western Numbers" Artificial Intelligence Science and Technology Research Young Scientists Forum to deliver a talk "Towards Unconstrained Intelligent Perception and Deep Understanding on Images/Videos" on 28th October 2023.
					     <li> I was invited by CAAI to deliver a talk "Towards Unconstrained Human-centric Intelligent Perception and Deep Understanding" on 29th October 2022 (<a target="_blank" href="https://mp.weixin.qq.com/s/2WmhVTrLc-DppgrNU7QOvg">Link</a>).
					     <li> I was invited by BSIG to deliver a talk "Towards Unconstrained Human-centric Intelligent Perception and Deep Understanding" on 9th September 2022 (<a target="_blank" href="https://mp.weixin.qq.com/s/XmeeWHHivLHxObD2NRKr9w">Link</a>).
					     <li> I was invited by Tsinghua AIR Webinar to deliver a talk "Towards Unconstrained Image/Video Deep Understanding" on 19th July 2022 (<a target="_blank" href="https://mp.weixin.qq.com/s/6LXelSYFByRt1VdayeGvRQ">Link</a>).
					     <li> I was invited by CSIG Webinar to deliver a talk "Towards Unconstrained Image/Video Deep Understanding" on 28th June 2022 (<a target="_blank" href="https://mp.weixin.qq.com/s/dcS3JCAItatpol4RuwGKlQ">Link</a>).
					     <li> Officially Interviewed by CSIG. (<a target="_blank" href="https://mp.weixin.qq.com/s/TVVMmBPGSuM41QPaBW4Okw">Link</a>)
					     <li> Officially Interviewed by Beijing Association for Science and Technology, due to a series of contributions on "Unconstrained Image/Video Deep Understanding" (<a target="_blank" href="https://mp.weixin.qq.com/s/C9VpNkyKWMv5v1KYKCRAig">BAST Official Interview1</a>, <a href="pub/interview22.pdf" target="_blank">BAST Official Interview2</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/XyXkrOySH1oAZSQ4z5vMew">BAST Official Interview3</a>).
					     <li> Baidu PaddlePaddle officially merged <a target="_blank" href="https://github.com/ZhaoJ9014/face.evoLVe">face.evoLVe</a> to better facilitate more cutting-edge researches and applications on facial analytics and human-centric multi-media understanding (<a target="_blank" href="https://mp.weixin.qq.com/s/JT_4pqRvSsAOhQln0GSH_g">Official Announcement</a>).
					     <li> I have co-organized a VALSE Tutorial with Assoc. Prof. <a target="_blank" href="https://sites.google.com/site/xingxingwei1988/">Xingxing Wei</a> on the topic of "对抗环境下的深度合成和鉴别" on 08/09/2021 (<a target="_blank" href="https://mp.weixin.qq.com/s/Mfy639FcGKt6XYQU-iXJgA">Link</a>).
					     <li> I have co-organized a VALSE Webinar with Dr. <a target="_blank" href="https://sites.google.com/view/wenguanwang">Wenguan Wang</a> and Prof. <a target="_blank" href="https://wangzwhu.github.io/home/">Zheng Wang</a> on the topic of "Human-Centric Vision Techniques" on 13/01/2021 (<a target="_blank" href="https://mp.weixin.qq.com/s/cAovsOUUJtSFqqJHNOI5Ew">Link</a>).
					     <li> I was invited by Qihoo 360 to attend the <a href="pub/2020SH.jpg" target="_blank">"2020上海数字创新大会"</a> on 05/12/2020 as a panel guest of "人工智能给网络空间带来的机遇与挑战" session.
					     <li> I have co-organized a VALSE Webinar with Prof. <a target="_blank" href="http://changxu.xyz">Chang Xu</a> on the topic of "Visual Generation and Synthesis" on 23/09/2020 (<a target="_blank" href="http://valser.org/article-385-1.html">Link</a>).
					     <li> I was invited by Jiang Men to attend the <a target="_blank" href="https://mp.weixin.qq.com/s/333zqGhzrTXao-8Yc4TCJA">"将门ECCV 2020鲜声夺人云际会"</a> on 30/08/2020 as a panel guest of "奔涌吧后浪：从PhD到助理教授身份转变" session. <a target="_blank" href="https://mobile.techbeat.net/talks/MTU5ODkzODI4MzIxMi00MzAtNzA3NDE=?utm_campaign=eccv%E6%B4%BB%E5%8A%A8%E5%9B%9E%E9%A1%BE&utm_medium=%E7%BE%A4%E5%85%AC%E5%91%8A&utm_source=%E5%BE%AE%E4%BF%A1%E6%8A%80%E6%9C%AF%E7%A4%BE%E7%BE%A4&gio_link_id=L9G3eaB9">Video</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/JnR_NIEubxCsZMl9cU3x_g">Review</a>
					     <li> I was invited by Prof. <a target="_blank" href="https://www.xjtlu.edu.cn/en/departments/academic-departments/electrical-and-electronic-engineering/staff/jimin-xiao">Jimin Xiao</a> at Xi'an Jiaotong-Liverpool University, Xi'an, China to deliver a talk on "Deep Learning for Human-Centric Image Analysis: From Face Recognition to Human Parsing" on 31/07/2020.
					     <li> I have co-organized a VALSE Webinar with Prof. <a target="_blank" href="http://people.ucas.edu.cn/~0003913?language=en">Shiguang Shan</a> on the topic of "Face-based Human Understanding: beyond Face Recognition" on 25/03/2020 (<a target="_blank" href="https://mp.weixin.qq.com/s/pvh4qISb1lwu31mK6G70Ig">Link1</a>, <a target="_blank" href="https://i.eqxiu.com/s/K364TUy8?share_level=4&from_user=9092cbce-bf77-4f72-8d6c-79ea640ddd2a&from_id=e2f4c5ee-f&share_time=1584662404673&from=timeline&isappinstalled=0">Link2</a>).
					     <li> I was invited by Prof. Congyan Lang at Beijing Jiaotong University, Beijing, China to deliver a talk on "Deep Learning for Human-Centric Image Analysis: From Face Recognition to Human Parsing" on 24/10/2019.
					     <li> I was <a href="pub/BAIDU.pdf" target="_blank">invited</a> by Dr. Jingtuo Liu at Baidu, Beijing, China to deliver a talk on "Deep Learning for Human-Centric Image Analysis: From Face Recognition to Human Parsing" on 05/09/2019.
					     <li> I was <a href="pub/HERAN_invitation.jpg" target="_blank">invited</a> by Prof. <a target="_blank" href="http://www.nlpr.ia.ac.cn/english/irds/People/rhe.html">Ran He</a> at Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China to deliver a talk on "Deep Learning for Human-Centric Image Analysis: From Face Recognition to Human Parsing" on 29/08/2019.
					     <li> I was invited by Huawei Noah's Ark Lab to deliver a talk on "Understanding Humans in Visual Scenes" on 13th June 2019.
					     <li> I was <a href="pub/jianzhao-PCL.pdf" target="_blank">invited</a> by Peng Cheng Laboratory (<a target="_blank" href="http://www.pcl.ac.cn/">PCL</a>) to attend the 2019 Overseas Young Scientist Forum at Shenzhen China during 30/03/2019-01/04/2019 and deliver a talk on "Deep Learning for Human-Centric Image Analysis: From Face Recognition to Human Parsing".
					     <li> I delivered a <a target="_blank" href="http://valser.org/2019/#/poster">spotlight</a> talk on "Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing" on VALSE 2019.      
					     <li> I was invited by CoLab, School of Computer, Beihang University to deliver a talk on "Deep Learning for Human-Centric Image Analysis: From Face Recognition to Human Parsing" on 23rd March 2019.
					     <li> I was invited by Tencent Deep Sea AI Lab to deliver a talk "Margin-based Representation Learning, Residual Knowledge Distillation and Prior-Aided Super Resolution" on 01st March 2019.
					     <li> I was invited by UBTECH to deliver a talk "Deep Learning for Human-Centric Image Understanding" on 08th January 2019.
					     <li> I was invited by OmniVision to deliver a talk "Facial Analytics" on 16th November 2018.
					     <li> I was invited by Jiang Men to deliver a talk "Deep Learning for Human-Centric Image Understanding" on 30th August 2018 (<a target="_blank" href="https://mp.weixin.qq.com/s/nZcGJZwXmJvvB72EnBtsCA">Link</a>, <a target="_blank" href="pub/JiangMen_Poster.jpeg">Poster</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/BZsopAYrUadQRdwj-JwZKA">Summary</a>).
					     <li> I was invited by VALSE Webinar to deliver a talk "Deep Learning for Human-Centric Image Understanding" on 22nd August 2018 (<a target="_blank" href="https://mp.weixin.qq.com/s/ZKjR-l3pz06TbL5URHJGWw">Link</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/jEFv4mfimKZW5EIahPATIg">Summary</a>).	
					     <li> I represented our group to "Launch of NUS new Vision, Mission and Values" at University Culture Center on 15th August 2018, and presented our recent work on Facial Analytics to NUS President Prof. <a href="http://president.nus.edu.sg/biography.php" target="_blank">Tan Eng Chye</a>. <a href="https://www.instagram.com/p/BmhlvOvg3be/?utm_source=ig_share_sheet&igshid=56r2rgyrhc5v" target="_blank">NUS Instagram</a>, <a href="pub/Instagram.jpeg" target="_blank">NUS News</a>, <a href="pub/20180815.jpeg" target="_blank">Gallery</a>
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
			    
                        <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Selected Awards</h3>
                                        </div>
                                        <ul type=disc>
					     <li> 2023 CAAI WU WEN JUN AI Outstanding Youth Award, CAAI. <a target="_blank" href="https://mp.weixin.qq.com/s/yKmZ5qwepRmdlw6v58I3UA">Official Announcement</a>
					     <li> Outstanding Contribution Award, <a target="_blank" href="https://anti-uav.github.io/">the 3-rd Anti-UAV Workshop & Challenge</a> with CVPR 2023. <a href="pub/3rd-anti-uav.jpeg" target="_blank">Award Certificate</a>
					     <li> 1st Prize Award, 2/6, the 2022 CAAI WU WEN JUN AI Natural Science Award. <a target="_blank" href="https://mp.weixin.qq.com/s/RxPKp8SQSaqJsKXR0dD8Iw">Official Announcement</a>, <a href="pub/WWJ-2023-CEM.jpeg" target="_blank">Award Ceremony</a>, <a href="pub/WWJ-2023.jpeg" target="_blank">Award Certificate</a>
					     <li> 1st-Place Award, Advisor, Huawei Special Award in the 2022 "Huawei Cup" 4th China Graduate Artificial Intelligence Innovation Challenge. <a href="pub/huawei22.jpg" target="_blank">Award Certificate</a>
					     <li> 1st-Place Award, Advisor, <a target="_blank" href="https://ibug.doc.ic.ac.uk/resources/masked-face-recognition-challenge-workshop-iccv-21/">Masked Face Recognition Challenge</a> (WebFace260M Track) with ICCV 2021. <a href="pub/WebFace260M.jpg" target="_blank">Award Certificate</a>
					     <li> 2020-2022 Young Elite Scientist Sponsorship Program, China Association for Science and Technology. <a target="_blank" href="http://www.csig.org.cn/detail/3056">CSIG Official Announcement</a>, <a target="_blank" href="https://mp.weixin.qq.com/s/yGc_VTCNgqYRKAo75hre8g">CSIG Official News</a>, <a target="_blank" href="https://www.cast.org.cn/art/2021/9/9/art_458_168103.html">CAST Official Announcement</a>, <a href="pub/CAST.pdf" target="_blank">CAST Official Announcement Doc</a>, <a href="pub/Certificate-CAST.PDF" target="_blank">Certificate</a>
					     <li> <a href="pub/360-1.jpg" target="_blank">A-Level Certificate</a>, 4th author, <a target="_blank" href="https://ai.xm.gov.cn/">China Artificial Intelligence Competition</a> Activity Recognition Track, 2020. <a href="pub/360.jpg" target="_blank">Award Ceremony</a>
					     <li> 2021-2023 Beijing Young Elite Scientist Sponsorship Program, Beijing Association for Science and Technology. <a target="_blank" href="http://www.bsig.org.cn/detail/2430">BSIG Official Announcement</a>, <a href="pub/Certificate-BAST.pdf" target="_blank">Certificate</a>
					     <li> 2021-2023 Youth Program of National Natural Science Foundation of China, National Natural Science Foundation of China. <a href="pub/NSFC2020.pdf" target="_blank">Approval Notification</a>
					     <li> Lee Hwee Kuan Award (Gold Award), 1st author, PREMIA 2019. <a href="pub/PREMIA-19.jpeg" target="_blank">Award Certificate</a>, <a target="_blank" href="http://www.premiasg.org/for-members/premia-best-student-paper-awards/premia-best-student-paper-awards-2019/">PREMIA Official Announcement</a>
					     <li> Best Student Paper Award, 1st author, ACM MM 2018. <a href="pub/ACM MM18_Best Student Paper.pdf" target="_blank">Award Certificate</a>, <a target="_blank" href="http://www.acmmm.org/2018/awards/">ACM Official News</a>, <a target="_blank" href="https://www.eng.nus.edu.sg/2018/lv-team-wins-the-best-student-paper-award-at-acm-mm-2018/">NUS FOE News</a>, <a target="_blank" href="http://ece.nus.edu.sg/drupal/?q=node/25">NUS ECE Announcement</a>, <a target="_blank" href="http://ece.nus.edu.sg/drupal/node/243">NUS ECE News</a>
					     <li> 1st-Place Award, 1st author, MS-Celeb-1M face recognition <a target="_blank" href="http://www.msceleb.org/leaderboard/iccvworkshop-c1">hard set</a>/<a target="_blank" href="http://www.msceleb.org/leaderboard/iccvworkshop-c1">random set</a>/<a target="_blank" href="http://www.msceleb.org/leaderboard/c2">low-shot learning</a> challenges with ICCV 2017. <a href="http://mp.weixin.qq.com/s/-G94Mj-8972i2HtEcIZDpA" target="_blank">WeChat News</a>, <a target="_blank" href="http://ece.nus.edu.sg/drupal/?q=node/215">NUS ECE News</a>, <a href="pub/ECE_Poster.jpeg" target="_blank">NUS ECE Poster</a>, <a href="pub/MS-Track1.jpeg" target="_blank">Award Certificate for Track-1</a>, <a href="pub/MS-Track2.jpeg" target="_blank">Award Certificate for Track-2</a>, <a href="pub/MS-Awards.jpeg" target="_blank">Award Ceremony</a>
					     <li> 2nd-Place Award, 1st author, L.I.P human <a target="_blank" href="http://hcp.sysu.edu.cn/lip/leaderboard.php">parsing/pose</a> estimation challenges with CVPR 2017. <a href="pub/LIP_Certificate_Parsing.pdf" target="_blank">Award Certificate for Parsing</a>, <a href="pub/LIP_Certificate_Pose.pdf" target="_blank">Award Certificate for Pose</a>, <a href="pub/LIP-Awards.jpeg" target="_blank">Award Ceremony</a>
					     <li> 1st-Place Award, 1st author, NIST IJB-A unconstrained face <a href="pub/IJBA_11_report.pdf" target="_blank">verification</a>/<a href="pub/IJBA_1N_report.pdf" target="_blank">identification</a> challenges, 2017. <a href="https://mp.weixin.qq.com/s/s9H_OXX-CCakrTAQUFDm8g" target="_blank">WeChat News</a>
					     <li> 3rd-Place Award, 2nd author, MS-Celeb-1M face recognition <a target="_blank" href="https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/">hard set</a> challenge with ACM MM 2016.
                                             <li> Excellent Student Award (<10%), School of Computer, National University of Defense Technology, 2016.
                                             <li> Excellent Student Award (<10%), School of Computer, National University of Defense Technology, 2015.
　　                                          <li> Excellent Graduate Award (<2%), National University of Defense Technology, 2014.
                                             <li> Excellent Student Award (<2%), National University of Defense Technology, 2014. <a href="pub/14ExcellentStudentAward.jpeg" target="_blank">Award Certificate</a>
                                             <li> Guanghua Fellowship (<2%), National University of Defense Technology, 2014. <a href="pub/Guanghua.jpeg" target="_blank">Award Certificate</a>
                                             <li> Contribution Prize, for Engineering Implementation of Tianhe-2 supercomputer (No.1 on Top500, Jun, 2013), National University of Defense Technology, 2013. <a href="pub/13Tianhe.jpeg" target="_blank">Award Certificate</a>
					     <li> 3rd-Place Award, 1st author, 13th "Great Wall Information Cup" challenge, National University of Defense Technology, 2013.
                                             <li> Excellent Student Award (<10%), School of Computer, National University of Defense Technology, 2013. <a href="pub/13ExcellentStudentAward.jpeg" target="_blank">Award Certificate</a>
                                             <li> 1st-Place Award, 1st author, Big Data Processing and Information Sub-Forum of the 6th Graduate Innovation Forum, Provincial Education Department of Hunan Province, 2013. <a href="pub/13FirstPrize.jpeg" target="_blank">Award Certificate</a>
                                             <li> Excellent Graduate Award (<2%), Beihang University, 2012. <a href="pub/12ExcellentGraduate.jpeg" target="_blank">Award Certificate</a>
                                             <li> 2nd-Place Award, 1st author, 5th Student Research Training Program (SRTP), Beihang University, 2012. <a href="pub/SRTP.jpeg" target="_blank">Award Certificate</a>
                                             <li> National Endeavor Fellowship, Central Government & Beijing Government of China, 2011.
                                             <li> 3rd-Place Award, 1st author, "Feng Ru Cup" challenge, School of Automation Science and Electrical Engineering, Beihang University, 2011. <a href="pub/11Fengru.jpeg" target="_blank">Award Certificate</a>
                                             <li> SMC Fellowship, Beihang University, 2010.
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>

                       <div class="section color-2">
                            <div class="section-container">
                                <div class="row">
                                    <div id="publications" class="col-md-10 col-md-offset-1">
                                        <div class="title text-center">
                                            <h3>Open Positions</h3>
                                        </div>
                                        <ul type=disc>
					     <li> Several research fellow, master/Ph.D. student, engineer, and research assistant positions are available. Interested candidates with strong publication record please email me.
				             <li> Self-motivated students are highly welcomed. 
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
                            
                        <div class="pagecontents">
                            <div class="section color-1">
                                <div class="section-container">
                                    <div id="contact" class="row">
                                        <div  class="col-md-10 col-md-offset-1">
                                            <div class="title text-center">
                                                <h3>Contact</h3> 
                                            </div>
                                            <p><b>Email</b>: jian_zhao (at) nwpu (dot) edu (dot) cn OR zhaojian90 (at) u (dot) nus (dot) edu OR zhaoj08 (at) pcl (dot) ac (dot) cn</p>
                                            <p><b>Phone</b>: (86) 183 0135 5501 </p>
                                            <p><b>Address</b>: 11 Dongzhimen South Street, Dongcheng District, Beijing, China 100027 </p>
<p style="margin-top:25px;font-size:smaller;text-align:center">Modified: 13 March 2024</p>                            
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5o66f75yvk2&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33&amp;s=170" async="async"></script>
 </div>
                                    </div>
                                </div>

                            </div>
                    </div>
                </div>
        </div>
</div>        </div>

<script>
  mixpanel.track("Page zhaoj9014.github.io load");
</script>
</body>
</html>
